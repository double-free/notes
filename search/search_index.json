{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ye's Notes Notes for my random readings. There's no explicit goal, but I'd love to keep my brain busy. The notes can be roughly split to 3 topics, they are related to my work to varying degrees: Trading Computer science Mathematics Again, I'm not an expert in any of these areas, sorry for any error or incompleteness, and appreciate if you can point them out. Trading WIP Computer Science WIP Mathematics WIP","title":"Ye's Notes"},{"location":"#yes-notes","text":"Notes for my random readings. There's no explicit goal, but I'd love to keep my brain busy. The notes can be roughly split to 3 topics, they are related to my work to varying degrees: Trading Computer science Mathematics Again, I'm not an expert in any of these areas, sorry for any error or incompleteness, and appreciate if you can point them out.","title":"Ye's Notes"},{"location":"#trading","text":"WIP","title":"Trading"},{"location":"#computer-science","text":"WIP","title":"Computer Science"},{"location":"#mathematics","text":"WIP","title":"Mathematics"},{"location":"mathematics/ESL3_LinearMethodsForRegression/","text":"\u4e00\u4e2a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u5047\u8bbe\u56de\u5f52\u51fd\u6570 E(Y|X) \u5bf9\u4e8e\u8f93\u5165 X \u662f\u7ebf\u6027\u7684\u3002 \u5b83\u7684\u4f18\u52bf\u5728\u4e8e\uff1a - \u7b80\u5355 - \u80fd\u591f\u8868\u793a\u6bcf\u4e2a\u8f93\u5165\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd - \u8f93\u5165\u53ef\u4ee5\u8fdb\u884c\u53d8\u6362 - \u4ed6\u4eec\u6709\u65f6\u5019\u6bd4\u590d\u6742\u7684\u65b9\u6cd5\u66f4\u7cbe\u51c6\uff0c\u5c24\u5176\u662f\u5728\u6837\u672c\u6570\u91cf\u5c11\u3001\u4f4e\u4fe1\u566a\u6bd4\u6216\u8005\u7a00\u758f\u77e9\u9635\u7684\u60c5\u5f62\u3002 3.2 Linear Regression Models and Least Squares $p$ \u7ef4\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u5f62\u5f0f\u5982\u4e0b\uff1a $$f(X) = \\beta_0 + \\sum_{j=1}^p X_j \\beta_j$$ \u6211\u4eec\u9700\u8981\u4f30\u8ba1\u4e00\u7ec4\u53c2\u6570 $\\beta$\uff0c\u4f7f\u6b8b\u5dee\u5e73\u65b9\u548c\uff08Residual Sum of Squares\uff09\u6700\u5c0f\uff1a $$\\begin{align} \\text{RSS}(\\boldsymbol{\\beta}) &= (\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} )^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} ) \\ &= \\boldsymbol{y}^T\\boldsymbol{y} - \\boldsymbol{y}^T\\boldsymbol{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\boldsymbol{X}^T\\boldsymbol{y} + \\boldsymbol{\\beta}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\beta} \\end{align}$$ \u5176\u4e2d\uff0c$ \\boldsymbol{X} $ \u662f\u4e00\u4e2a $N \\times (p+1)$ \u77e9\u9635\uff0c$\\boldsymbol{y}$ \u662f $N \\times 1$ \u89c2\u6d4b\u503c\u3002 \u5bf9 $\\beta$ \u6c42\u5bfc\u53ef\u4ee5\u5f97\u5230\uff1a $$ \\frac{\\partial \\text{RSS}(\\beta)}{\\partial \\beta} = -2 \\boldsymbol{X}^T\\boldsymbol{y} + 2\\boldsymbol{X}^T\\boldsymbol{X} \\boldsymbol{\\beta}$$ \u7531\u4e8e\u4e8c\u9636\u5bfc\u6570\u6b63\u5b9a\uff0c\u4ee4\u4e00\u9636\u5bfc\u6570\u4e3a 0 \u5411\u91cf\uff0c\u5f97\u51fa\u6781\u503c\u70b9\uff08\u5373\u4f30\u8ba1\u503c\uff09\uff1a $$ \\hat{\\beta}= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}$$ $$\\hat{\\boldsymbol{y}} = \\boldsymbol{X} \\hat{\\beta} = \\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}$$ \u6211\u4eec\u79f0 $\\boldsymbol{H} = \\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T$ \u4e3a\u4f30\u8ba1\u77e9\u9635\uff08\"hat\" matrix\uff09\uff0c\u5b83\u6ee1\u8db3\u5bf9\u79f0\u6027\u548c\u5e42\u7b49\u6027\uff1a $$\\boldsymbol{H}^T = \\boldsymbol{H}$$ $$\\boldsymbol{H}^T\\boldsymbol{H} = \\boldsymbol{H}$$ \u5f53 $\\boldsymbol{X}$ \u4e2d\u67d0\u4e9b\u5217\u7ebf\u6027\u76f8\u5173\uff08\u5373\u975e\u6ee1\u79e9\u77e9\u9635\uff09\u65f6\uff0c$(\\boldsymbol{X}^T\\boldsymbol{X})$ \u662f\u5947\u5f02\u77e9\u9635\uff0c\u5b83\u53ea\u80fd\u6c42\u5e7f\u4e49\u9006\u77e9\u9635\uff0c\u4e0d\u6b62\u4e00\u4e2a\u89e3\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5c06\u5197\u4f59\u7684\u8f93\u5165\u5254\u9664\u6389\uff0c\u5927\u90e8\u5206\u6c42\u89e3\u8f6f\u4ef6\u90fd\u5b9e\u73b0\u4e86\u8fd9\u4e2a\u529f\u80fd\u3002 \u4f30\u8ba1\u53c2\u6570\u7684\u7edf\u8ba1\u7279\u6027 \u4e3a\u4e86\u786e\u5b9a\u4f30\u8ba1\u7684\u53c2\u6570 $\\boldsymbol{\\hat{\\beta}}$ \u7684\u7edf\u8ba1\u7279\u6027\uff0c\u6211\u4eec\u5047\u8bbe\uff1a \u6bcf\u4e2a\u89c2\u6d4b\u503c $y_i$ \u76f8\u4e92\u72ec\u7acb $y_i$\u6709\u56fa\u5b9a\u7684\u566a\u58f0 $\\varepsilon \\sim N(0, \\sigma^2)$ \u90a3\u4e48\u4f30\u8ba1\u503c $\\hat{\\beta}$ \u7684\u65b9\u5dee\u4e3a\uff1a $$ \\text{Var}(\\hat{\\beta}) = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\sigma^2$$ where : $$\\hat{\\sigma}^2 = \\frac{\\text{RSS}}{N-p-1}= \\frac{1}{N-p-1} \\sum_{i=1}^{N} (y_i-\\hat{y})^2$$ \u8bc1\u660e N \u4e2a y \u7684\u89c2\u6d4b\u503c\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a $$ \\boldsymbol{y} = \\boldsymbol{X}\\beta + \\boldsymbol{\\varepsilon}$$ \u5176\u4e2d $ \\boldsymbol{\\varepsilon} $ \u662f $N \\times 1$ \u7684\u566a\u58f0\u3002\u56e0\u6b64\u6709\uff1a $$\\begin{align} \\hat{\\beta} &= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y} \\ &= \\beta + (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{\\varepsilon} \\end{align}$$ \u65e0\u504f\u6027\uff08\u671f\u671b\u503c\u4e3a $\\beta$\uff09\uff1a $$E(\\hat{\\beta}) = \\beta + (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T E(\\boldsymbol{\\varepsilon}) = \\beta$$ \u534f\u65b9\u5dee\u77e9\u9635\uff08\u6ce8\u610f\u662f$\\beta \\beta^T$ \u800c\u975e $\\beta^T \\beta$\uff0c\u662f\u4e00\u4e2a\u77e9\u9635\uff09\uff1a $$\\begin{align} \\text{Var}(\\hat{\\beta}) &= E[(\\beta - \\hat{\\beta})(\\beta - \\hat{\\beta})^T] \\ &=E[(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^T\\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}] \\ &= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T E(\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^T) \\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\ &= \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{I} \\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\ &= \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\end{align}$$ \u53ef\u4ee5\u5f97\u5230\uff1a $$ \\hat{\\beta} \\sim N(\\beta, \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1})$$ \u4e0b\u9762\u6765\u786e\u5b9a $\\sigma^2$ \u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u89c2\u6d4b\u503c $y$ \u548c\u9884\u6d4b\u503c $\\hat{y}$ \u7684\u5dee\u6765\u5f97\u5230\u566a\u58f0 $\\varepsilon$\u3002 $$\\begin{align} \\boldsymbol{y - \\hat{y}} &= \\boldsymbol{X}\\beta + \\boldsymbol{\\varepsilon} -\\boldsymbol{X}\\hat{\\beta} \\ &= \\boldsymbol{X}\\beta + \\boldsymbol{\\varepsilon} - \\boldsymbol{X}(\\beta + (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{\\varepsilon}) \\ &= (\\boldsymbol{I -H} )\\boldsymbol{\\varepsilon} \\end{align}$$ $$\\begin{align} \\sum_{i=1}^N(y_i - \\hat{y_i})^2 &= (\\boldsymbol{y - \\hat{y}})^T (\\boldsymbol{y - \\hat{y}}) \\ &= \\boldsymbol{\\varepsilon}^T(\\boldsymbol{I - H}) \\boldsymbol{\\varepsilon} \\ &= \\sum_{k =1}^N \\varepsilon_k^2- \\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij} \\end{align}$$ \u5176\u671f\u671b\u503c\u4e3a\uff1a $$\\begin{align} E[\\sum_{i=1}^N(y_i - \\hat{y_i})^2] &= E[\\sum_{k =1}^N \\varepsilon_k^2- \\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij} ] \\ &= N\\sigma^2 - E(\\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij}) \\end{align}$$ \u7531\u4e8e $\\varepsilon_i, \\varepsilon_j$ \u662f\u72ec\u7acb\u7684\uff0c\u5f53 $i \\neq j$ \u65f6\uff1a $$\\text{Cov}(\\varepsilon_i, \\varepsilon_j) = E(\\varepsilon_i \\varepsilon_j) - E(\\varepsilon_i)E(\\varepsilon_j) = 0$$ \u56e0\u6b64\uff1a $$\\begin{align} E[\\sum_{i=1}^N(y_i - \\hat{y_i})^2] &= N\\sigma^2 - E(\\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij}) \\ &= N\\sigma^2 - E(\\sum_{i=1}^{N}\\varepsilon_i^2H_{ii}) \\ &= \\sigma^2[N - \\text{trace}(\\boldsymbol{H})] \\end{align}$$ \u8fd9\u91cc\u518d\u5229\u7528\u516c\u5f0f\uff1a $$\\text{trace}(ABC) = \\text{trace}(CAB) $$ \u5f97\u5230\uff1a $$\\begin{align} E[\\sum_{i=1}^N(y_i - \\hat{y_i})^2] &= \\sigma^2[N - \\text{trace}(\\boldsymbol{H})] \\ &= \\sigma^2[N - \\text{trace}(\\boldsymbol{X(X^TX)^{-1}X^T})] \\ &= \\sigma^2[N - \\text{trace}(\\boldsymbol{X^TX(X^TX)^{-1}} {(p+1) \\times (p+1)})] \\ &= \\sigma^2[N - \\text{trace}(\\boldsymbol{I} {(p+1) \\times (p+1)})] \\ &= \\sigma^2(N - p -1) \\end{align}$$ \u56e0\u6b64\uff0c\u5bf9 $\\sigma^2$ \u7684\u65e0\u504f\u4f30\u8ba1\u5c31\u662f\uff1a $$\\hat{\\sigma}^2 = \\frac{1}{N-p-1} \\sum_{i=1}^{N} (y_i-\\hat{y})^2$$ \u6a21\u578b\u8bef\u5dee\u7684\u7edf\u8ba1\u7279\u6027 \u7531\u4e8e\u6211\u4eec\u5bf9\u7b2c i \u4e2a\u6837\u672c\u7684\u566a\u58f0 $\\varepsilon_i $ \u65e0\u504f\u4f30\u8ba1\u5c31\u662f $\\hat{\\varepsilon_i} = y_i - \\hat{y_i}$\uff0c\u6211\u4eec\u8ba1\u7b97\u5176\u65b9\u5dee\uff1a $$\\begin{align} \\text{Var}(\\hat{\\boldsymbol{\\varepsilon}}) &= \\text{Var}(\\boldsymbol{y} - \\hat{\\boldsymbol{y}}) \\ &= \\text{Var}[(\\boldsymbol{I} - \\boldsymbol{H}){\\boldsymbol{\\varepsilon}}] \\end{align}$$ \u7531\u4e8e $D(AX) = AD(X)A^T$\uff1a $$\\begin{align} \\text{Var}(\\hat{\\boldsymbol{\\varepsilon}}) &= \\text{Var}[(\\boldsymbol{I} - \\boldsymbol{H}){\\boldsymbol{\\varepsilon}}] \\ &= (\\boldsymbol{I} - \\boldsymbol{H}) \\text{Var}(\\boldsymbol{\\varepsilon}) (\\boldsymbol{I} - \\boldsymbol{H}) \\end{align}$$ \u7531\u4e8e $\\varepsilon \\sim N(0, \\sigma^2)$\uff0c\u56e0\u6b64\uff1a $$\\text{Var}(\\boldsymbol{\\varepsilon}) = \\sigma^2 \\boldsymbol{I}_{N \\times N}$$ \u800c $\\boldsymbol{H} = \\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T$ \u6ee1\u8db3\u5bf9\u79f0\u6027\u548c\u5e42\u7b49\u6027\uff1a $$\\boldsymbol{H}^T = \\boldsymbol{H}$$ $$\\boldsymbol{H}^T\\boldsymbol{H} = \\boldsymbol{H}$$ \u56e0\u6b64\u6709\u7ed3\u8bba\uff1a $$\\text{Var}(\\hat{\\boldsymbol{\\varepsilon}}) = \\sigma^2 (\\boldsymbol{I} - \\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T)$$ \u663e\u8457\u6027\u5206\u6790 \u5f53\u6211\u4eec\u5224\u65ad\u54ea\u4e9b\u53c2\u6570\u53ef\u4ee5\u5ffd\u7565\u4ee5\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 F-statistic \u8fdb\u884c\u663e\u8457\u6027\u5206\u6790\u3002\u5047\u8bbe\u6211\u4eec\u5c06 $\\beta$ \u7ef4\u5ea6\u4ece $p_1 + 1$ \u964d\u4f4e\u5230 $p_0 + 1$\uff1a $$ F = \\frac{(\\text{RSS}_0 - \\text{RSS}_1) / (p_1 - p_0)}{\\text{RSS}_1 / (N- p_1 -1)} $$ F-statistic \u63cf\u8ff0\u4e86\u6bcf\u4e2a\u88ab\u5ffd\u7565\u7684\u53c2\u6570\u5bf9 RSS \u7684\u5e73\u5747\u8d21\u732e\uff0c\u7528 $\\hat{\\sigma}^2$ \u8fdb\u884c\u4e86 normalize\u3002 \u5f53 $p_1 - p_0 =1$ \u5373\u4ec5\u53bb\u6389\u4e00\u4e2a\u53c2\u6570\u65f6\uff08\u5047\u8bbe $\\beta_j = 0$\uff09\uff0c\u8be5\u516c\u5f0f\u53ef\u4ee5\u7b80\u5316\u4e3a\u5bf9\u5e94\u7684 z-score \u7684\u5e73\u65b9\uff0c\u5176\u4e2d z-score \u4e3a\uff1a $$ z_j = \\frac{\\hat{\\beta}_j}{\\hat{\\sigma} \\sqrt{v_j} }$$ where: $$\\hat{\\sigma}^2 =\\frac{\\text{RSS} 1}{N-p-1} =\\frac{1}{N-p-1} \\sum {i=1}^{N} (y_i-\\hat{y})^2$$ $$v_j = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}_{jj}$$ \u8bc1\u660e \u8fd9\u4e2a\u8bc1\u660e\u540c\u65f6\u4e5f\u662f\u4e60\u9898 3.1 Ex. 3.1 Show that the F statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding z-score (3.12). \u5b9e\u9645\u4e0a\u6211\u4eec\u9700\u8981\u8bc1\u660e\uff0c\u5728\u53bb\u6389\u6a21\u578b\u7684\u7b2c j \u4e2a\u53c2\u6570\u540e\uff1a $$ \\text{RSS}_0 - \\text{RSS}_1 = \\frac{\\hat{\\beta}_j^2}{v_j} $$ \u4e0a\u5f0f\u4e2d\u552f\u4e00\u672a\u77e5\u7684\u5c31\u662f $\\text{RSS}_0$\uff0c\u5b83\u5b9e\u8d28\u4e0a\u662f\u6c42\u4e00\u4e2a\u5e26\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff1a $$\\begin{align} \\min_{\\beta \\in \\mathbb{R}^{(p+1) \\times 1}} (\\textbf{y} - \\textbf{X}\\beta)^T(\\textbf{y}-\\textbf{X}\\beta) \\ \\text{s.t.} ~\\beta_j = 0 \\end{align}$$ \u6211\u4eec\u53ef\u4ee5\u7528\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u6cd5\u6765\u89e3\u51b3\u3002 $$L(\\beta, \\lambda) = (\\textbf{y} - \\textbf{X}\\beta)^T(\\textbf{y}-\\textbf{X}\\beta) + \\lambda e_j^T \\beta $$ \u5bf9 $\\beta$ \u6c42\u5bfc\uff0c\u5e76\u4ee4\u5bfc\u6570\u4e3a 0\uff0c\u6709\uff1a $$\\frac{\\partial L(\\beta, \\lambda)}{\\partial \\beta} = - 2\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\beta) + \\lambda e_j = 0$$ \u89e3\u51fa\uff1a $$\\begin{align} \\beta_0 &= (\\textbf{X}^T\\textbf{X})^{-1} \\textbf{X}^T\\textbf{y} - \\frac{\\lambda}{2}(\\textbf{X}^T \\textbf{X})^{-1} e_j \\ &= \\hat{\\beta}- \\frac{\\lambda}{2}(\\textbf{X}^T \\textbf{X})^{-1} e_j \\end{align}$$ \u7b49\u5f0f\u4e24\u8fb9\u4e58\u4ee5 $e_j^T$\uff0c\u5e76\u5e26\u5165$\\beta_j = 0$\uff0c\u6709\uff1a $$\\begin{align} e_j^T\\beta_0 = 0 &= e_j^T \\hat{\\beta} + \\frac{\\lambda}{2} e_j^T(\\textbf{X}^T \\textbf{X})^{-1} e_j \\ &= \\hat{\\beta}_j + \\frac{\\lambda}{2}v_j \\end{align}$$ \u56e0\u6b64\u6709\uff1a $$\\lambda = - \\frac{2\\hat{\\beta}_j}{v_j}$$ \u5e26\u5165\u53ef\u5f97\uff1a $$\\begin{align} \\text{RSS}_0 &= (\\textbf{y} - \\textbf{X}\\beta_0)^T(\\textbf{y}-\\textbf{X}\\beta_0) \\ &= (\\textbf{y} - \\textbf{X}\\hat{\\beta} + \\frac{\\lambda}{2}\\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1} e_j)^T(\\textbf{y}-\\textbf{X}\\hat{\\beta} + \\frac{\\lambda}{2}\\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1} e_j) \\ &= \\text{RSS}_1 + \\frac{\\lambda}{2} [e_j^T(\\textbf{X}^T \\textbf{X})^{-1}\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\hat{\\beta}) + (\\textbf{y} - \\textbf{X}\\hat{\\beta})^T \\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1} e_j)] \\ &~~~~ + \\frac{\\lambda^2}{4}e_j^T (\\textbf{X}^T \\textbf{X})^{-1} e_j \\ &= \\text{RSS}_1 + \\frac{\\lambda^2}{4}e_j^T (\\textbf{X}^T \\textbf{X})^{-1} e_j \\ &= \\text{RSS}_1 + \\frac{\\hat{\\beta}_j^2}{v_j} \\end{align}$$ \u5176\u4e2d\uff0c\u4e2d\u95f4\u9879\u53ef\u4ee5\u6d88\u53bb\u7684\u539f\u56e0\u662f\uff1a $$\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\hat{\\beta}) = \\textbf{X}^T[\\textbf{y} - \\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}] = 0 $$ \u76f4\u89c2\u7406\u89e3\uff0c$\\textbf{X}$ \u548c $\\textbf{y} - \\textbf{X}\\hat{\\beta}$ \u662f\u6b63\u4ea4\u7684\uff0c\u56e0\u4e3a $\\textbf{X}\\hat{\\beta}$ \u6b63\u662f $\\textbf{y}$ \u5728 $\\textbf{X}$ \u6240\u5728\u5e73\u9762\u4e0a\u7684\u6295\u5f71\u3002 3.2.2 The Gauss\u2013Markov Theorem \u6700\u5c0f\u4e8c\u4e58\u6cd5\u5f97\u51fa\u7684 $\\beta$ \u5728\u6240\u6709\u7ebf\u6027 \u65e0\u504f \u4f30\u8ba1\u4e2d\u5747\u65b9\u8bef\u5dee\u6700\u5c0f\u3002\u5f53\u7136\uff0c\u5982\u679c\u6211\u4eec\u613f\u610f\u4e3a\u4e86\u8fdb\u4e00\u6b65\u51cf\u5c0f\u8bef\u5dee\u5f15\u5165\u4e00\u70b9 bias\uff0c\u5b8c\u5168\u53ef\u80fd\u627e\u5230\u4e00\u4e2a\u66f4\u5c0f\u5747\u65b9\u8bef\u5dee\u7684 \u6709\u504f \u4f30\u8ba1\u3002 the least squares estimates of the parameters \u03b2 have the smallest variance among all linear unbiased estimates \u73b0\u5728\u6211\u4eec\u6765\u8bc1\u660e\u8fd9\u4e2a\u7ed3\u8bba\u3002\u5bf9\u4e8e\u7ebf\u6027\u4f30\u8ba1\uff1a $$\\boldsymbol{y} = \\boldsymbol{X}\\beta$$ $\\boldsymbol{y}$ \u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u90fd\u53ef\u4ee5\u770b\u4f5c $\\boldsymbol{X}$ \u4e2d\u7684\u4e00\u884c\u4e0e\u5411\u91cf $\\beta$ \u7684\u7ebf\u6027\u7ec4\u5408\u3002 \u65e0\u504f\u6027 \u90a3\u4e48\uff0c\u9488\u5bf9\u65e0\u504f\u6027\uff0c\u6211\u4eec\u9700\u8981\u8bc1\u660e\u6700\u5c0f\u4e8c\u4e58\u6cd5\u4f30\u8ba1\u51fa\u7684 $\\hat{\\beta}$ \u6ee1\u8db3\uff1a $$ E(\\alpha^T \\hat{\\beta}) = \\alpha^T\\beta$$ \u5176\u4e2d $\\alpha$ \u662f\u4efb\u610f\u5411\u91cf\u3002 $$\\begin{align} E(\\alpha^T \\hat{\\beta}) &= E(\\alpha^T (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}) \\ &= E(\\alpha^T (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{X} \\beta) \\ &= \\alpha^T \\beta \\end{align} $$ \u5747\u65b9\u8bef\u5dee\u6700\u5c0f Gauss\u2013Markov theorem \u6307\u51fa\uff0c\u5982\u679c\u8fd8\u5b58\u5728\u5176\u4ed6\u7ebf\u6027\u4f30\u8ba1 $c^T \\boldsymbol{y}$ \u6ee1\u8db3\uff1a $$ E(c^T \\boldsymbol{y}) = \\alpha^T\\beta$$ \u90a3\u4e48\u5fc5\u7136\u6709\uff1a $$\\text{Var}(\\alpha^T \\hat{\\beta}) \\leq \\text{Var}(c^T \\boldsymbol{y})$$ \u8bc1\u660e\uff1a TBD 3.3 Subset Selection \u6700\u5c0f\u4e8c\u4e58\u6cd5\u7684\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a \u9884\u6d4b\u7cbe\u5ea6\u3002\u867d\u7136\u5b83\u662f\u65e0\u504f\u7684\uff0c\u4f46\u662f\u65b9\u5dee\u5f88\u5927\u3002\u5982\u679c\u6211\u4eec\u5ffd\u7565\u4e00\u90e8\u5206\u6a21\u578b\u53c2\u6570\uff0c\u867d\u7136\u4f1a\u53d8\u6210\u6709\u504f\u4f30\u8ba1\uff0c\u4f46\u662f\u53ef\u80fd\u4f1a\u6781\u5927\u63d0\u9ad8\u7cbe\u5ea6\u3002 \u53ef\u89e3\u91ca\u6027\uff08\u5373\u6a21\u578b\u590d\u6742\u5ea6\uff09\u3002\u5f53\u6a21\u578b\u53c2\u6570\u5f88\u591a\u65f6\uff0c\u6211\u4eec\u60f3\u53bb\u786e\u5b9a\u4e00\u5c0f\u90e8\u5206\u5177\u6709\u6700\u5927\u5f71\u54cd\u7684\u6a21\u578b\u53c2\u6570\uff0c\u4e3a\u6b64\u6211\u4eec\u613f\u610f\u727a\u7272\u4e00\u90e8\u5206\u65e0\u5173\u7d27\u8981\u7684\u53c2\u6570\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u9009\u53d6\u53d8\u91cf\u5b50\u96c6\uff0c\u5373\u201cmodel selection\u201d\u3002 3.3.1 Best-Subset Selection \u6700\u4f73\u5b50\u96c6\u662f\u6307\u4ece\u6240\u6709\u5177\u6709 $k (k <= p)$ \u4e2a\u53d8\u91cf\u7684\u5b50\u96c6\u4e2d\uff0cRSS \u6700\u5c0f\u7684\u90a3\u4e2a\u3002 \u5f53\u7136\uff0c\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u5c31\u662f\u4ece\u904d\u5386\u6240\u6709\u7684\u7ec4\u5408\u3002\u8fd9\u6837\u505a\u7684\u590d\u6742\u5ea6\u662f $2^p$\uff0c\u53ea\u9002\u7528\u4e8e\u5c0f\u89c4\u6a21\u7684\u95ee\u9898\u3002 3.3.2 Forward- and Backward-Stepwise Selection \u201c\u524d\u5411\u9010\u6b65\u9009\u62e9\u201d\u662f\u4e00\u79cd\u8d2a\u5fc3\u7b97\u6cd5\u3002\u5b83\u6309\u987a\u5e8f\u52a0\u5165\u6700\u80fd\u63d0\u9ad8\u62df\u5408\u5ea6\u7684\u53c2\u6570\u3002\u5b83\u867d\u7136\u4e0d\u4e00\u5b9a\u627e\u5230\u6700\u4f18\u89e3\uff0c\u4f46\u662f\u5b83\u4f18\u52bf\u5728\u4e8e\uff1a \u8fd0\u7b97\u91cf\u5c0f\u3002\u5f53\u7ef4\u5ea6 $p >= 40$ \u65f6\uff0c\u51e0\u4e4e\u65e0\u6cd5\u7b97\u51fa\u6700\u4f18\u89e3\u3002\u4f46\u662f\u4f9d\u65e7\u53ef\u4ee5\u7528 forward stepwise selection \uff08\u5373\u4f7f\u7ef4\u5ea6 p \u5927\u4e8e\u6837\u672c\u6570 N\uff09\u3002 \u65b9\u5dee\u5c0f\u3002\u6700\u4f18\u5b50\u96c6\u65b9\u5dee\u6bd4 forward stepwise selection \u5927\uff0c\u867d\u7136\u540e\u8005\u53ef\u80fd\u4f1a\u6709\u4e00\u5b9a\u7684 bias\u3002 \u90a3\u4e48\u5982\u4f55\u9009\u62e9\u201c\u6700\u80fd\u63d0\u9ad8\u62df\u5408\u5ea6\u201c\u7684\u53c2\u6570\u5462\uff1f\u6211\u4eec\u5728\u4e4b\u524d\u201c\u663e\u8457\u6027\u5206\u6790\u201d\u4e2d\u5df2\u7ecf\u8bc1\u660e\u4e86\uff0c\u53bb\u6389\u4e00\u4e2a\u53c2\u6570\u5bf9\u6b8b\u5dee\u7684\u5f71\u54cd\u4e3a\u5176 z-score \u7684\u5e73\u65b9\u3002\u90a3\u4e48\uff0c\u6211\u4eec\u76f4\u63a5 \u4ece z-score \u6700\u5927\u7684\u53c2\u6570\u5f00\u59cb\u4f9d\u6b21\u52a0\u5165 \u5373\u53ef\u3002\u7b2c $j$ \u4e2a\u53c2\u6570\u7684 z-score \u53ef\u4ee5\u7531\u4e8e\u4e0b\u5f0f\u8ba1\u7b97\uff1a $$ z_j = \\frac{\\hat{\\beta}_j}{\\hat{\\sigma} \\sqrt{v_j} }$$ where: $$\\hat{\\sigma}^2 =\\frac{\\text{RSS} 1}{N-p-1} =\\frac{1}{N-p-1} \\sum {i=1}^{N} (y_i-\\hat{y})^2$$ $$v_j = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}_{jj}$$ \u201c\u540e\u5411\u9010\u6b65\u9009\u62e9\u201d \u4e0e \u201c\u524d\u5411\u9010\u6b65\u9009\u62e9\u201c\u76f8\u53cd\u3002\u5b83\u4ece\u5168\u96c6\u5f00\u59cb\uff0c\u4f9d\u6b21\u53bb\u6389\u6700\u65e0\u5173\u7d27\u8981\u7684\u53d8\u91cf\uff08z-score \u6700\u5c0f\u7684\uff09\u3002\u5b83\u53ea\u80fd\u7528\u4e8e\u6837\u672c\u6570 N \u5927\u4e8e\u7ef4\u5ea6 p \u7684\u60c5\u5f62\u3002 3.4 Shrinkage Methods Subset selection \u786e\u5b9e\u53ef\u4ee5\u5e2e\u6211\u4eec\u7b80\u5316\u6a21\u578b\uff0c\u5e76\u4e14\u8fd8\u53ef\u80fd\u964d\u4f4e\u8bef\u5dee\u3002\u4f46\u662f\uff0c\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u79bb\u6563\u7684\u8fc7\u7a0b\uff08\u53c2\u6570\u8981\u4e48\u88ab\u4e22\u5f03\u8981\u4e48\u88ab\u4fdd\u7559\uff0c\u6ca1\u6709\u4e2d\u95f4\u72b6\u6001\uff09\uff0c\u5b83\u901a\u5e38\u5177\u6709\u8f83\u5927\u7684\u65b9\u5dee\u3002Shrinkage methods \u66f4\u52a0\u8fde\u7eed\uff0c\u56e0\u6b64\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u3002 3.4.1 Ridge Regression Ridge Regression \u901a\u8fc7\u7ed9\u53c2\u6570\u6570\u91cf\u589e\u52a0\u4e00\u4e2a\u60e9\u7f5a\u9879\u6765\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u3002\u5b83\u7684\u4f18\u5316\u76ee\u6807\uff1a $$\\hat{\\beta} = \\mathop{\\arg \\min} {\\beta} \\sum {i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2$$ \u8fd9\u91cc\u7684 $\\lambda$ \u63a7\u5236\u6a21\u578b\u201c\u7f29\u5c0f\u201d\u7684\u7a0b\u5ea6\uff0c$\\lambda$ \u8d8a\u5927\uff0c\u5f97\u5230\u7684\u6a21\u578b\u590d\u6742\u5ea6\u8d8a\u4f4e\u3002 \u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c \u60e9\u7f5a\u9879\u4e2d\u4e0d\u5305\u542b\u5e38\u6570\u9879 $\\beta_0$\uff0c\u5426\u5219\u6a21\u578b\u4e0d\u7a33\u5b9a\u3002\u5f53\u9009\u53d6 $y_i = y_i + c$ \u65f6\uff0c\u9884\u6d4b\u503c $\\hat{y}_i$ \u7684\u53d8\u5316\u91cf\u4e0d\u662f $c$\u3002 \u4e0e\u7ecf\u5178\u7684 Linear Regression \u4e0d\u540c\uff0cRidge Regression \u8981\u6c42\u8f93\u5165 $\\textbf{X}, \\textbf{y}$ \u662f\u7ecf\u8fc7\u4e86 \u4e2d\u5fc3\u5316 (centering) \u7684\u3002\u5e76\u4e14\uff0c\u8fd9\u91cc\u7684\u6a21\u578b\u53c2\u6570 $\\beta$ \u662f $p$ \u7ef4\u800c\u4e0d\u662f $p+1$ \u7ef4\u7684\u3002 \u4e0b\u9762\u6211\u4eec\u6765\u8bc1\u660e\u8fd9\u4e00\u70b9\u3002 $\\beta_0$ \u7531\u4e8e\u4e0d\u542b $\\lambda$\uff0c\u53ef\u4ee5\u5355\u72ec\u4f18\u5316\u3002\u6211\u4eec\u5148\u5bf9 $\\beta_0$ \u6c42\u5bfc\uff0c\u5e76\u4ee4\u5bfc\u6570\u4e3a0: $$\\sum_{i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j) = 0$$ \u5f97\u5230\uff1a $$\\beta_0 = \\frac{1}{N}(\\sum_{i=1}^N y_i - \\sum_{i=1}^N \\sum_{j=1}^{p} x_{ij}\\beta_j) $$ \u4ee4 $\\overline{x_j} = \\frac{1}{N} \\sum_{i=1}^N x_{ij}$\uff0c\u6709\uff1a $$\\beta_0 = \\frac{1}{N}\\sum_{i=1}^N y_i - \\sum_{j=1}^{p} \\overline{x_{j}} \\beta_j $$ \u6211\u4eec\u4ee5\u4e0b\u7684\u53d8\u5f62\u4e3b\u8981\u662f\u4e3a\u4e86\u5c06\u4f18\u5316\u76ee\u6807\u51fd\u6570\u5199\u6210\u77e9\u9635\u4e58\u6cd5\u5f62\u5f0f\uff0c\u4ee5\u8fdb\u884c\u8fd0\u7b97\u3002 $$\\begin{align} \\hat{\\beta} &= \\mathop{\\arg \\min} {\\beta} \\sum {i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\ &= \\mathop{\\arg \\min} {\\beta} \\sum {i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p \\overline{x_j}\\beta_j - \\sum_{j=1}^p (x_{ij} - \\overline{x_j}) \\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\end{align}$$ \u73b0\u5728\u6211\u4eec\u4ee4\uff1a $$\\begin{align} \\beta_0^c &= \\beta_0 + \\sum_{j=1}^p \\overline{x_j}\\beta_j =\\frac{1}{N} \\sum_{i=1}^N y_{i} \\ \\beta_j^c&= \\beta_j & (j>=1) \\end{align}$$ \u53ef\u4ee5\u5f97\u51fa\uff1a $$\\begin{align} \\hat{\\beta} &= \\mathop{\\arg \\min} {\\beta^c} \\sum {i=1}^N(y_i - \\beta_0^c - \\sum_{j=1}^p (x_{ij} - \\overline{x_j}) \\beta_j^c)^2 + \\lambda \\sum_{j=1}^p {\\beta_j^c}^2 \\end{align}$$ \u6211\u4eec\u518d\u4ee4\uff1a $$\\begin{align} y_i^c &= y_i - \\beta_0^c = y_i - \\frac{1}{N} \\sum_{i=1}^N y_i \\ x_{ij}^c&= x_{ij} - \\overline{x_j} & (j >=1) \\end{align}$$ \u6709\uff1a $$\\begin{align} \\hat{\\beta} &= \\mathop{\\arg \\min} {\\beta^c} \\sum {i=1}^N(y_i^c - \\sum_{j=1}^p (x_{ij}^c \\beta_j^c)^2) + \\lambda \\sum_{j=1}^p {\\beta_j^c}^2 \\ &=\\mathop{\\arg \\min}_{\\beta} (\\textbf{y} - \\textbf{X}\\beta)^T(\\textbf{y} - \\textbf{X}\\beta) + \\lambda(\\beta^T\\beta) \\end{align}$$ \u5176\u4e2d\uff0c$\\textbf{X}, \\textbf{y}, \\beta$ \u90fd\u7ecf\u8fc7\u4e86\u4e2d\u5fc3\u5316\uff0c\u5e76\u4e14\u662f $p$ \u7ef4\u7684\u3002 \u8be5\u5f0f\u5bf9 $\\beta$ \u6c42\u5bfc\u5e76\u4ee4\u5bfc\u6570\u4e3a 0\uff0c\u6709\uff1a $$ -\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\beta) + \\lambda \\beta = 0$$ \u89e3\u5f97\uff1a $$ \\beta = (\\textbf{X}^T\\textbf{X} + \\lambda \\textbf{I})^{-1} \\textbf{X}^T \\textbf{y}$$ \u6211\u4eec\u770b\u5230\uff0c\u5373\u4f7f $\\textbf{X}^T\\textbf{X}$ \u662f\u975e\u6ee1\u79e9\u7684\uff0c\u7531\u4e8e\u591a\u52a0\u4e86\u4e00\u4e2a $\\lambda \\textbf{I}$\uff0c\u5b83\u4ecd\u662f\u4e00\u4e2a\u53ef\u9006\u77e9\u9635\u3002\u8fd9\u4e5f\u662f ridge regression \u7684\u53e6\u4e00\u4e2a\u4f18\u52bf\u3002 Reference ESL solution ESL Chinese Simple Linear Regression Proofs involving ordinary least squares","title":"ESL3 LinearMethodsForRegression"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#32-linear-regression-models-and-least-squares","text":"$p$ \u7ef4\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u5f62\u5f0f\u5982\u4e0b\uff1a $$f(X) = \\beta_0 + \\sum_{j=1}^p X_j \\beta_j$$ \u6211\u4eec\u9700\u8981\u4f30\u8ba1\u4e00\u7ec4\u53c2\u6570 $\\beta$\uff0c\u4f7f\u6b8b\u5dee\u5e73\u65b9\u548c\uff08Residual Sum of Squares\uff09\u6700\u5c0f\uff1a $$\\begin{align} \\text{RSS}(\\boldsymbol{\\beta}) &= (\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} )^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} ) \\ &= \\boldsymbol{y}^T\\boldsymbol{y} - \\boldsymbol{y}^T\\boldsymbol{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\boldsymbol{X}^T\\boldsymbol{y} + \\boldsymbol{\\beta}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\beta} \\end{align}$$ \u5176\u4e2d\uff0c$ \\boldsymbol{X} $ \u662f\u4e00\u4e2a $N \\times (p+1)$ \u77e9\u9635\uff0c$\\boldsymbol{y}$ \u662f $N \\times 1$ \u89c2\u6d4b\u503c\u3002 \u5bf9 $\\beta$ \u6c42\u5bfc\u53ef\u4ee5\u5f97\u5230\uff1a $$ \\frac{\\partial \\text{RSS}(\\beta)}{\\partial \\beta} = -2 \\boldsymbol{X}^T\\boldsymbol{y} + 2\\boldsymbol{X}^T\\boldsymbol{X} \\boldsymbol{\\beta}$$ \u7531\u4e8e\u4e8c\u9636\u5bfc\u6570\u6b63\u5b9a\uff0c\u4ee4\u4e00\u9636\u5bfc\u6570\u4e3a 0 \u5411\u91cf\uff0c\u5f97\u51fa\u6781\u503c\u70b9\uff08\u5373\u4f30\u8ba1\u503c\uff09\uff1a $$ \\hat{\\beta}= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}$$ $$\\hat{\\boldsymbol{y}} = \\boldsymbol{X} \\hat{\\beta} = \\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}$$ \u6211\u4eec\u79f0 $\\boldsymbol{H} = \\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T$ \u4e3a\u4f30\u8ba1\u77e9\u9635\uff08\"hat\" matrix\uff09\uff0c\u5b83\u6ee1\u8db3\u5bf9\u79f0\u6027\u548c\u5e42\u7b49\u6027\uff1a $$\\boldsymbol{H}^T = \\boldsymbol{H}$$ $$\\boldsymbol{H}^T\\boldsymbol{H} = \\boldsymbol{H}$$ \u5f53 $\\boldsymbol{X}$ \u4e2d\u67d0\u4e9b\u5217\u7ebf\u6027\u76f8\u5173\uff08\u5373\u975e\u6ee1\u79e9\u77e9\u9635\uff09\u65f6\uff0c$(\\boldsymbol{X}^T\\boldsymbol{X})$ \u662f\u5947\u5f02\u77e9\u9635\uff0c\u5b83\u53ea\u80fd\u6c42\u5e7f\u4e49\u9006\u77e9\u9635\uff0c\u4e0d\u6b62\u4e00\u4e2a\u89e3\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5c06\u5197\u4f59\u7684\u8f93\u5165\u5254\u9664\u6389\uff0c\u5927\u90e8\u5206\u6c42\u89e3\u8f6f\u4ef6\u90fd\u5b9e\u73b0\u4e86\u8fd9\u4e2a\u529f\u80fd\u3002","title":"3.2 Linear Regression Models and Least Squares"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#_1","text":"\u4e3a\u4e86\u786e\u5b9a\u4f30\u8ba1\u7684\u53c2\u6570 $\\boldsymbol{\\hat{\\beta}}$ \u7684\u7edf\u8ba1\u7279\u6027\uff0c\u6211\u4eec\u5047\u8bbe\uff1a \u6bcf\u4e2a\u89c2\u6d4b\u503c $y_i$ \u76f8\u4e92\u72ec\u7acb $y_i$\u6709\u56fa\u5b9a\u7684\u566a\u58f0 $\\varepsilon \\sim N(0, \\sigma^2)$ \u90a3\u4e48\u4f30\u8ba1\u503c $\\hat{\\beta}$ \u7684\u65b9\u5dee\u4e3a\uff1a $$ \\text{Var}(\\hat{\\beta}) = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\sigma^2$$ where : $$\\hat{\\sigma}^2 = \\frac{\\text{RSS}}{N-p-1}= \\frac{1}{N-p-1} \\sum_{i=1}^{N} (y_i-\\hat{y})^2$$","title":"\u4f30\u8ba1\u53c2\u6570\u7684\u7edf\u8ba1\u7279\u6027"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#_2","text":"N \u4e2a y \u7684\u89c2\u6d4b\u503c\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a $$ \\boldsymbol{y} = \\boldsymbol{X}\\beta + \\boldsymbol{\\varepsilon}$$ \u5176\u4e2d $ \\boldsymbol{\\varepsilon} $ \u662f $N \\times 1$ \u7684\u566a\u58f0\u3002\u56e0\u6b64\u6709\uff1a $$\\begin{align} \\hat{\\beta} &= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y} \\ &= \\beta + (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{\\varepsilon} \\end{align}$$ \u65e0\u504f\u6027\uff08\u671f\u671b\u503c\u4e3a $\\beta$\uff09\uff1a $$E(\\hat{\\beta}) = \\beta + (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T E(\\boldsymbol{\\varepsilon}) = \\beta$$ \u534f\u65b9\u5dee\u77e9\u9635\uff08\u6ce8\u610f\u662f$\\beta \\beta^T$ \u800c\u975e $\\beta^T \\beta$\uff0c\u662f\u4e00\u4e2a\u77e9\u9635\uff09\uff1a $$\\begin{align} \\text{Var}(\\hat{\\beta}) &= E[(\\beta - \\hat{\\beta})(\\beta - \\hat{\\beta})^T] \\ &=E[(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^T\\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}] \\ &= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T E(\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^T) \\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\ &= \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{I} \\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\ &= \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\end{align}$$ \u53ef\u4ee5\u5f97\u5230\uff1a $$ \\hat{\\beta} \\sim N(\\beta, \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1})$$ \u4e0b\u9762\u6765\u786e\u5b9a $\\sigma^2$ \u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u89c2\u6d4b\u503c $y$ \u548c\u9884\u6d4b\u503c $\\hat{y}$ \u7684\u5dee\u6765\u5f97\u5230\u566a\u58f0 $\\varepsilon$\u3002 $$\\begin{align} \\boldsymbol{y - \\hat{y}} &= \\boldsymbol{X}\\beta + \\boldsymbol{\\varepsilon} -\\boldsymbol{X}\\hat{\\beta} \\ &= \\boldsymbol{X}\\beta + \\boldsymbol{\\varepsilon} - \\boldsymbol{X}(\\beta + (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{\\varepsilon}) \\ &= (\\boldsymbol{I -H} )\\boldsymbol{\\varepsilon} \\end{align}$$ $$\\begin{align} \\sum_{i=1}^N(y_i - \\hat{y_i})^2 &= (\\boldsymbol{y - \\hat{y}})^T (\\boldsymbol{y - \\hat{y}}) \\ &= \\boldsymbol{\\varepsilon}^T(\\boldsymbol{I - H}) \\boldsymbol{\\varepsilon} \\ &= \\sum_{k =1}^N \\varepsilon_k^2- \\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij} \\end{align}$$ \u5176\u671f\u671b\u503c\u4e3a\uff1a $$\\begin{align} E[\\sum_{i=1}^N(y_i - \\hat{y_i})^2] &= E[\\sum_{k =1}^N \\varepsilon_k^2- \\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij} ] \\ &= N\\sigma^2 - E(\\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij}) \\end{align}$$ \u7531\u4e8e $\\varepsilon_i, \\varepsilon_j$ \u662f\u72ec\u7acb\u7684\uff0c\u5f53 $i \\neq j$ \u65f6\uff1a $$\\text{Cov}(\\varepsilon_i, \\varepsilon_j) = E(\\varepsilon_i \\varepsilon_j) - E(\\varepsilon_i)E(\\varepsilon_j) = 0$$ \u56e0\u6b64\uff1a $$\\begin{align} E[\\sum_{i=1}^N(y_i - \\hat{y_i})^2] &= N\\sigma^2 - E(\\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij}) \\ &= N\\sigma^2 - E(\\sum_{i=1}^{N}\\varepsilon_i^2H_{ii}) \\ &= \\sigma^2[N - \\text{trace}(\\boldsymbol{H})] \\end{align}$$ \u8fd9\u91cc\u518d\u5229\u7528\u516c\u5f0f\uff1a $$\\text{trace}(ABC) = \\text{trace}(CAB) $$ \u5f97\u5230\uff1a $$\\begin{align} E[\\sum_{i=1}^N(y_i - \\hat{y_i})^2] &= \\sigma^2[N - \\text{trace}(\\boldsymbol{H})] \\ &= \\sigma^2[N - \\text{trace}(\\boldsymbol{X(X^TX)^{-1}X^T})] \\ &= \\sigma^2[N - \\text{trace}(\\boldsymbol{X^TX(X^TX)^{-1}} {(p+1) \\times (p+1)})] \\ &= \\sigma^2[N - \\text{trace}(\\boldsymbol{I} {(p+1) \\times (p+1)})] \\ &= \\sigma^2(N - p -1) \\end{align}$$ \u56e0\u6b64\uff0c\u5bf9 $\\sigma^2$ \u7684\u65e0\u504f\u4f30\u8ba1\u5c31\u662f\uff1a $$\\hat{\\sigma}^2 = \\frac{1}{N-p-1} \\sum_{i=1}^{N} (y_i-\\hat{y})^2$$","title":"\u8bc1\u660e"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#_3","text":"\u7531\u4e8e\u6211\u4eec\u5bf9\u7b2c i \u4e2a\u6837\u672c\u7684\u566a\u58f0 $\\varepsilon_i $ \u65e0\u504f\u4f30\u8ba1\u5c31\u662f $\\hat{\\varepsilon_i} = y_i - \\hat{y_i}$\uff0c\u6211\u4eec\u8ba1\u7b97\u5176\u65b9\u5dee\uff1a $$\\begin{align} \\text{Var}(\\hat{\\boldsymbol{\\varepsilon}}) &= \\text{Var}(\\boldsymbol{y} - \\hat{\\boldsymbol{y}}) \\ &= \\text{Var}[(\\boldsymbol{I} - \\boldsymbol{H}){\\boldsymbol{\\varepsilon}}] \\end{align}$$ \u7531\u4e8e $D(AX) = AD(X)A^T$\uff1a $$\\begin{align} \\text{Var}(\\hat{\\boldsymbol{\\varepsilon}}) &= \\text{Var}[(\\boldsymbol{I} - \\boldsymbol{H}){\\boldsymbol{\\varepsilon}}] \\ &= (\\boldsymbol{I} - \\boldsymbol{H}) \\text{Var}(\\boldsymbol{\\varepsilon}) (\\boldsymbol{I} - \\boldsymbol{H}) \\end{align}$$ \u7531\u4e8e $\\varepsilon \\sim N(0, \\sigma^2)$\uff0c\u56e0\u6b64\uff1a $$\\text{Var}(\\boldsymbol{\\varepsilon}) = \\sigma^2 \\boldsymbol{I}_{N \\times N}$$ \u800c $\\boldsymbol{H} = \\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T$ \u6ee1\u8db3\u5bf9\u79f0\u6027\u548c\u5e42\u7b49\u6027\uff1a $$\\boldsymbol{H}^T = \\boldsymbol{H}$$ $$\\boldsymbol{H}^T\\boldsymbol{H} = \\boldsymbol{H}$$ \u56e0\u6b64\u6709\u7ed3\u8bba\uff1a $$\\text{Var}(\\hat{\\boldsymbol{\\varepsilon}}) = \\sigma^2 (\\boldsymbol{I} - \\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T)$$","title":"\u6a21\u578b\u8bef\u5dee\u7684\u7edf\u8ba1\u7279\u6027"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#_4","text":"\u5f53\u6211\u4eec\u5224\u65ad\u54ea\u4e9b\u53c2\u6570\u53ef\u4ee5\u5ffd\u7565\u4ee5\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 F-statistic \u8fdb\u884c\u663e\u8457\u6027\u5206\u6790\u3002\u5047\u8bbe\u6211\u4eec\u5c06 $\\beta$ \u7ef4\u5ea6\u4ece $p_1 + 1$ \u964d\u4f4e\u5230 $p_0 + 1$\uff1a $$ F = \\frac{(\\text{RSS}_0 - \\text{RSS}_1) / (p_1 - p_0)}{\\text{RSS}_1 / (N- p_1 -1)} $$ F-statistic \u63cf\u8ff0\u4e86\u6bcf\u4e2a\u88ab\u5ffd\u7565\u7684\u53c2\u6570\u5bf9 RSS \u7684\u5e73\u5747\u8d21\u732e\uff0c\u7528 $\\hat{\\sigma}^2$ \u8fdb\u884c\u4e86 normalize\u3002 \u5f53 $p_1 - p_0 =1$ \u5373\u4ec5\u53bb\u6389\u4e00\u4e2a\u53c2\u6570\u65f6\uff08\u5047\u8bbe $\\beta_j = 0$\uff09\uff0c\u8be5\u516c\u5f0f\u53ef\u4ee5\u7b80\u5316\u4e3a\u5bf9\u5e94\u7684 z-score \u7684\u5e73\u65b9\uff0c\u5176\u4e2d z-score \u4e3a\uff1a $$ z_j = \\frac{\\hat{\\beta}_j}{\\hat{\\sigma} \\sqrt{v_j} }$$ where: $$\\hat{\\sigma}^2 =\\frac{\\text{RSS} 1}{N-p-1} =\\frac{1}{N-p-1} \\sum {i=1}^{N} (y_i-\\hat{y})^2$$ $$v_j = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}_{jj}$$","title":"\u663e\u8457\u6027\u5206\u6790"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#_5","text":"\u8fd9\u4e2a\u8bc1\u660e\u540c\u65f6\u4e5f\u662f\u4e60\u9898 3.1 Ex. 3.1 Show that the F statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding z-score (3.12). \u5b9e\u9645\u4e0a\u6211\u4eec\u9700\u8981\u8bc1\u660e\uff0c\u5728\u53bb\u6389\u6a21\u578b\u7684\u7b2c j \u4e2a\u53c2\u6570\u540e\uff1a $$ \\text{RSS}_0 - \\text{RSS}_1 = \\frac{\\hat{\\beta}_j^2}{v_j} $$ \u4e0a\u5f0f\u4e2d\u552f\u4e00\u672a\u77e5\u7684\u5c31\u662f $\\text{RSS}_0$\uff0c\u5b83\u5b9e\u8d28\u4e0a\u662f\u6c42\u4e00\u4e2a\u5e26\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff1a $$\\begin{align} \\min_{\\beta \\in \\mathbb{R}^{(p+1) \\times 1}} (\\textbf{y} - \\textbf{X}\\beta)^T(\\textbf{y}-\\textbf{X}\\beta) \\ \\text{s.t.} ~\\beta_j = 0 \\end{align}$$ \u6211\u4eec\u53ef\u4ee5\u7528\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u6cd5\u6765\u89e3\u51b3\u3002 $$L(\\beta, \\lambda) = (\\textbf{y} - \\textbf{X}\\beta)^T(\\textbf{y}-\\textbf{X}\\beta) + \\lambda e_j^T \\beta $$ \u5bf9 $\\beta$ \u6c42\u5bfc\uff0c\u5e76\u4ee4\u5bfc\u6570\u4e3a 0\uff0c\u6709\uff1a $$\\frac{\\partial L(\\beta, \\lambda)}{\\partial \\beta} = - 2\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\beta) + \\lambda e_j = 0$$ \u89e3\u51fa\uff1a $$\\begin{align} \\beta_0 &= (\\textbf{X}^T\\textbf{X})^{-1} \\textbf{X}^T\\textbf{y} - \\frac{\\lambda}{2}(\\textbf{X}^T \\textbf{X})^{-1} e_j \\ &= \\hat{\\beta}- \\frac{\\lambda}{2}(\\textbf{X}^T \\textbf{X})^{-1} e_j \\end{align}$$ \u7b49\u5f0f\u4e24\u8fb9\u4e58\u4ee5 $e_j^T$\uff0c\u5e76\u5e26\u5165$\\beta_j = 0$\uff0c\u6709\uff1a $$\\begin{align} e_j^T\\beta_0 = 0 &= e_j^T \\hat{\\beta} + \\frac{\\lambda}{2} e_j^T(\\textbf{X}^T \\textbf{X})^{-1} e_j \\ &= \\hat{\\beta}_j + \\frac{\\lambda}{2}v_j \\end{align}$$ \u56e0\u6b64\u6709\uff1a $$\\lambda = - \\frac{2\\hat{\\beta}_j}{v_j}$$ \u5e26\u5165\u53ef\u5f97\uff1a $$\\begin{align} \\text{RSS}_0 &= (\\textbf{y} - \\textbf{X}\\beta_0)^T(\\textbf{y}-\\textbf{X}\\beta_0) \\ &= (\\textbf{y} - \\textbf{X}\\hat{\\beta} + \\frac{\\lambda}{2}\\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1} e_j)^T(\\textbf{y}-\\textbf{X}\\hat{\\beta} + \\frac{\\lambda}{2}\\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1} e_j) \\ &= \\text{RSS}_1 + \\frac{\\lambda}{2} [e_j^T(\\textbf{X}^T \\textbf{X})^{-1}\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\hat{\\beta}) + (\\textbf{y} - \\textbf{X}\\hat{\\beta})^T \\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1} e_j)] \\ &~~~~ + \\frac{\\lambda^2}{4}e_j^T (\\textbf{X}^T \\textbf{X})^{-1} e_j \\ &= \\text{RSS}_1 + \\frac{\\lambda^2}{4}e_j^T (\\textbf{X}^T \\textbf{X})^{-1} e_j \\ &= \\text{RSS}_1 + \\frac{\\hat{\\beta}_j^2}{v_j} \\end{align}$$ \u5176\u4e2d\uff0c\u4e2d\u95f4\u9879\u53ef\u4ee5\u6d88\u53bb\u7684\u539f\u56e0\u662f\uff1a $$\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\hat{\\beta}) = \\textbf{X}^T[\\textbf{y} - \\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}] = 0 $$ \u76f4\u89c2\u7406\u89e3\uff0c$\\textbf{X}$ \u548c $\\textbf{y} - \\textbf{X}\\hat{\\beta}$ \u662f\u6b63\u4ea4\u7684\uff0c\u56e0\u4e3a $\\textbf{X}\\hat{\\beta}$ \u6b63\u662f $\\textbf{y}$ \u5728 $\\textbf{X}$ \u6240\u5728\u5e73\u9762\u4e0a\u7684\u6295\u5f71\u3002","title":"\u8bc1\u660e"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#322-the-gaussmarkov-theorem","text":"\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5f97\u51fa\u7684 $\\beta$ \u5728\u6240\u6709\u7ebf\u6027 \u65e0\u504f \u4f30\u8ba1\u4e2d\u5747\u65b9\u8bef\u5dee\u6700\u5c0f\u3002\u5f53\u7136\uff0c\u5982\u679c\u6211\u4eec\u613f\u610f\u4e3a\u4e86\u8fdb\u4e00\u6b65\u51cf\u5c0f\u8bef\u5dee\u5f15\u5165\u4e00\u70b9 bias\uff0c\u5b8c\u5168\u53ef\u80fd\u627e\u5230\u4e00\u4e2a\u66f4\u5c0f\u5747\u65b9\u8bef\u5dee\u7684 \u6709\u504f \u4f30\u8ba1\u3002 the least squares estimates of the parameters \u03b2 have the smallest variance among all linear unbiased estimates \u73b0\u5728\u6211\u4eec\u6765\u8bc1\u660e\u8fd9\u4e2a\u7ed3\u8bba\u3002\u5bf9\u4e8e\u7ebf\u6027\u4f30\u8ba1\uff1a $$\\boldsymbol{y} = \\boldsymbol{X}\\beta$$ $\\boldsymbol{y}$ \u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u90fd\u53ef\u4ee5\u770b\u4f5c $\\boldsymbol{X}$ \u4e2d\u7684\u4e00\u884c\u4e0e\u5411\u91cf $\\beta$ \u7684\u7ebf\u6027\u7ec4\u5408\u3002","title":"3.2.2 The Gauss\u2013Markov Theorem"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#_6","text":"\u90a3\u4e48\uff0c\u9488\u5bf9\u65e0\u504f\u6027\uff0c\u6211\u4eec\u9700\u8981\u8bc1\u660e\u6700\u5c0f\u4e8c\u4e58\u6cd5\u4f30\u8ba1\u51fa\u7684 $\\hat{\\beta}$ \u6ee1\u8db3\uff1a $$ E(\\alpha^T \\hat{\\beta}) = \\alpha^T\\beta$$ \u5176\u4e2d $\\alpha$ \u662f\u4efb\u610f\u5411\u91cf\u3002 $$\\begin{align} E(\\alpha^T \\hat{\\beta}) &= E(\\alpha^T (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}) \\ &= E(\\alpha^T (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{X} \\beta) \\ &= \\alpha^T \\beta \\end{align} $$","title":"\u65e0\u504f\u6027"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#_7","text":"Gauss\u2013Markov theorem \u6307\u51fa\uff0c\u5982\u679c\u8fd8\u5b58\u5728\u5176\u4ed6\u7ebf\u6027\u4f30\u8ba1 $c^T \\boldsymbol{y}$ \u6ee1\u8db3\uff1a $$ E(c^T \\boldsymbol{y}) = \\alpha^T\\beta$$ \u90a3\u4e48\u5fc5\u7136\u6709\uff1a $$\\text{Var}(\\alpha^T \\hat{\\beta}) \\leq \\text{Var}(c^T \\boldsymbol{y})$$ \u8bc1\u660e\uff1a TBD","title":"\u5747\u65b9\u8bef\u5dee\u6700\u5c0f"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#33-subset-selection","text":"\u6700\u5c0f\u4e8c\u4e58\u6cd5\u7684\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a \u9884\u6d4b\u7cbe\u5ea6\u3002\u867d\u7136\u5b83\u662f\u65e0\u504f\u7684\uff0c\u4f46\u662f\u65b9\u5dee\u5f88\u5927\u3002\u5982\u679c\u6211\u4eec\u5ffd\u7565\u4e00\u90e8\u5206\u6a21\u578b\u53c2\u6570\uff0c\u867d\u7136\u4f1a\u53d8\u6210\u6709\u504f\u4f30\u8ba1\uff0c\u4f46\u662f\u53ef\u80fd\u4f1a\u6781\u5927\u63d0\u9ad8\u7cbe\u5ea6\u3002 \u53ef\u89e3\u91ca\u6027\uff08\u5373\u6a21\u578b\u590d\u6742\u5ea6\uff09\u3002\u5f53\u6a21\u578b\u53c2\u6570\u5f88\u591a\u65f6\uff0c\u6211\u4eec\u60f3\u53bb\u786e\u5b9a\u4e00\u5c0f\u90e8\u5206\u5177\u6709\u6700\u5927\u5f71\u54cd\u7684\u6a21\u578b\u53c2\u6570\uff0c\u4e3a\u6b64\u6211\u4eec\u613f\u610f\u727a\u7272\u4e00\u90e8\u5206\u65e0\u5173\u7d27\u8981\u7684\u53c2\u6570\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u9009\u53d6\u53d8\u91cf\u5b50\u96c6\uff0c\u5373\u201cmodel selection\u201d\u3002","title":"3.3 Subset Selection"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#331-best-subset-selection","text":"\u6700\u4f73\u5b50\u96c6\u662f\u6307\u4ece\u6240\u6709\u5177\u6709 $k (k <= p)$ \u4e2a\u53d8\u91cf\u7684\u5b50\u96c6\u4e2d\uff0cRSS \u6700\u5c0f\u7684\u90a3\u4e2a\u3002 \u5f53\u7136\uff0c\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u5c31\u662f\u4ece\u904d\u5386\u6240\u6709\u7684\u7ec4\u5408\u3002\u8fd9\u6837\u505a\u7684\u590d\u6742\u5ea6\u662f $2^p$\uff0c\u53ea\u9002\u7528\u4e8e\u5c0f\u89c4\u6a21\u7684\u95ee\u9898\u3002","title":"3.3.1 Best-Subset Selection"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#332-forward-and-backward-stepwise-selection","text":"\u201c\u524d\u5411\u9010\u6b65\u9009\u62e9\u201d\u662f\u4e00\u79cd\u8d2a\u5fc3\u7b97\u6cd5\u3002\u5b83\u6309\u987a\u5e8f\u52a0\u5165\u6700\u80fd\u63d0\u9ad8\u62df\u5408\u5ea6\u7684\u53c2\u6570\u3002\u5b83\u867d\u7136\u4e0d\u4e00\u5b9a\u627e\u5230\u6700\u4f18\u89e3\uff0c\u4f46\u662f\u5b83\u4f18\u52bf\u5728\u4e8e\uff1a \u8fd0\u7b97\u91cf\u5c0f\u3002\u5f53\u7ef4\u5ea6 $p >= 40$ \u65f6\uff0c\u51e0\u4e4e\u65e0\u6cd5\u7b97\u51fa\u6700\u4f18\u89e3\u3002\u4f46\u662f\u4f9d\u65e7\u53ef\u4ee5\u7528 forward stepwise selection \uff08\u5373\u4f7f\u7ef4\u5ea6 p \u5927\u4e8e\u6837\u672c\u6570 N\uff09\u3002 \u65b9\u5dee\u5c0f\u3002\u6700\u4f18\u5b50\u96c6\u65b9\u5dee\u6bd4 forward stepwise selection \u5927\uff0c\u867d\u7136\u540e\u8005\u53ef\u80fd\u4f1a\u6709\u4e00\u5b9a\u7684 bias\u3002 \u90a3\u4e48\u5982\u4f55\u9009\u62e9\u201c\u6700\u80fd\u63d0\u9ad8\u62df\u5408\u5ea6\u201c\u7684\u53c2\u6570\u5462\uff1f\u6211\u4eec\u5728\u4e4b\u524d\u201c\u663e\u8457\u6027\u5206\u6790\u201d\u4e2d\u5df2\u7ecf\u8bc1\u660e\u4e86\uff0c\u53bb\u6389\u4e00\u4e2a\u53c2\u6570\u5bf9\u6b8b\u5dee\u7684\u5f71\u54cd\u4e3a\u5176 z-score \u7684\u5e73\u65b9\u3002\u90a3\u4e48\uff0c\u6211\u4eec\u76f4\u63a5 \u4ece z-score \u6700\u5927\u7684\u53c2\u6570\u5f00\u59cb\u4f9d\u6b21\u52a0\u5165 \u5373\u53ef\u3002\u7b2c $j$ \u4e2a\u53c2\u6570\u7684 z-score \u53ef\u4ee5\u7531\u4e8e\u4e0b\u5f0f\u8ba1\u7b97\uff1a $$ z_j = \\frac{\\hat{\\beta}_j}{\\hat{\\sigma} \\sqrt{v_j} }$$ where: $$\\hat{\\sigma}^2 =\\frac{\\text{RSS} 1}{N-p-1} =\\frac{1}{N-p-1} \\sum {i=1}^{N} (y_i-\\hat{y})^2$$ $$v_j = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}_{jj}$$ \u201c\u540e\u5411\u9010\u6b65\u9009\u62e9\u201d \u4e0e \u201c\u524d\u5411\u9010\u6b65\u9009\u62e9\u201c\u76f8\u53cd\u3002\u5b83\u4ece\u5168\u96c6\u5f00\u59cb\uff0c\u4f9d\u6b21\u53bb\u6389\u6700\u65e0\u5173\u7d27\u8981\u7684\u53d8\u91cf\uff08z-score \u6700\u5c0f\u7684\uff09\u3002\u5b83\u53ea\u80fd\u7528\u4e8e\u6837\u672c\u6570 N \u5927\u4e8e\u7ef4\u5ea6 p \u7684\u60c5\u5f62\u3002","title":"3.3.2 Forward- and Backward-Stepwise Selection"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#34-shrinkage-methods","text":"Subset selection \u786e\u5b9e\u53ef\u4ee5\u5e2e\u6211\u4eec\u7b80\u5316\u6a21\u578b\uff0c\u5e76\u4e14\u8fd8\u53ef\u80fd\u964d\u4f4e\u8bef\u5dee\u3002\u4f46\u662f\uff0c\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u79bb\u6563\u7684\u8fc7\u7a0b\uff08\u53c2\u6570\u8981\u4e48\u88ab\u4e22\u5f03\u8981\u4e48\u88ab\u4fdd\u7559\uff0c\u6ca1\u6709\u4e2d\u95f4\u72b6\u6001\uff09\uff0c\u5b83\u901a\u5e38\u5177\u6709\u8f83\u5927\u7684\u65b9\u5dee\u3002Shrinkage methods \u66f4\u52a0\u8fde\u7eed\uff0c\u56e0\u6b64\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u3002","title":"3.4 Shrinkage Methods"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#341-ridge-regression","text":"Ridge Regression \u901a\u8fc7\u7ed9\u53c2\u6570\u6570\u91cf\u589e\u52a0\u4e00\u4e2a\u60e9\u7f5a\u9879\u6765\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u3002\u5b83\u7684\u4f18\u5316\u76ee\u6807\uff1a $$\\hat{\\beta} = \\mathop{\\arg \\min} {\\beta} \\sum {i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2$$ \u8fd9\u91cc\u7684 $\\lambda$ \u63a7\u5236\u6a21\u578b\u201c\u7f29\u5c0f\u201d\u7684\u7a0b\u5ea6\uff0c$\\lambda$ \u8d8a\u5927\uff0c\u5f97\u5230\u7684\u6a21\u578b\u590d\u6742\u5ea6\u8d8a\u4f4e\u3002 \u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c \u60e9\u7f5a\u9879\u4e2d\u4e0d\u5305\u542b\u5e38\u6570\u9879 $\\beta_0$\uff0c\u5426\u5219\u6a21\u578b\u4e0d\u7a33\u5b9a\u3002\u5f53\u9009\u53d6 $y_i = y_i + c$ \u65f6\uff0c\u9884\u6d4b\u503c $\\hat{y}_i$ \u7684\u53d8\u5316\u91cf\u4e0d\u662f $c$\u3002 \u4e0e\u7ecf\u5178\u7684 Linear Regression \u4e0d\u540c\uff0cRidge Regression \u8981\u6c42\u8f93\u5165 $\\textbf{X}, \\textbf{y}$ \u662f\u7ecf\u8fc7\u4e86 \u4e2d\u5fc3\u5316 (centering) \u7684\u3002\u5e76\u4e14\uff0c\u8fd9\u91cc\u7684\u6a21\u578b\u53c2\u6570 $\\beta$ \u662f $p$ \u7ef4\u800c\u4e0d\u662f $p+1$ \u7ef4\u7684\u3002 \u4e0b\u9762\u6211\u4eec\u6765\u8bc1\u660e\u8fd9\u4e00\u70b9\u3002 $\\beta_0$ \u7531\u4e8e\u4e0d\u542b $\\lambda$\uff0c\u53ef\u4ee5\u5355\u72ec\u4f18\u5316\u3002\u6211\u4eec\u5148\u5bf9 $\\beta_0$ \u6c42\u5bfc\uff0c\u5e76\u4ee4\u5bfc\u6570\u4e3a0: $$\\sum_{i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j) = 0$$ \u5f97\u5230\uff1a $$\\beta_0 = \\frac{1}{N}(\\sum_{i=1}^N y_i - \\sum_{i=1}^N \\sum_{j=1}^{p} x_{ij}\\beta_j) $$ \u4ee4 $\\overline{x_j} = \\frac{1}{N} \\sum_{i=1}^N x_{ij}$\uff0c\u6709\uff1a $$\\beta_0 = \\frac{1}{N}\\sum_{i=1}^N y_i - \\sum_{j=1}^{p} \\overline{x_{j}} \\beta_j $$ \u6211\u4eec\u4ee5\u4e0b\u7684\u53d8\u5f62\u4e3b\u8981\u662f\u4e3a\u4e86\u5c06\u4f18\u5316\u76ee\u6807\u51fd\u6570\u5199\u6210\u77e9\u9635\u4e58\u6cd5\u5f62\u5f0f\uff0c\u4ee5\u8fdb\u884c\u8fd0\u7b97\u3002 $$\\begin{align} \\hat{\\beta} &= \\mathop{\\arg \\min} {\\beta} \\sum {i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\ &= \\mathop{\\arg \\min} {\\beta} \\sum {i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p \\overline{x_j}\\beta_j - \\sum_{j=1}^p (x_{ij} - \\overline{x_j}) \\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\end{align}$$ \u73b0\u5728\u6211\u4eec\u4ee4\uff1a $$\\begin{align} \\beta_0^c &= \\beta_0 + \\sum_{j=1}^p \\overline{x_j}\\beta_j =\\frac{1}{N} \\sum_{i=1}^N y_{i} \\ \\beta_j^c&= \\beta_j & (j>=1) \\end{align}$$ \u53ef\u4ee5\u5f97\u51fa\uff1a $$\\begin{align} \\hat{\\beta} &= \\mathop{\\arg \\min} {\\beta^c} \\sum {i=1}^N(y_i - \\beta_0^c - \\sum_{j=1}^p (x_{ij} - \\overline{x_j}) \\beta_j^c)^2 + \\lambda \\sum_{j=1}^p {\\beta_j^c}^2 \\end{align}$$ \u6211\u4eec\u518d\u4ee4\uff1a $$\\begin{align} y_i^c &= y_i - \\beta_0^c = y_i - \\frac{1}{N} \\sum_{i=1}^N y_i \\ x_{ij}^c&= x_{ij} - \\overline{x_j} & (j >=1) \\end{align}$$ \u6709\uff1a $$\\begin{align} \\hat{\\beta} &= \\mathop{\\arg \\min} {\\beta^c} \\sum {i=1}^N(y_i^c - \\sum_{j=1}^p (x_{ij}^c \\beta_j^c)^2) + \\lambda \\sum_{j=1}^p {\\beta_j^c}^2 \\ &=\\mathop{\\arg \\min}_{\\beta} (\\textbf{y} - \\textbf{X}\\beta)^T(\\textbf{y} - \\textbf{X}\\beta) + \\lambda(\\beta^T\\beta) \\end{align}$$ \u5176\u4e2d\uff0c$\\textbf{X}, \\textbf{y}, \\beta$ \u90fd\u7ecf\u8fc7\u4e86\u4e2d\u5fc3\u5316\uff0c\u5e76\u4e14\u662f $p$ \u7ef4\u7684\u3002 \u8be5\u5f0f\u5bf9 $\\beta$ \u6c42\u5bfc\u5e76\u4ee4\u5bfc\u6570\u4e3a 0\uff0c\u6709\uff1a $$ -\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\beta) + \\lambda \\beta = 0$$ \u89e3\u5f97\uff1a $$ \\beta = (\\textbf{X}^T\\textbf{X} + \\lambda \\textbf{I})^{-1} \\textbf{X}^T \\textbf{y}$$ \u6211\u4eec\u770b\u5230\uff0c\u5373\u4f7f $\\textbf{X}^T\\textbf{X}$ \u662f\u975e\u6ee1\u79e9\u7684\uff0c\u7531\u4e8e\u591a\u52a0\u4e86\u4e00\u4e2a $\\lambda \\textbf{I}$\uff0c\u5b83\u4ecd\u662f\u4e00\u4e2a\u53ef\u9006\u77e9\u9635\u3002\u8fd9\u4e5f\u662f ridge regression \u7684\u53e6\u4e00\u4e2a\u4f18\u52bf\u3002","title":"3.4.1 Ridge Regression"},{"location":"mathematics/ESL3_LinearMethodsForRegression/#reference","text":"ESL solution ESL Chinese Simple Linear Regression Proofs involving ordinary least squares","title":"Reference"}]}