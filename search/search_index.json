{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ye's Notes Notes for my random readings. There's no explicit goal, but I'd love to keep my brain busy. The notes can be roughly split to 3 topics, they are related to my work to varying degrees: Trading Computer science Mathematics Again, I'm not an expert in any of these areas, sorry for any error or incompleteness, and appreciate if you can point them out. Trading WIP Computer Science WIP Mathematics WIP","title":"Ye's Notes"},{"location":"#yes-notes","text":"Notes for my random readings. There's no explicit goal, but I'd love to keep my brain busy. The notes can be roughly split to 3 topics, they are related to my work to varying degrees: Trading Computer science Mathematics Again, I'm not an expert in any of these areas, sorry for any error or incompleteness, and appreciate if you can point them out.","title":"Ye's Notes"},{"location":"#trading","text":"WIP","title":"Trading"},{"location":"#computer-science","text":"WIP","title":"Computer Science"},{"location":"#mathematics","text":"WIP","title":"Mathematics"},{"location":"mathematics/CFR/CfrAndKuhnPoker/","text":"Counterfactual Regret Minimization and Kuhn Poker \u5728\u4e0a\u4e00\u7bc7\u6587\u7ae0 Regret Matching and Blotto Game \u4e2d\uff0c\u6211\u4eec\u7528 regret matching \u65b9\u6cd5\u6765\u627e\u535a\u5f08\u7684\u7eb3\u4ec0\u5747\u8861\u70b9\u3002 regret matching \u65b9\u6cd5\u7684\u5c40\u9650\u6027\u5728\u4e8e\uff0c\u5b83\u53ea\u9002\u7528\u4e8e\u80fd\u7528\u77e9\u9635\u8868\u793a\u7684\u535a\u5f08\uff0c\u5373\u6bcf\u4e2a\u73a9\u5bb6\u540c\u65f6\u91c7\u53d6\u884c\u52a8\u7684\u535a\u5f08\uff0c\u4f8b\u5982\u526a\u5200\u77f3\u5934\u5e03\u3001blotto game\u3002 1 Kuhn Poker \u8fd8\u6709\u4e00\u79cd\u535a\u5f08\u53eb\u505a\u201d\u8fde\u7eed\u535a\u5f08\u201c(sequential game)\uff0c\u5373\u73a9\u5bb6\u7684\u52a8\u4f5c\u5e76\u4e0d\u540c\u65f6\u53d1\u751f\uff0c\u800c\u662f\u4f9d\u6b21\u53d1\u751f\u7684\uff0c\u4f8b\u5982\u5fb7\u5dde\u6251\u514b\u3002\u5b83\u6709\u4e2a\u7b80\u5316\u7248\u672c Kuhn Poker\uff0c\u6211\u4eec\u5c06\u7528\u5b83\u4f5c\u4e3a\u4f8b\u5b50\u5b9e\u73b0 Counterfactual Regret Minimization\u3002 Kuhn Poker is a simple 3-card poker game by Harold E. Kuhn [8]. Two players each ante 1 chip, i.e. bet 1 chip blind into the pot before the deal. Three cards, marked with numbers 1, 2, and 3, are shuffled, and one card is dealt to each player and held as private information. Play alternates starting with player 1. On a turn, a player may either pass or bet. A player that bets places an additional chip into the pot. When a player passes after a bet, the opponent takes all chips in the pot. When there are two successive passes or two successive bets, both players reveal their cards, and the player with the higher card takes all chips in the pot. Kuhn Poker \u53ef\u4ee5\u7528\u6811\u6765\u8868\u793a\u3002\u4e00\u5171\u6709\u4e09\u7c7b\u8282\u70b9\uff1a chance node\uff0c\u5982\u56fe\u4e2d\u7684\u6839\u7ed3\u70b9\u8868\u793a\u7684\u53d1\u724c\u8282\u70b9 decision node\uff0c\u73a9\u5bb6\u505a\u51b3\u7b56\u7684\u8282\u70b9 terminal node\uff0c\u6e38\u620f\u7ed3\u675f\uff0c\u7ed3\u7b97 payoff \u7684\u8282\u70b9\uff0c\u5373\u6811\u4e2d\u7684\u53f6\u5b50\u8282\u70b9\u3002 Player 1 Player 2 Player 1 Payoff pass pass +1 to player with higher card pass bet pass +1 to player 2 pass bet bet +2 to player with higher card bet pass +1 to player 1 bet bet +2 to player with higher card \u56fe\u4e2d\u7684\u8981\u70b9\uff1a \u6bcf\u4e2a\u8282\u70b9\u4ee3\u8868\u4e00\u4e2a\u72b6\u6001\uff0c\u8282\u70b9\u4e0e\u8282\u70b9\u7684\u8fb9\u4ee3\u8868 action\u3002\u72b6\u6001\u5c31\u662f information set\uff0c\u5373__\u505a\u51b3\u7b56\u65f6__\u6240\u6709\u53ef\u4ee5\u83b7\u53d6\u7684\u4fe1\u606f\uff08\u5305\u62ec\u5386\u53f2 game \u4fe1\u606f\uff09 \u6bcf\u4e2a\u8282\u70b9\u6709\u4e24\u4e2a\u5206\u652f\uff0c\u5206\u522b\u4ee3\u8868\u8ddf\u724c (pass) \u548c\u52a0\u6ce8 (fold)\u3002 \u5982\u679c\u5728\u5bf9\u65b9 bet \u540e\u9009\u62e9 pass \u5219\u8ba4\u5b9a\u4e3a\u8d1f \u5982\u679c\u90fd bet \u6216\u8005\u90fd pass\uff0c\u724c\u5927\u7684\u80dc Kuhn \u5171\u6709 12 \u79cd information set\uff0c\u5bf9\u624b\u724c 1\uff0c2\uff0c3 \u5404\u6709 4 \u79cd\uff1a \u672c\u65b9\u5148\u624b\uff0c\u51b3\u5b9a pass \u8fd8\u662f bet \u672c\u65b9\u5148\u624b pass\uff0c\u5bf9\u624b bet\uff0c\u518d\u6b21\u51b3\u5b9a pass \u8fd8\u662f bet \u672c\u65b9\u540e\u624b\uff0c\u5bf9\u65b9 pass\uff0c\u51b3\u5b9a pass \u8fd8\u662f bet \u672c\u65b9\u540e\u624b\uff0c\u5bf9\u65b9 bet\uff0c\u51b3\u5b9a pass \u8fd8\u662f bet \u7531\u4e8e\u6211\u4eec\u65e0\u6cd5\u5f97\u77e5\u5bf9\u65b9\u624b\u724c\uff0c\u6bcf\u79cd information set \u5305\u62ec\u4e86 2 \u4e2a game state\u3002\u56e0\u4e3a\u603b\u5171\u6709 3 \u5f20\u724c\uff0c\u5bf9\u624b\u624b\u724c\u53ef\u80fd\u662f\u9664\u53bb\u6211\u4eec\u624b\u724c\u5916\u7684 2 \u5f20\u4efb\u610f\u4e00\u5f20\u3002 \u8fd9\u4e5f\u8bf4\u660e\u4e86 information set \u4e0e game state \u7684\u533a\u522b\u3002game state \u662f\u5ba2\u89c2\u5b58\u5728\u7684\u72b6\u6001\uff0c\u800c information set \u662f\u4e3b\u89c2\u89c2\u5bdf\u5230\u7684\u4fe1\u606f\uff0c\u5b83\u4e0d\u4e00\u5b9a\u662f\u5b8c\u6574\u7684\uff0c\u56e0\u6b64\u4e0e game state \u53ef\u80fd\u662f\u4e00\u5bf9\u591a\u7684\u5173\u7cfb\u3002 2 Counterfactual Regret Minimization \u201cCounterfactual\u201d means contrary to fact. Intuitively, the counterfactual regret closely measures the potential gain if we do something contrary to the fact, such as deliberately making action \\(a\\) at information set \\(I\\) instead of following strategy \\(\\sigma^t\\) . 2.1 Notation symbol term comment \\(h\\) action history from root of the game \\(\\sigma_i\\) player \\(i\\) 's strategy probability of choosing action \\(a\\) in information set \\(I\\) \\(\\sigma\\) strategy profile all player strategies together \\(\\pi ^ {\\sigma} (h)\\) reach probability reach probability of game history \\(h\\) with strategy profile \\(\\sigma\\) \\(u\\) utility payoff \\(t\\) time step every information set has an independent \\(t\\) , it is incremented with each visit to the information set 2.2 Formula Let \\(Z\\) denote the set of all terminal game histories (sequence from root to leaf). Then proper prefix \\(h \\sqsubset z\\) for \\(z \\in Z\\) is a nonterminal game history. Z are the all possible endings from h . Define the counterfactual value at non-terminal history \\(h\\) \u0012 for player \\(i\\) as : \\[ v_i(\\sigma, h) = \\sum_{z \\in Z, h \\sqsubset z } \\pi_{-i}^{\\sigma} (h) \\pi ^{\\sigma} (h, z) u_i (z) \\] \\(\\pi_{-i}^{\\sigma} (h)\\) is the probability of reaching \\(h\\) with strategy profile \\(\\sigma\\) excluding the randomness of player \\(i\\) 's actions (player \\(i\\) has probability 1.0 to take current actions). \\(\\pi^{\\sigma}(h, z)\\) is the probability of reaching \\(z\\) from \\(h\\) with strategy profile \\(\\sigma\\) . In the graph above, because \\(i = 0\\) , the p0 player has 100% probability to choose action \\(a_2\\) and \\(a_4\\) . That's why the probability of reaching \\(h\\) is \\(P(a_1)P(a_3|a_2)\\) . We treat the computation as if player \\(i\\) 's strategy was modified to have intentionally played to information set \\(I\\) . Put another way, we exclude the probabilities that factually came into player \\(i\\) \u2019s play from the computation. The counterfactual regret of not taking action \\(a\\) at history \\(h\\) is defined as: \\[r(h, a) = v_i(\\sigma_{I \\rightarrow a}, h) - v_i(\\sigma, h)\\] \\(\\sigma_{I \\rightarrow a}\\) denotes a profile equivalent to \\(\\sigma\\) , except that action \\(a\\) is always chosen at information set \\(I\\) . Since an information set may be reached through multiple game histories , the counterfactual regret of not taking action \\(a\\) at information set \\(I\\) is: \\[ r(I, a) = \\sum_{h \\in I} r(h, a) \\] Let \\(r_i^t(I, a)\\) refer to the regret in time \\(t\\) belonging to player \\(i\\) , the cumulative counterfactual regret is defined as: \\( \\(R_i^T(I, a) = \\sum_{t=1}^T r_i^t(I, a)\\) \\) \u0012 For each information set \\(I\\) , the probability of choosing action \\(a\\) is calculated by: \\[ \\sigma^{T+1}_i (I, a) = \\begin{cases} \\frac{R_i^{T, +}(I, a)}{ \\sum_{a \\in A(I)} R_i^{T, +}(I, a) } & \\text{if} \\sum_{a \\in A(I)} R_i^{T, +}(I, a) > 0\\\\ \\frac{1}{|A(I)|} & \\text{otherwise.} \\end{cases} \\] \\(+\\) means positive (>0). It selects actions in proportion to positive regrets. This is the same as the regret matching algorithm in Regret Matching and Blotto Game . 2.3 Algorithm CFR \u53c2\u6570\uff1a 1. action history: \\(h\\) 2. learning player id: \\(i\\) 3. time step: \\(t\\) 4. reach probability of action history \\(h\\) for player 1: \\(\\pi_1\\) 5. reach probability of action history \\(h\\) for player 2: \\(\\pi_2\\) \u8fd9\u4e2a\u7b97\u6cd5\u5176\u5b9e\u5199\u7684\u6bd4\u8f83\u6666\u6da9\uff0c\u53c2\u6570\u8bbe\u8ba1\u4e5f\u4e0d\u662f\u7279\u522b\u5408\u7406\u3002\u5b9e\u9645\u4e0a\uff0c\u6211\u4eec\u53ea\u9700\u8981\u4e09\u4e2a\u53c2\u6570\uff1a \u6e38\u620f\u5386\u53f2 history \u73a9\u5bb6\u7684\u624b\u724c cards \u6bcf\u4e2a\u73a9\u5bb6\u5230\u8fbe\u8fd9\u4e2a\u5386\u53f2\u8282\u70b9\u7684\u6982\u7387 reach_probs \u5f53\u524d\u73a9\u5bb6 id \u53ef\u4ee5\u901a\u8fc7\u6e38\u620f\u5386\u53f2\u6765\u63a8\u65ad\u3002 \u6211\u81ea\u5df1\u7684\u5b9e\u73b0\u5982\u4e0b\uff1a fn cfr ( & mut self , history : kuhn :: ActionHistory , cards : & Vec < i32 > , reach_probs : HashMap < i32 , f64 > , ) -> f64 { // current active player let player_id = ( history . 0. len () % 2 ) as i32 ; let maybe_payoff = kuhn :: get_payoff ( & history , cards ); if maybe_payoff . is_some () { let payoff = match player_id { 0 => maybe_payoff . unwrap (), 1 => - maybe_payoff . unwrap (), _ => panic! ( \"unexpected player id {}\" , player_id ), }; return payoff as f64 ; } // not the terminal node let info_set = InformationSet { action_history : history . clone (), hand_card : cards [ player_id as usize ], }; if self . cfr_info . contains_key ( & info_set ) == false { self . cfr_info . insert ( info_set . clone (), CfrNode :: new ()); } let action_probs = self . cfr_info . get ( & info_set ) . unwrap () . get_action_probability (); let mut action_payoffs = Vec :: new (); let mut node_value = 0.0 ; for ( action_id , action_prob ) in action_probs . iter (). enumerate () { // next history, appending the new action to it let mut next_history = history . clone (); next_history . 0 . push ( kuhn :: Action :: from_int ( action_id as u32 )); // update history probability let mut next_reach_probs = reach_probs . clone (); * next_reach_probs . get_mut ( & player_id ). unwrap () *= action_prob ; // recursive call, \"-\" here because the return value is the opponent's payoff action_payoffs . push ( - self . cfr ( next_history , cards , next_reach_probs )); node_value += action_prob * action_payoffs [ action_id ]; } assert_eq! ( action_payoffs . len (), 2 ); // update regret let node = self . cfr_info . get_mut ( & info_set ). unwrap (); for ( action_id , payoff ) in action_payoffs . iter (). enumerate () { let regret = payoff - node_value ; let opponent = 1 - player_id ; node . cum_regrets [ action_id ] += reach_probs [ & opponent ] * regret ; } for ( action_id , action_prob ) in action_probs . iter (). enumerate () { node . cum_strategy [ action_id ] += reach_probs [ & player_id ] * action_prob ; } return node_value ; } \u7b97\u6cd5\u4e2d\u7684 chance node \u5176\u5b9e\u5c31\u662f\u7c7b\u4f3c\u4e8e\u53d1\u724c\uff0c\u63b7\u9ab0\u5b50\u8fd9\u7c7b\u968f\u673a\u8282\u70b9\u3002\u5728 kuhn \u4e2d\u4ec5\u6709\u4e00\u4e2a\u968f\u673a\u8282\u70b9\uff0c\u5c31\u662f\u5f00\u5c40\u7684\u53d1\u724c\u73af\u8282\u3002\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u63d0\u524d\u968f\u673a\u53d1\u597d\u724c\uff0c\u4e0d\u7528\u5728 cfr \u7b97\u6cd5\u5185\u5904\u7406\u3002 \u8fd9\u662f\u4e00\u79cd\u591a\u53c9\u6811\u7684\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u3002\u6bcf\u4e2a information set \u5c31\u662f\u4e00\u4e2a\u8282\u70b9\u3002\u6bcf\u4e2a\u8282\u70b9\u91c7\u53d6\u7684\u4e0d\u540c action \u4f1a\u901a\u5411\u4e0b\u4e00\u4e2a\u5b50\u8282\u70b9\uff0c\u76f4\u5230\u53f6\u5b50\u8282\u70b9\uff08\u5373\u6e38\u620f\u7ed3\u675f\uff0c\u6709\u5177\u4f53 payoff \u503c\u7684\u8282\u70b9\uff09\u3002 \u6838\u5fc3\u601d\u8def\uff1a \u6bcf\u4e2a\u8282\u70b9\u91c7\u53d6\u7684 action \u53ef\u4ee5\u901a\u5411\u4e0b\u4e00\u4e2a\u8282\u70b9\u3002 \u5bf9\u4e8e\u6bcf\u4e2a\u8282\u70b9\uff0c\u8282\u70b9\u4ef7\u503c\u7684\u8ba1\u7b97\u662f\u6240\u6709\u901a\u8fc7 action \u53ef\u4ee5\u5230\u8fbe\u7684\u5b50\u8282\u70b9\u4ef7\u503c\u7684\u52a0\u6743\u5e73\u5747\u3002\u6743\u91cd\u4e3a action \u7684\u9009\u53d6\u6982\u7387\u3002 action\u7684\u9009\u53d6\u6982\u7387\u6839\u636e regret matching \u7b97\u6cd5\u786e\u5b9a\uff0c\u82e5 regret \u5c1a\u672a\u521d\u59cb\u5316\u5c31\u968f\u673a\u9009\u62e9 action action \u7684 regret \u53ef\u4ee5\u7531\u9009\u53d6\u8be5 action \u5230\u8fbe\u7684\u5b50\u8282\u70b9\u4ef7\u503c\u51cf\u53bb\u5f53\u524d\u8282\u70b9\u4ef7\u503c\u5f97\u5230\u3002 \u6bcf\u6b21\u8fed\u4ee3\uff0c\u5c06 action \u7684 regret \u4e58\u4e0a\u5230\u8fbe\u8be5\u8282\u70b9\u6982\u7387 ( reach_probs[&opponent] )\uff0c\u7d2f\u8ba1\u5230\u8fd9\u4e2a\u8282\u70b9\u8fd9\u4e2a action \u7684 cum_regret \u4e2d\u3002\u8fd9\u91cc\u91c7\u7528 \u5bf9\u624b\u65b9 \u5230\u8fbe\u6982\u7387\u7684\u539f\u56e0\u662f\u5df1\u65b9 action \u6211\u4eec\u90fd\u662f\u4ee5 1.0 \u7684\u6982\u7387\u9009\u62e9\u7684\u3002 \u201c\u53cd\u4e8b\u5b9e\u201d\u4f53\u73b0\u5728\u6bcf\u4e2a\u8282\u70b9\uff0c\u867d\u7136\u4e8b\u5b9e\u4e0a\u6211\u4eec\u53ea\u80fd\u9009\u62e9\u4e00\u4e2a\u884c\u52a8\uff0c\u4f46\u662f\u6211\u4eec\u53ef\u4ee5\u865a\u62df\u5730\u5c1d\u8bd5\u6240\u6709\u7684\u884c\u52a8\u3002 \u6b64\u5916\uff0c\u6700\u540e\u8d8b\u8fd1\u4e8e Nash Equilibrium \u7684\u4e0d\u662f\u6211\u4eec\u7684 regret matching \u7b56\u7565\uff0c\u800c\u662f\u7d2f\u8ba1\u7b56\u7565\uff08 cum_strategy \uff09\uff0c\u8ba1\u7b97\u65b9\u5f0f\u89c1\u4ee3\u7801\u672b\u5c3e\u3002 2.4 \u7ed3\u8bba \u6709\u4e00\u4e9b\u7b80\u5355\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff1a Kuhn Poker \u7684\u7b2c\u4e00\u4f4d\u73a9\u5bb6\u6bcf\u5c40\u6536\u76ca\u671f\u671b\u662f -1/18\uff0c\u56e0\u4e3a\u7b2c\u4e8c\u4f4d\u73a9\u5bb6\u6709\u4fe1\u606f\u4e0a\u7684\u4f18\u52bf\uff08\u77e5\u9053\u7b2c\u4e00\u4f4d\u73a9\u5bb6\u7684\u884c\u52a8\uff09\u3002 \u73a9\u5bb6 1 \u62ff\u5230\u624b\u724c 1 \u9009\u62e9 Bet \u7684\u6982\u7387\u5e94\u8be5\u5728 (0.0, 1/3) \u4e4b\u95f4\uff08\u539f\u56e0\u89c1 Q & A\uff09 \u73a9\u5bb6 1 \u62ff\u5230\u624b\u724c 2 \u5e94\u8be5\u6c38\u8fdc\u9009\u62e9 Check\uff0c\u56e0\u4e3a\u5982\u679c\u9009\u62e9 bet\uff0c\u5bf9\u65b9\u624b\u724c\u662f 3\uff0c\u5fc5\u5b9a bet\uff0c\u8f93 $ 2, \u5982\u679c\u5bf9\u65b9\u624b\u724c\u662f 1\uff0c\u5fc5\u5b9a\u4e5f check\uff0c\u8d62 $1\uff0c\u4e0e check \u8d62\u7684\u94b1\u4e00\u81f4\uff0c\u4f46\u662f\u989d\u5916\u627f\u53d7\u98ce\u9669\u3002 \u73a9\u5bb6 1 \u62ff\u5230\u624b\u724c 3 \u9009\u62e9 Bet \u7684\u6982\u7387\u5e94\u8be5\u662f\u624b\u724c 1 \u9009\u62e9 bet \u6982\u7387\u7684 3 \u500d\uff08\u539f\u56e0\u89c1 Q & A\uff09 \u5404 information set \u9009\u53d6 check \u6216\u8005 bet \u7684\u6982\u7387\uff08information set\u8868\u793a\u4e3a \u624b\u724c + \u5f53\u524d action history\uff09\uff1a History Check Probability Bet Probability 1 0.77 0.23 1C 0.67 0.33 1B 1.00 0.00 1CB 1.00 0.00 2 1.00 0.00 2C 1.00 0.00 2B 0.66 0.34 2CB 0.42 0.58 3 0.30 0.70 3C 0.00 1.00 3B 0.00 1.00 3CB 0.00 1.00 2.5 \u4e00\u4e9b\u96be\u70b9 \u672c\u8282\u4e3b\u8981\u4ee5 Q & A \u7684\u5f62\u5f0f\u9610\u660e\u4e00\u4e0b\u666e\u904d\u4f1a\u9047\u5230\u7684\u56f0\u60d1\u3002 2.5.1 \u4e3a\u4ec0\u4e48\u9700\u8981\u8003\u8651 reach probability \u8003\u8651 reach probability \u624d\u80fd\u7b97\u51fa\u52a8\u4f5c\u7684\u671f\u671b\u6536\u76ca\uff0c\u4ece\u800c\u8ba1\u7b97\u6700\u4f73\u5e94\u5bf9\uff08best response\uff09\u3002 \u7528\u4e00\u4e2a\u4f8b\u5b50\u8bf4\u660e\u3002\u5047\u8bbe\u73b0\u5728\u6211\u4eec\u624b\u724c\u662f 2\uff0c\u5bf9\u65b9 Bet\uff0c\u6211\u4eec\u65e0\u6cd5\u77e5\u9053\u5bf9\u65b9\u624b\u724c\u662f 1 \u8fd8\u662f 3\u3002\u4f46\u662f\u6211\u4eec\u77e5\u9053\uff1a \u5982\u679c\u5bf9\u65b9\u624b\u724c\u662f1\uff0c\u6211\u4eec\u8ddf Bet\uff0cpayoff = 2\uff0c\u5982\u679c\u9009\u62e9 Check\uff0cpayoff = -1 \u5982\u679c\u5bf9\u65b9\u624b\u724c\u662f3\uff0c\u6211\u4eec\u8ddf Bet\uff0cpayoff = -2\uff0c\u5982\u679c\u9009\u62e9 Check\uff0cpayoff = -1 \u5047\u8bbe\u5bf9\u624b\u4ee5\u624b\u724c 1 Bet \u7684\u6982\u7387\uff08\u5373 history= \u201c1B\u201d \u7684 reach probability\uff09\u4e3a \\(a\\) \uff0c\u4ee5\u624b\u724c 3 Bet \u7684\u6982\u7387\uff08\u5373 history = \u201c3B\u201d \u7684 reach probability\uff09\u662f \\(b\\) \u3002\u6211\u4eec\u53ef\u4ee5\u5f97\u5230 Check \u7684\u671f\u671b\u6536\u76ca\uff1a \\[ \\text{EV}_{Check} = - a - b \\] \u540c\u7406\uff0cBet \u7684\u671f\u671b\u6536\u76ca\uff1a \\[ \\text{EV}_{Bet} = 2a - 2b \\] \u6211\u4eec\u80af\u5b9a\u503e\u5411\u4e8e\u9009\u62e9\u671f\u671b\u6536\u76ca\u5927\u7684\u884c\u52a8\u3002\u5373\uff0c\u82e5 \\(b > 3a\\) \uff0c\u5219\u5e94\u8be5\u9009\u62e9 Check\uff0c\u53cd\u4e4b\u5219\u9009\u62e9 Bet\u3002\u5728 Nash Equilibrium \u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e24\u4e2a\u52a8\u4f5c\u7684\u671f\u671b\u6536\u76ca\u5e94\u8be5\u76f8\u7b49\uff0c\u5373 \\(b = 3a\\) \uff0c\u6b64\u65f6\u6211\u4eec\u65e0\u6cd5exploit \u6211\u4eec\u7684\u5bf9\u624b\uff0c\u5f53\u7136\uff0c\u5bf9\u624b\u4e5f\u65e0\u6cd5 exploit \u6211\u4eec\u3002\u8fd9\u5c31\u662f\u4e4b\u524d\u9a8c\u8bc1\u7ed3\u679c\u65f6\u7684 \u201c3 \u500d\u201d \u7684\u539f\u56e0\u3002 2.5.2 \u4e3a\u4ec0\u4e48\u9700\u8981\u7ef4\u62a4\u4e00\u4e2a strategy sum \u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0ccumulative regret \u7684\u503c\u4e0d\u662f\u5f88\u7a33\u5b9a\uff0c\u4e00\u4e9b\u91cd\u8981\u7684 action \u53ef\u80fd\u6070\u597d\u5728 0.0 \u9644\u8fd1\u6447\u6446\uff0c\u5982\u679c\u9009\u7528\u5b83\u6765\u6c42\u6700\u7ec8\u7684\u7b56\u7565\uff0c\u53ef\u80fd\u5bfc\u81f4\u6709\u7684\u52a8\u4f5c\u65e0\u6cd5\u88ab\u9009\u62e9\u3002 \u4f7f\u7528 strategy sum \u5c31\u4e0d\u5b58\u5728\u8fd9\u4e2a\u95ee\u9898\uff0c\u5b83\u6c38\u8fdc\u662f\u6b63\u503c\uff0c\u5e76\u4e14\u6570\u5b66\u4e0a\u6536\u655b\u5230 Nash Equilibrium\u3002 2.6 CFR \u7b97\u6cd5\u7684\u4e0d\u8db3 \u5b83\u9700\u8981\u904d\u5386\u6574\u4e2a\u6e38\u620f\u6811 \u5b83\u9700\u8981\u77e5\u9053\u5bf9\u624b\u7684\u7b56\u7565\uff0c\u8fd9\u5728\u5b9e\u9645\u60c5\u51b5\u4e0b\u5f88\u96be\u6ee1\u8db3 \u9488\u5bf9\u8fd9\u4e24\u4e2a\u7f3a\u70b9\uff0cMarc Lanctot \u7b49\u4eba\u63d0\u51fa\u4e86 Monte-Carlo CFR\u3002\u6211\u4eec\u5c06\u5728\u4e0b\u4e00\u7bc7\u6587\u7ae0\u4e2d\u4ecb\u7ecd\u3002 Reference Building a Poker AI Monte Carlo Sampling for Regret Minimization in Extensive Games","title":"Counterfactual Regret Minimization and Kuhn Poker"},{"location":"mathematics/CFR/CfrAndKuhnPoker/#counterfactual-regret-minimization-and-kuhn-poker","text":"\u5728\u4e0a\u4e00\u7bc7\u6587\u7ae0 Regret Matching and Blotto Game \u4e2d\uff0c\u6211\u4eec\u7528 regret matching \u65b9\u6cd5\u6765\u627e\u535a\u5f08\u7684\u7eb3\u4ec0\u5747\u8861\u70b9\u3002 regret matching \u65b9\u6cd5\u7684\u5c40\u9650\u6027\u5728\u4e8e\uff0c\u5b83\u53ea\u9002\u7528\u4e8e\u80fd\u7528\u77e9\u9635\u8868\u793a\u7684\u535a\u5f08\uff0c\u5373\u6bcf\u4e2a\u73a9\u5bb6\u540c\u65f6\u91c7\u53d6\u884c\u52a8\u7684\u535a\u5f08\uff0c\u4f8b\u5982\u526a\u5200\u77f3\u5934\u5e03\u3001blotto game\u3002","title":"Counterfactual Regret Minimization and Kuhn Poker"},{"location":"mathematics/CFR/CfrAndKuhnPoker/#1-kuhn-poker","text":"\u8fd8\u6709\u4e00\u79cd\u535a\u5f08\u53eb\u505a\u201d\u8fde\u7eed\u535a\u5f08\u201c(sequential game)\uff0c\u5373\u73a9\u5bb6\u7684\u52a8\u4f5c\u5e76\u4e0d\u540c\u65f6\u53d1\u751f\uff0c\u800c\u662f\u4f9d\u6b21\u53d1\u751f\u7684\uff0c\u4f8b\u5982\u5fb7\u5dde\u6251\u514b\u3002\u5b83\u6709\u4e2a\u7b80\u5316\u7248\u672c Kuhn Poker\uff0c\u6211\u4eec\u5c06\u7528\u5b83\u4f5c\u4e3a\u4f8b\u5b50\u5b9e\u73b0 Counterfactual Regret Minimization\u3002 Kuhn Poker is a simple 3-card poker game by Harold E. Kuhn [8]. Two players each ante 1 chip, i.e. bet 1 chip blind into the pot before the deal. Three cards, marked with numbers 1, 2, and 3, are shuffled, and one card is dealt to each player and held as private information. Play alternates starting with player 1. On a turn, a player may either pass or bet. A player that bets places an additional chip into the pot. When a player passes after a bet, the opponent takes all chips in the pot. When there are two successive passes or two successive bets, both players reveal their cards, and the player with the higher card takes all chips in the pot. Kuhn Poker \u53ef\u4ee5\u7528\u6811\u6765\u8868\u793a\u3002\u4e00\u5171\u6709\u4e09\u7c7b\u8282\u70b9\uff1a chance node\uff0c\u5982\u56fe\u4e2d\u7684\u6839\u7ed3\u70b9\u8868\u793a\u7684\u53d1\u724c\u8282\u70b9 decision node\uff0c\u73a9\u5bb6\u505a\u51b3\u7b56\u7684\u8282\u70b9 terminal node\uff0c\u6e38\u620f\u7ed3\u675f\uff0c\u7ed3\u7b97 payoff \u7684\u8282\u70b9\uff0c\u5373\u6811\u4e2d\u7684\u53f6\u5b50\u8282\u70b9\u3002 Player 1 Player 2 Player 1 Payoff pass pass +1 to player with higher card pass bet pass +1 to player 2 pass bet bet +2 to player with higher card bet pass +1 to player 1 bet bet +2 to player with higher card \u56fe\u4e2d\u7684\u8981\u70b9\uff1a \u6bcf\u4e2a\u8282\u70b9\u4ee3\u8868\u4e00\u4e2a\u72b6\u6001\uff0c\u8282\u70b9\u4e0e\u8282\u70b9\u7684\u8fb9\u4ee3\u8868 action\u3002\u72b6\u6001\u5c31\u662f information set\uff0c\u5373__\u505a\u51b3\u7b56\u65f6__\u6240\u6709\u53ef\u4ee5\u83b7\u53d6\u7684\u4fe1\u606f\uff08\u5305\u62ec\u5386\u53f2 game \u4fe1\u606f\uff09 \u6bcf\u4e2a\u8282\u70b9\u6709\u4e24\u4e2a\u5206\u652f\uff0c\u5206\u522b\u4ee3\u8868\u8ddf\u724c (pass) \u548c\u52a0\u6ce8 (fold)\u3002 \u5982\u679c\u5728\u5bf9\u65b9 bet \u540e\u9009\u62e9 pass \u5219\u8ba4\u5b9a\u4e3a\u8d1f \u5982\u679c\u90fd bet \u6216\u8005\u90fd pass\uff0c\u724c\u5927\u7684\u80dc Kuhn \u5171\u6709 12 \u79cd information set\uff0c\u5bf9\u624b\u724c 1\uff0c2\uff0c3 \u5404\u6709 4 \u79cd\uff1a \u672c\u65b9\u5148\u624b\uff0c\u51b3\u5b9a pass \u8fd8\u662f bet \u672c\u65b9\u5148\u624b pass\uff0c\u5bf9\u624b bet\uff0c\u518d\u6b21\u51b3\u5b9a pass \u8fd8\u662f bet \u672c\u65b9\u540e\u624b\uff0c\u5bf9\u65b9 pass\uff0c\u51b3\u5b9a pass \u8fd8\u662f bet \u672c\u65b9\u540e\u624b\uff0c\u5bf9\u65b9 bet\uff0c\u51b3\u5b9a pass \u8fd8\u662f bet \u7531\u4e8e\u6211\u4eec\u65e0\u6cd5\u5f97\u77e5\u5bf9\u65b9\u624b\u724c\uff0c\u6bcf\u79cd information set \u5305\u62ec\u4e86 2 \u4e2a game state\u3002\u56e0\u4e3a\u603b\u5171\u6709 3 \u5f20\u724c\uff0c\u5bf9\u624b\u624b\u724c\u53ef\u80fd\u662f\u9664\u53bb\u6211\u4eec\u624b\u724c\u5916\u7684 2 \u5f20\u4efb\u610f\u4e00\u5f20\u3002 \u8fd9\u4e5f\u8bf4\u660e\u4e86 information set \u4e0e game state \u7684\u533a\u522b\u3002game state \u662f\u5ba2\u89c2\u5b58\u5728\u7684\u72b6\u6001\uff0c\u800c information set \u662f\u4e3b\u89c2\u89c2\u5bdf\u5230\u7684\u4fe1\u606f\uff0c\u5b83\u4e0d\u4e00\u5b9a\u662f\u5b8c\u6574\u7684\uff0c\u56e0\u6b64\u4e0e game state \u53ef\u80fd\u662f\u4e00\u5bf9\u591a\u7684\u5173\u7cfb\u3002","title":"1 Kuhn Poker"},{"location":"mathematics/CFR/CfrAndKuhnPoker/#2-counterfactual-regret-minimization","text":"\u201cCounterfactual\u201d means contrary to fact. Intuitively, the counterfactual regret closely measures the potential gain if we do something contrary to the fact, such as deliberately making action \\(a\\) at information set \\(I\\) instead of following strategy \\(\\sigma^t\\) .","title":"2 Counterfactual Regret Minimization"},{"location":"mathematics/CFR/CfrAndKuhnPoker/#21-notation","text":"symbol term comment \\(h\\) action history from root of the game \\(\\sigma_i\\) player \\(i\\) 's strategy probability of choosing action \\(a\\) in information set \\(I\\) \\(\\sigma\\) strategy profile all player strategies together \\(\\pi ^ {\\sigma} (h)\\) reach probability reach probability of game history \\(h\\) with strategy profile \\(\\sigma\\) \\(u\\) utility payoff \\(t\\) time step every information set has an independent \\(t\\) , it is incremented with each visit to the information set","title":"2.1 Notation"},{"location":"mathematics/CFR/CfrAndKuhnPoker/#22-formula","text":"Let \\(Z\\) denote the set of all terminal game histories (sequence from root to leaf). Then proper prefix \\(h \\sqsubset z\\) for \\(z \\in Z\\) is a nonterminal game history. Z are the all possible endings from h . Define the counterfactual value at non-terminal history \\(h\\) \u0012 for player \\(i\\) as : \\[ v_i(\\sigma, h) = \\sum_{z \\in Z, h \\sqsubset z } \\pi_{-i}^{\\sigma} (h) \\pi ^{\\sigma} (h, z) u_i (z) \\] \\(\\pi_{-i}^{\\sigma} (h)\\) is the probability of reaching \\(h\\) with strategy profile \\(\\sigma\\) excluding the randomness of player \\(i\\) 's actions (player \\(i\\) has probability 1.0 to take current actions). \\(\\pi^{\\sigma}(h, z)\\) is the probability of reaching \\(z\\) from \\(h\\) with strategy profile \\(\\sigma\\) . In the graph above, because \\(i = 0\\) , the p0 player has 100% probability to choose action \\(a_2\\) and \\(a_4\\) . That's why the probability of reaching \\(h\\) is \\(P(a_1)P(a_3|a_2)\\) . We treat the computation as if player \\(i\\) 's strategy was modified to have intentionally played to information set \\(I\\) . Put another way, we exclude the probabilities that factually came into player \\(i\\) \u2019s play from the computation. The counterfactual regret of not taking action \\(a\\) at history \\(h\\) is defined as: \\[r(h, a) = v_i(\\sigma_{I \\rightarrow a}, h) - v_i(\\sigma, h)\\] \\(\\sigma_{I \\rightarrow a}\\) denotes a profile equivalent to \\(\\sigma\\) , except that action \\(a\\) is always chosen at information set \\(I\\) . Since an information set may be reached through multiple game histories , the counterfactual regret of not taking action \\(a\\) at information set \\(I\\) is: \\[ r(I, a) = \\sum_{h \\in I} r(h, a) \\] Let \\(r_i^t(I, a)\\) refer to the regret in time \\(t\\) belonging to player \\(i\\) , the cumulative counterfactual regret is defined as: \\( \\(R_i^T(I, a) = \\sum_{t=1}^T r_i^t(I, a)\\) \\) \u0012 For each information set \\(I\\) , the probability of choosing action \\(a\\) is calculated by: \\[ \\sigma^{T+1}_i (I, a) = \\begin{cases} \\frac{R_i^{T, +}(I, a)}{ \\sum_{a \\in A(I)} R_i^{T, +}(I, a) } & \\text{if} \\sum_{a \\in A(I)} R_i^{T, +}(I, a) > 0\\\\ \\frac{1}{|A(I)|} & \\text{otherwise.} \\end{cases} \\] \\(+\\) means positive (>0). It selects actions in proportion to positive regrets. This is the same as the regret matching algorithm in Regret Matching and Blotto Game .","title":"2.2 Formula"},{"location":"mathematics/CFR/CfrAndKuhnPoker/#23-algorithm","text":"CFR \u53c2\u6570\uff1a 1. action history: \\(h\\) 2. learning player id: \\(i\\) 3. time step: \\(t\\) 4. reach probability of action history \\(h\\) for player 1: \\(\\pi_1\\) 5. reach probability of action history \\(h\\) for player 2: \\(\\pi_2\\) \u8fd9\u4e2a\u7b97\u6cd5\u5176\u5b9e\u5199\u7684\u6bd4\u8f83\u6666\u6da9\uff0c\u53c2\u6570\u8bbe\u8ba1\u4e5f\u4e0d\u662f\u7279\u522b\u5408\u7406\u3002\u5b9e\u9645\u4e0a\uff0c\u6211\u4eec\u53ea\u9700\u8981\u4e09\u4e2a\u53c2\u6570\uff1a \u6e38\u620f\u5386\u53f2 history \u73a9\u5bb6\u7684\u624b\u724c cards \u6bcf\u4e2a\u73a9\u5bb6\u5230\u8fbe\u8fd9\u4e2a\u5386\u53f2\u8282\u70b9\u7684\u6982\u7387 reach_probs \u5f53\u524d\u73a9\u5bb6 id \u53ef\u4ee5\u901a\u8fc7\u6e38\u620f\u5386\u53f2\u6765\u63a8\u65ad\u3002 \u6211\u81ea\u5df1\u7684\u5b9e\u73b0\u5982\u4e0b\uff1a fn cfr ( & mut self , history : kuhn :: ActionHistory , cards : & Vec < i32 > , reach_probs : HashMap < i32 , f64 > , ) -> f64 { // current active player let player_id = ( history . 0. len () % 2 ) as i32 ; let maybe_payoff = kuhn :: get_payoff ( & history , cards ); if maybe_payoff . is_some () { let payoff = match player_id { 0 => maybe_payoff . unwrap (), 1 => - maybe_payoff . unwrap (), _ => panic! ( \"unexpected player id {}\" , player_id ), }; return payoff as f64 ; } // not the terminal node let info_set = InformationSet { action_history : history . clone (), hand_card : cards [ player_id as usize ], }; if self . cfr_info . contains_key ( & info_set ) == false { self . cfr_info . insert ( info_set . clone (), CfrNode :: new ()); } let action_probs = self . cfr_info . get ( & info_set ) . unwrap () . get_action_probability (); let mut action_payoffs = Vec :: new (); let mut node_value = 0.0 ; for ( action_id , action_prob ) in action_probs . iter (). enumerate () { // next history, appending the new action to it let mut next_history = history . clone (); next_history . 0 . push ( kuhn :: Action :: from_int ( action_id as u32 )); // update history probability let mut next_reach_probs = reach_probs . clone (); * next_reach_probs . get_mut ( & player_id ). unwrap () *= action_prob ; // recursive call, \"-\" here because the return value is the opponent's payoff action_payoffs . push ( - self . cfr ( next_history , cards , next_reach_probs )); node_value += action_prob * action_payoffs [ action_id ]; } assert_eq! ( action_payoffs . len (), 2 ); // update regret let node = self . cfr_info . get_mut ( & info_set ). unwrap (); for ( action_id , payoff ) in action_payoffs . iter (). enumerate () { let regret = payoff - node_value ; let opponent = 1 - player_id ; node . cum_regrets [ action_id ] += reach_probs [ & opponent ] * regret ; } for ( action_id , action_prob ) in action_probs . iter (). enumerate () { node . cum_strategy [ action_id ] += reach_probs [ & player_id ] * action_prob ; } return node_value ; } \u7b97\u6cd5\u4e2d\u7684 chance node \u5176\u5b9e\u5c31\u662f\u7c7b\u4f3c\u4e8e\u53d1\u724c\uff0c\u63b7\u9ab0\u5b50\u8fd9\u7c7b\u968f\u673a\u8282\u70b9\u3002\u5728 kuhn \u4e2d\u4ec5\u6709\u4e00\u4e2a\u968f\u673a\u8282\u70b9\uff0c\u5c31\u662f\u5f00\u5c40\u7684\u53d1\u724c\u73af\u8282\u3002\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u63d0\u524d\u968f\u673a\u53d1\u597d\u724c\uff0c\u4e0d\u7528\u5728 cfr \u7b97\u6cd5\u5185\u5904\u7406\u3002 \u8fd9\u662f\u4e00\u79cd\u591a\u53c9\u6811\u7684\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u3002\u6bcf\u4e2a information set \u5c31\u662f\u4e00\u4e2a\u8282\u70b9\u3002\u6bcf\u4e2a\u8282\u70b9\u91c7\u53d6\u7684\u4e0d\u540c action \u4f1a\u901a\u5411\u4e0b\u4e00\u4e2a\u5b50\u8282\u70b9\uff0c\u76f4\u5230\u53f6\u5b50\u8282\u70b9\uff08\u5373\u6e38\u620f\u7ed3\u675f\uff0c\u6709\u5177\u4f53 payoff \u503c\u7684\u8282\u70b9\uff09\u3002 \u6838\u5fc3\u601d\u8def\uff1a \u6bcf\u4e2a\u8282\u70b9\u91c7\u53d6\u7684 action \u53ef\u4ee5\u901a\u5411\u4e0b\u4e00\u4e2a\u8282\u70b9\u3002 \u5bf9\u4e8e\u6bcf\u4e2a\u8282\u70b9\uff0c\u8282\u70b9\u4ef7\u503c\u7684\u8ba1\u7b97\u662f\u6240\u6709\u901a\u8fc7 action \u53ef\u4ee5\u5230\u8fbe\u7684\u5b50\u8282\u70b9\u4ef7\u503c\u7684\u52a0\u6743\u5e73\u5747\u3002\u6743\u91cd\u4e3a action \u7684\u9009\u53d6\u6982\u7387\u3002 action\u7684\u9009\u53d6\u6982\u7387\u6839\u636e regret matching \u7b97\u6cd5\u786e\u5b9a\uff0c\u82e5 regret \u5c1a\u672a\u521d\u59cb\u5316\u5c31\u968f\u673a\u9009\u62e9 action action \u7684 regret \u53ef\u4ee5\u7531\u9009\u53d6\u8be5 action \u5230\u8fbe\u7684\u5b50\u8282\u70b9\u4ef7\u503c\u51cf\u53bb\u5f53\u524d\u8282\u70b9\u4ef7\u503c\u5f97\u5230\u3002 \u6bcf\u6b21\u8fed\u4ee3\uff0c\u5c06 action \u7684 regret \u4e58\u4e0a\u5230\u8fbe\u8be5\u8282\u70b9\u6982\u7387 ( reach_probs[&opponent] )\uff0c\u7d2f\u8ba1\u5230\u8fd9\u4e2a\u8282\u70b9\u8fd9\u4e2a action \u7684 cum_regret \u4e2d\u3002\u8fd9\u91cc\u91c7\u7528 \u5bf9\u624b\u65b9 \u5230\u8fbe\u6982\u7387\u7684\u539f\u56e0\u662f\u5df1\u65b9 action \u6211\u4eec\u90fd\u662f\u4ee5 1.0 \u7684\u6982\u7387\u9009\u62e9\u7684\u3002 \u201c\u53cd\u4e8b\u5b9e\u201d\u4f53\u73b0\u5728\u6bcf\u4e2a\u8282\u70b9\uff0c\u867d\u7136\u4e8b\u5b9e\u4e0a\u6211\u4eec\u53ea\u80fd\u9009\u62e9\u4e00\u4e2a\u884c\u52a8\uff0c\u4f46\u662f\u6211\u4eec\u53ef\u4ee5\u865a\u62df\u5730\u5c1d\u8bd5\u6240\u6709\u7684\u884c\u52a8\u3002 \u6b64\u5916\uff0c\u6700\u540e\u8d8b\u8fd1\u4e8e Nash Equilibrium \u7684\u4e0d\u662f\u6211\u4eec\u7684 regret matching \u7b56\u7565\uff0c\u800c\u662f\u7d2f\u8ba1\u7b56\u7565\uff08 cum_strategy \uff09\uff0c\u8ba1\u7b97\u65b9\u5f0f\u89c1\u4ee3\u7801\u672b\u5c3e\u3002","title":"2.3 Algorithm"},{"location":"mathematics/CFR/CfrAndKuhnPoker/#24","text":"\u6709\u4e00\u4e9b\u7b80\u5355\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff1a Kuhn Poker \u7684\u7b2c\u4e00\u4f4d\u73a9\u5bb6\u6bcf\u5c40\u6536\u76ca\u671f\u671b\u662f -1/18\uff0c\u56e0\u4e3a\u7b2c\u4e8c\u4f4d\u73a9\u5bb6\u6709\u4fe1\u606f\u4e0a\u7684\u4f18\u52bf\uff08\u77e5\u9053\u7b2c\u4e00\u4f4d\u73a9\u5bb6\u7684\u884c\u52a8\uff09\u3002 \u73a9\u5bb6 1 \u62ff\u5230\u624b\u724c 1 \u9009\u62e9 Bet \u7684\u6982\u7387\u5e94\u8be5\u5728 (0.0, 1/3) \u4e4b\u95f4\uff08\u539f\u56e0\u89c1 Q & A\uff09 \u73a9\u5bb6 1 \u62ff\u5230\u624b\u724c 2 \u5e94\u8be5\u6c38\u8fdc\u9009\u62e9 Check\uff0c\u56e0\u4e3a\u5982\u679c\u9009\u62e9 bet\uff0c\u5bf9\u65b9\u624b\u724c\u662f 3\uff0c\u5fc5\u5b9a bet\uff0c\u8f93 $ 2, \u5982\u679c\u5bf9\u65b9\u624b\u724c\u662f 1\uff0c\u5fc5\u5b9a\u4e5f check\uff0c\u8d62 $1\uff0c\u4e0e check \u8d62\u7684\u94b1\u4e00\u81f4\uff0c\u4f46\u662f\u989d\u5916\u627f\u53d7\u98ce\u9669\u3002 \u73a9\u5bb6 1 \u62ff\u5230\u624b\u724c 3 \u9009\u62e9 Bet \u7684\u6982\u7387\u5e94\u8be5\u662f\u624b\u724c 1 \u9009\u62e9 bet \u6982\u7387\u7684 3 \u500d\uff08\u539f\u56e0\u89c1 Q & A\uff09 \u5404 information set \u9009\u53d6 check \u6216\u8005 bet \u7684\u6982\u7387\uff08information set\u8868\u793a\u4e3a \u624b\u724c + \u5f53\u524d action history\uff09\uff1a History Check Probability Bet Probability 1 0.77 0.23 1C 0.67 0.33 1B 1.00 0.00 1CB 1.00 0.00 2 1.00 0.00 2C 1.00 0.00 2B 0.66 0.34 2CB 0.42 0.58 3 0.30 0.70 3C 0.00 1.00 3B 0.00 1.00 3CB 0.00 1.00","title":"2.4 \u7ed3\u8bba"},{"location":"mathematics/CFR/CfrAndKuhnPoker/#25","text":"\u672c\u8282\u4e3b\u8981\u4ee5 Q & A \u7684\u5f62\u5f0f\u9610\u660e\u4e00\u4e0b\u666e\u904d\u4f1a\u9047\u5230\u7684\u56f0\u60d1\u3002","title":"2.5 \u4e00\u4e9b\u96be\u70b9"},{"location":"mathematics/CFR/CfrAndKuhnPoker/#251-reach-probability","text":"\u8003\u8651 reach probability \u624d\u80fd\u7b97\u51fa\u52a8\u4f5c\u7684\u671f\u671b\u6536\u76ca\uff0c\u4ece\u800c\u8ba1\u7b97\u6700\u4f73\u5e94\u5bf9\uff08best response\uff09\u3002 \u7528\u4e00\u4e2a\u4f8b\u5b50\u8bf4\u660e\u3002\u5047\u8bbe\u73b0\u5728\u6211\u4eec\u624b\u724c\u662f 2\uff0c\u5bf9\u65b9 Bet\uff0c\u6211\u4eec\u65e0\u6cd5\u77e5\u9053\u5bf9\u65b9\u624b\u724c\u662f 1 \u8fd8\u662f 3\u3002\u4f46\u662f\u6211\u4eec\u77e5\u9053\uff1a \u5982\u679c\u5bf9\u65b9\u624b\u724c\u662f1\uff0c\u6211\u4eec\u8ddf Bet\uff0cpayoff = 2\uff0c\u5982\u679c\u9009\u62e9 Check\uff0cpayoff = -1 \u5982\u679c\u5bf9\u65b9\u624b\u724c\u662f3\uff0c\u6211\u4eec\u8ddf Bet\uff0cpayoff = -2\uff0c\u5982\u679c\u9009\u62e9 Check\uff0cpayoff = -1 \u5047\u8bbe\u5bf9\u624b\u4ee5\u624b\u724c 1 Bet \u7684\u6982\u7387\uff08\u5373 history= \u201c1B\u201d \u7684 reach probability\uff09\u4e3a \\(a\\) \uff0c\u4ee5\u624b\u724c 3 Bet \u7684\u6982\u7387\uff08\u5373 history = \u201c3B\u201d \u7684 reach probability\uff09\u662f \\(b\\) \u3002\u6211\u4eec\u53ef\u4ee5\u5f97\u5230 Check \u7684\u671f\u671b\u6536\u76ca\uff1a \\[ \\text{EV}_{Check} = - a - b \\] \u540c\u7406\uff0cBet \u7684\u671f\u671b\u6536\u76ca\uff1a \\[ \\text{EV}_{Bet} = 2a - 2b \\] \u6211\u4eec\u80af\u5b9a\u503e\u5411\u4e8e\u9009\u62e9\u671f\u671b\u6536\u76ca\u5927\u7684\u884c\u52a8\u3002\u5373\uff0c\u82e5 \\(b > 3a\\) \uff0c\u5219\u5e94\u8be5\u9009\u62e9 Check\uff0c\u53cd\u4e4b\u5219\u9009\u62e9 Bet\u3002\u5728 Nash Equilibrium \u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e24\u4e2a\u52a8\u4f5c\u7684\u671f\u671b\u6536\u76ca\u5e94\u8be5\u76f8\u7b49\uff0c\u5373 \\(b = 3a\\) \uff0c\u6b64\u65f6\u6211\u4eec\u65e0\u6cd5exploit \u6211\u4eec\u7684\u5bf9\u624b\uff0c\u5f53\u7136\uff0c\u5bf9\u624b\u4e5f\u65e0\u6cd5 exploit \u6211\u4eec\u3002\u8fd9\u5c31\u662f\u4e4b\u524d\u9a8c\u8bc1\u7ed3\u679c\u65f6\u7684 \u201c3 \u500d\u201d \u7684\u539f\u56e0\u3002","title":"2.5.1 \u4e3a\u4ec0\u4e48\u9700\u8981\u8003\u8651 reach probability"},{"location":"mathematics/CFR/CfrAndKuhnPoker/#252-strategy-sum","text":"\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0ccumulative regret \u7684\u503c\u4e0d\u662f\u5f88\u7a33\u5b9a\uff0c\u4e00\u4e9b\u91cd\u8981\u7684 action \u53ef\u80fd\u6070\u597d\u5728 0.0 \u9644\u8fd1\u6447\u6446\uff0c\u5982\u679c\u9009\u7528\u5b83\u6765\u6c42\u6700\u7ec8\u7684\u7b56\u7565\uff0c\u53ef\u80fd\u5bfc\u81f4\u6709\u7684\u52a8\u4f5c\u65e0\u6cd5\u88ab\u9009\u62e9\u3002 \u4f7f\u7528 strategy sum \u5c31\u4e0d\u5b58\u5728\u8fd9\u4e2a\u95ee\u9898\uff0c\u5b83\u6c38\u8fdc\u662f\u6b63\u503c\uff0c\u5e76\u4e14\u6570\u5b66\u4e0a\u6536\u655b\u5230 Nash Equilibrium\u3002","title":"2.5.2  \u4e3a\u4ec0\u4e48\u9700\u8981\u7ef4\u62a4\u4e00\u4e2a strategy sum"},{"location":"mathematics/CFR/CfrAndKuhnPoker/#26-cfr","text":"\u5b83\u9700\u8981\u904d\u5386\u6574\u4e2a\u6e38\u620f\u6811 \u5b83\u9700\u8981\u77e5\u9053\u5bf9\u624b\u7684\u7b56\u7565\uff0c\u8fd9\u5728\u5b9e\u9645\u60c5\u51b5\u4e0b\u5f88\u96be\u6ee1\u8db3 \u9488\u5bf9\u8fd9\u4e24\u4e2a\u7f3a\u70b9\uff0cMarc Lanctot \u7b49\u4eba\u63d0\u51fa\u4e86 Monte-Carlo CFR\u3002\u6211\u4eec\u5c06\u5728\u4e0b\u4e00\u7bc7\u6587\u7ae0\u4e2d\u4ecb\u7ecd\u3002","title":"2.6 CFR \u7b97\u6cd5\u7684\u4e0d\u8db3"},{"location":"mathematics/CFR/CfrAndKuhnPoker/#reference","text":"Building a Poker AI Monte Carlo Sampling for Regret Minimization in Extensive Games","title":"Reference"},{"location":"mathematics/CFR/MonteCarloCfr/","text":"Monte Carlo CFR \u5728\u4e0a\u4e00\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6211\u4eec\u8bb2\u4e86\u7ecf\u5178\u7684 CFR \u7b97\u6cd5\u7684\u539f\u7406\u53ca\u5b9e\u73b0\u3002\u4f46\u662f\uff0c\u5b83\u4e5f\u6709\u4e24\u4e2a\u81f4\u547d\u7684\u7f3a\u9677\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5e94\u7528\u5230\u5b9e\u9645\u7684\u590d\u6742\u535a\u5f08\u4e2d\uff1a \u5b83\u9700\u8981\u904d\u5386\u6574\u4e2a\u6e38\u620f\u6811 \u5b83\u9700\u8981\u77e5\u9053\u5bf9\u624b\u7684\u7b56\u7565\uff0c\u8fd9\u5728\u5b9e\u9645\u60c5\u51b5\u4e0b\u5f88\u96be\u6ee1\u8db3 \u672c\u6587\u5c06\u4ecb\u7ecd\u57fa\u4e8e Monte Carlo \u7684\u6539\u8fdb\u7b97\u6cd5\u3002\u5168\u90e8\u4ee3\u7801\u53ef\u4ee5\u53c2\u8003\u6211\u7684 github \u9879\u76ee\uff1a cfr-kuhn . Definition \u6838\u5fc3\u601d\u60f3\u662f\uff0c\u5728\u907f\u514d\u904d\u5386\u6574\u4e2a\u6e38\u620f\u6811\u7684\u524d\u63d0\u4e0b\uff0c\u6bcf\u4e2a information set \u7684\u6bcf\u4e2a action \u7684 counterfactual regret \u671f\u671b\u503c\u4fdd\u6301\u4e0d\u53d8\u3002 \u4ee4 \\(\\mathcal{Q} = \\{Q_1, Q_2, ..., Q_r \\}\\) \u662f\u6240\u6709 terminal node \\(Z\\) \u7684\u5b50\u96c6\uff0c\u5176\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20 \\(Q_j\\) \u6211\u4eec\u79f0\u4e3a block \uff0c\u6bcf\u4e2a block \u53ef\u80fd\u5305\u542b\u591a\u4e2a terminal node \u3002\u4ee4 \\(q_j > 0\\) \u662f\u9009\u62e9 \\(Q_j\\) \u7684\u6982\u7387\uff0c\u6ee1\u8db3 \\(\\sum_{j=1}^r q_j = 1\\) \u3002\u6bcf\u6b21\u8fed\u4ee3\uff0c\u6211\u4eec\u90fd\u4f1a\u4ece\u8fd9\u4e9b blocks \u4e2d\u91c7\u6837\u4e00\u4e2a\u5e76\u4e14\u53ea\u8003\u8651\u5728\u8be5 block \u4e2d\u7684 terminal node \u3002 \u56de\u5fc6 CFR \u7ecf\u5178\u7248\u7684 counterfactual value \u8ba1\u7b97\u516c\u5f0f\uff1a \\[ v_i(\\sigma, I) = \\sum_{z \\in Z_I } \\pi_{-i}^{\\sigma} (z[I]) \\pi ^{\\sigma} (z[I], z) u_i (z) \\] \u5728 MCCFR \u4e2d\uff0c\u5bf9\u4e8e block \\(Q_j\\) \u7684 sampled counterfactual value \u53ef\u4ee5\u8868\u793a\u4e3a\uff1a \\[v_i(\\sigma, I | j) = \\sum_{z \\in Q_j \\cap Z_I} \\frac{1}{q(z)} \\pi_{-i}^\\sigma (z[I]) \\pi^\\sigma (z[I], z) u_i(z)\\] \u5176\u4e2d\uff1a \\(z\\) \u662f terminal node \\(z_I\\) \u8868\u793a information set \\(I\\) \u53ef\u4ee5\u5230\u8fbe\u7684 \\(z\\) \u7684\u5168\u96c6 \\(z[I]\\) \u8868\u793a\u67d0\u4e2a\u53ef\u4ee5\u5230\u8fbe \\(z\\) \u7684 information set \\(i\\) \u8868\u793a\u73a9\u5bb6 id \\(j\\) \u8868\u793a block id \\(q(z)\\) \u8868\u793a \\(z\\) \u6240\u5728 block \u88ab\u9009\u4e2d\u7684\u6982\u7387\u4e4b\u548c\uff08\u4e00\u4e2a terminal node \u53ef\u4ee5\u88ab\u5206\u5230\u591a\u4e2a block \u4e2d\uff09 \u6211\u4eec\u53ef\u4ee5\u8bc1\u660e counterfactual value \u548c sampled counterfactual value \u671f\u671b\u662f\u76f8\u7b49\u7684\u3002 \u8fd9\u5c31\u662f MCCFR \u7684\u57fa\u672c\u601d\u60f3\u4e86\u3002\u4e8b\u5b9e\u4e0a\uff0cCFR \u53ef\u4ee5\u770b\u4f5c MCCFR \u5728 \\(\\mathcal{Q} = \\{Z\\}\\) \u4e14 \\(q_1 = 1.0\\) \u7684\u7279\u4f8b\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u53ef\u4ee5__\u5c06\u6bcf\u4e00\u4e2a block \u9009\u4e3a chance node \u7684\u4e00\u4e2a\u5206\u652f__\u3002\u4ee5 Kuhn Poker \u4e3a\u4f8b\uff0c\u6211\u4eec\u5bf9\u4e8e\u624b\u724c 1\uff0c2\uff0c3 \u53ef\u4ee5\u5efa\u7acb 3 \u4e2a block\uff1a \\(\\{ Q_1, Q_2, Q_3 \\}\\) \uff0c\u5206\u522b\u5305\u542b\u624b\u724c\u4e3a 1\uff0c2\uff0c3 \u7684\u6240\u6709 terminal nodes\u3002\u4ece\u800c\uff0c\u6bcf\u4e2a block \u7684\u6982\u7387\u4e5f\u81ea\u7136\u4e3a \\(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\) \u3002\u8fd9\u5c31\u662f chance-sampled CFR\u3002 Outcome-Sampling MCCFR \u5728 outcome-sampling MCCFR \u4e2d\uff0c\u6211\u4eec__\u628a\u6bcf\u4e2a terminal node \u4f5c\u4e3a\u4e00\u4e2a block__\u3002\u6bcf\u6b21\u8fed\u4ee3\uff0c\u6211\u4eec\u62bd\u53d6\u4e00\u4e2a terminal node \u5e76\u66f4\u65b0\u5b83\u7684\u524d\u7f00 information set\u3002\u4e00\u4e2a terminal node \u51fa\u73b0\u6982\u7387\u8d8a\u9ad8\uff0c\u6211\u4eec\u91c7\u6837\u6982\u7387\u4e5f\u8d8a\u9ad8\u3002\u56e0\u6b64\u6211\u4eec\u5fc5\u987b\u5f97\u5230\u6bcf\u4e2a terminal node j \u51fa\u73b0\u7684\u6982\u7387\u4f5c\u4e3a \\(q_j\\) \u3002\u5982\u4f55\u5f97\u5230\u5462\uff1f \u5728 CFR \u4e2d\uff0c\u6211\u4eec\u5728\u8ba1\u7b97 regret \u7684\u65f6\u5019\uff0c\u4f1a\u8ba1\u7b97\u5230\u8fbe\u6bcf\u4e2a history \u7684\u6982\u7387\u3002\u5bf9\u4e8e terminal node j\uff0c\u8fd9\u4e2a\u6982\u7387\u5176\u5b9e\u5c31\u662f\u91c7\u6837\u6982\u7387 \\(q_j\\) \u3002 CFR \u7b97\u6cd5\u5305\u542b\u4e86\u4e24\u4e2a\u65b9\u5411\u7684\u904d\u5386\uff1a \u524d\u5411\u904d\u5386\u3002\u7528\u4e8e\u8ba1\u7b97\u6bcf\u4e2a\u73a9\u5bb6\u4ece\u539f\u70b9\u5230\u8fbe\u5f53\u524d history \u7684\u6982\u7387 \\(\\pi_i^{\\sigma}(h)\\) \u540e\u5411\u904d\u5386\u3002\u7528\u4e8e\u8ba1\u7b97\u6bcf\u4e2a\u73a9\u5bb6\u4ece\u5f53\u524d history \u8fdb\u884c\u5230 terminal node \u7684\u6982\u7387 \\(\\pi_i^{\\sigma}(h,z)\\) \uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\uff0c\u8fd8\u4f1a\u8ba1\u7b97 sampled counterfactual regrets\u3002 \u540e\u5411\u904d\u5386\u4e2d\u8ba1\u7b97 sampled counterfactual regrets \u4f1a\u5206\u4e3a\u4e24\u79cd\u60c5\u51b5\u3002 \u5176\u4e00\u662f\u9009\u62e9\u4e86\u80fd\u901a\u5411\u5f53\u524d terminal node \\(z\\) \u7684 action\u3002\u5373\uff0c\u5728information set \\(I\\) \u91c7\u53d6\u4e86\u884c\u52a8 \\(a\\) \uff0c\u6b64\u65f6\u8ba1\u7b97\u65b9\u5f0f\u4e3a\uff1a \\[ \\begin{align} r(I, a) &= v_i(\\sigma_{I \\rightarrow a}, I) - v_i(\\sigma, I) \\\\ &= \\frac{\\pi_{-i}^\\sigma (z[I]) \\pi^\\sigma (z[I]a, z) u_i(z)}{q(z)} - \\frac{\\pi_{-i}^\\sigma (z[I]) \\pi^\\sigma (z[I], z) u_i(z)}{q(z)} \\\\ &= \\frac{\\pi_{-i}^\\sigma (z[I]) \\pi^\\sigma (z[I] a, z) u_i(z)}{q(z)} ( 1 - \\sigma(a|z[I])) \\end{align} \\] \u4e0a\u5f0f\u6bd4\u8f83\u96be\u4ee5\u7406\u89e3\u7684\u662f\uff1a \\[\\pi^\\sigma (z[I] a, z) \\cdot \\sigma(a|z[I]) = \\pi^\\sigma (z[I], z) \\] \u7ed3\u5408\u5b9a\u4e49\uff0c \\(\\pi^\\sigma (z[I] a, z)\\) \u662f\u4ece \\(I + a\\) \u5230\u8fbe \\(z\\) \u7684\u6982\u7387\uff0c\u76f8\u6bd4 \\(\\pi^\\sigma (z[I], z)\\) \u591a\u524d\u8fdb\u4e86\u4e00\u6b65\uff08\u9009\u62e9\u884c\u52a8 \\(a\\) \uff09\uff0c\u5728 \\(I\\) \u9009\u62e9 \\(a\\) \u7684\u6982\u7387\u662f \\(\\sigma(a|z[I])\\) \uff0c\u56e0\u6b64\uff0c\u5982\u679c\u4ece \\(I\\) \u5230 \\(z\\) \u7684\u6982\u7387\u4e3a \\(p\\) \uff0c\u7531\u4e8e \\(I + a\\) \u5c06\u9009\u62e9 \\(a\\) \u7684\u6982\u7387\u7531\u539f\u672c\u7684 \\(\\sigma(a|z[I])\\) \u53d8\u6210\u4e86 1.0\uff0c\u5b83\u5230 \\(z\\) \u7684\u6982\u7387\u5c31\u53d8\u6210\u4e86 \\(p / \\sigma(a|z[I])\\) \u3002 \u53e6\u4e00\u79cd\u60c5\u51b5\u662f\u9009\u62e9\u4e86\u5176\u4ed6\u884c\u52a8\uff0c\u6b64\u65f6 \\(I + a\\) \u4e0d\u662f \\(z\\) \u7684\u524d\u7f00\u3002\u6b64\u65f6\u7684 sampled counterfactual value \u662f\uff1a \\[ \\begin{align} r(I, a) &= 0 - v_i(\\sigma, I) \\\\ &= - \\frac{\\pi_{-i}^\\sigma (z[I]) \\pi^\\sigma (z[I] a, z) u_i(z)}{q(z)} \\sigma(a|z[I]) \\end{align} \\] \u9009\u53d6 0 \u662f\u56e0\u4e3a\u8be5\u52a8\u4f5c\u7684\u540e\u6094\u503c\u66f4\u65b0\u4e0d\u5f52 \\(z\\) \u7ba1\u3002\u5fc5\u7136\u6709\u522b\u7684 terminal node \u7684\u524d\u7f00\u662f \\(I + a\\) \uff0c\u5f53\u91c7\u6837\u5230\u8fd9\u4e9b terminal node \u7684\u65f6\u5019\u81ea\u7136\u4f1a\u66f4\u65b0\u3002 \u6211\u4eec\u4ee4\uff1a \\[ w_I = \\frac{\\pi_{-i}^\\sigma (z[I]) \\pi^\\sigma (z[I] a, z) u_i(z)}{q(z)} \\] \u7531\u4e8e\u5728 outcome sampling \u4e2d\uff0c\u6bcf\u4e2a terminal node \\(z\\) \u90fd\u5bf9\u5e94\u4e00\u4e2a block \\(Q\\) \uff0c\u56e0\u6b64 \\(q(z)\\) \u5c31\u662f\u9009\u4e2d \\(Q\\) \u7684\u6982\u7387\u3002\u7406\u8bba\u4e0a\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u91c7\u6837\u7684\u771f\u5b9e\u6027\uff0c\u8fd9\u4e2a\u6982\u7387\u5e94\u8be5\u4e0e\u6240\u6709\u73a9\u5bb6\u9075\u5faa\u7b56\u7565 \\(\\sigma\\) \u8fdb\u884c\u6e38\u620f\u5230\u8fbe \\(z\\) \u7684\u6982\u7387\u76f8\u540c\uff0c\u5373 \\[q(z) = \\pi^\\sigma(z)\\] \u6839\u636e\u5b9a\u4e49\uff0c \\(\\pi_{-i}^\\sigma (z)\\) \u662f\u5bf9\u624b\u5230\u8fbe \\(z\\) \u7684\u6982\u7387\uff0c\u5728\u8ba1\u7b97\u65f6\uff0c\u5047\u8bbe\u73a9\u5bb6 \\(i\\) \u4ee5 1.0 \u7684\u6982\u7387\u9009\u62e9\u52a8\u4f5c\uff0c\u800c \\(\\pi_i^\\sigma (z)\\) \u76f8\u53cd\u3002\u4e24\u8005\u7684\u4e58\u79ef\u5c31\u662f\u4ece\u5c40\u5916\u65c1\u89c2\u8005\u770b\u6765\u5230\u8fbe \\(z\\) \u7684\u6982\u7387\u3002\u56e0\u6b64\u6709\uff1a \\[ \\begin{align} q(z) &= \\pi^\\sigma(z) \\\\ &= \\pi^\\sigma(z[I]) \\cdot \\pi^\\sigma(z[I], z) \\\\ &= \\pi_i^\\sigma (z[I]) \\cdot \\pi_{-i}^\\sigma (z[I]) \\cdot \\pi^\\sigma(z[I], z) \\end{align} \\] \u5e26\u5165\u5f97\u5230\uff1a \\[\\begin{align} w_I &= \\frac{\\pi_{-i}^\\sigma (z[I]) \\pi^\\sigma (z[I] a, z) u_i(z)}{q(z)} \\\\ &= \\frac{\\pi^\\sigma (z[I] a, z) u_i(z)}{ \\pi_i^\\sigma (z[I]) \\cdot \\pi^\\sigma(z[I], z)} \\\\ &= \\frac{\\pi_i^\\sigma(z[I]a, z) u_i(z)}{\\pi_i^\\sigma(z)} \\\\ &= \\frac{u_i(z)}{ \\pi_i^\\sigma (z[I]) \\cdot \\sigma(a|z[I])} \\end{align}\\] \u6ce8\u610f\u5728\u8fd9\u4e2a\u8868\u8fbe\u5f0f\u4e2d\uff0c\u6211\u4eec\u4e0d\u518d\u9700\u8981\u5bf9\u624b\u7684\u4fe1\u606f \uff08\u53ea\u6709\u5e26\u4e0b\u6807 \\(i\\) \u7684\u90e8\u5206\u4e86\uff09\u3002\u6211\u4eec\u5728\u5316\u7b80\u4e2d\u523b\u610f\u6d88\u53bb\u4e86\u5bf9\u624b \\(-i\\) \u76f8\u5173\u7684\u4fe1\u606f\u3002 \u6bcf\u6b21\u8fed\u4ee3\u7d2f\u8ba1\u540e\u6094\u503c\u4f1a\u589e\u52a0\uff1a \\[ r(I,a) = \\begin{cases} w_I \\cdot (1 - \\sigma(a|z[I]) & \\text{if} ~ z(z[I]a) \\sqsubset z \\\\ -w_I \\cdot \\sigma(a|z[I]) & \\text{otherwise} \\end{cases}\\] \u8fd9\u6837\u8ba1\u7b97\u53ef\u4ee5\u4fdd\u8bc1\u7d2f\u8ba1\u540e\u6094\u503c\u7684\u671f\u671b\u662f\u4e0e\u4f20\u7edf cfr \u76f8\u540c\u7684\u3002 \u6bcf\u6b21\u8fed\u4ee3\uff0c\u6bcf\u4e2a\u52a8\u4f5c\u7684\u7d2f\u8ba1 strategy \u589e\u52a0\uff1a \\[s(I, a) = \\pi_i^\\sigma(z[I]) \\cdot \\sigma(a|z[I]) \\] \u6ce8\uff1a\u8bba\u6587\u4e2d\u8fd8\u589e\u52a0\u4e86 \\(t - c_I\\) \u6743\u91cd\uff0c\u5373\u672c\u6b21 iteration \u548c\u4e0a\u6b21\u8bbf\u95ee \\(I\\) \u7684 iteration \u4e4b\u5dee\uff0c\u4f46\u662f\u6211\u52a0\u4e0a\u6743\u91cd\u540e\u65e0\u6cd5\u5f97\u5230\u6b63\u786e\u7ed3\u679c\uff0c\u5c1a\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002 \u5b9e\u73b0\u7ec6\u8282 \u76f8\u6bd4 cfr\uff0cmccfr \u7684\u4e0d\u540c\u4e3b\u8981\u6709\uff1a 1. \u65e0\u9700\u5bf9\u624b\uff08-i\uff09\u7684\u4fe1\u606f 2. \u65e0\u9700\u904d\u5386\u6240\u6709\u884c\u52a8 3. \u8ba1\u7b97\u540e\u6094\u503c\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u9700\u8981\u589e\u52a0\u4e00\u4e2a\u57fa\u4e8e\u91c7\u6837\u6982\u7387\u7684\u6743\u91cd\u3002 4. \u9700\u8981\u5f15\u5165 epsilon \u4fdd\u8bc1\u63a2\u7d22\u6240\u6709 action fn mccfr ( & mut self , history : kuhn :: ActionHistory , cards : & Vec < i32 > , reach_probs : HashMap < i32 , f64 > , ) -> f64 { // current active player let player_id = ( history . 0. len () % 2 ) as i32 ; let opponent_id = 1 - player_id ; let maybe_payoff = kuhn :: get_payoff ( & history , cards ); if maybe_payoff . is_some () { let payoff = match player_id { 0 => maybe_payoff . unwrap (), 1 => - maybe_payoff . unwrap (), _ => panic! ( \"unexpected player id {}\" , player_id ), }; return payoff as f64 ; } // not the terminal node let info_set = InformationSet { action_history : history . clone (), hand_card : cards [ player_id as usize ], }; if self . cfr_info . contains_key ( & info_set ) == false { self . cfr_info . insert ( info_set . clone (), CfrNode :: new ( 0.06 )); } let action_probs = self . cfr_info . get ( & info_set ) . unwrap () . get_action_probability (); let chosen_action_id = sample ( & action_probs ); let chosen_action = kuhn :: Action :: from_int ( chosen_action_id ); let chosen_action_prob = action_probs [ chosen_action_id as usize ]; let mut next_history = history . clone (); next_history . 0. push ( chosen_action ); // modify reach prob for SELF (not opponent) // update history probability let mut next_reach_probs = reach_probs . clone (); * next_reach_probs . get_mut ( & player_id ). unwrap () *= chosen_action_prob ; // recursive call // final payoff of the terminal node let final_payoff = - self . mccfr ( next_history , cards , next_reach_probs ); // update regret value let node = self . cfr_info . get_mut ( & info_set ). unwrap (); for ( action_id , action_prob ) in action_probs . iter (). enumerate () { let action = kuhn :: Action :: from_int ( action_id ); // reach probability of SELF (not opponent) let weight = final_payoff / reach_probs [ & player_id ] / action_prob ; if action == chosen_action { node . cum_regrets [ action_id ] += weight * ( 1.0 - action_prob ); } else { node . cum_regrets [ action_id ] += - weight * action_prob ; } } // update strategy for ( action_id , action_prob ) in action_probs . iter (). enumerate () { node . cum_strategy [ action_id ] += action_prob * reach_probs [ & player_id ]; } return final_payoff ; } \u7ed3\u679c Explore with epsilon=0.06 . Average payoff = -0.05138 History Check Probability Bet Probability 1 0.805 0.195 1C 0.68 0.32 1B 0.97 0.03 1CB 0.97 0.03 2 0.97 0.03 2C 0.94 0.05 2B 0.56 0.44 2CB 0.47 0.53 3 0.422 0.578 3C 0.03 0.97 3B 0.03 0.97 3CB 0.03 0.97 We get 0.03 here because we choose epsilon = 0.06 and we have 2 actions to choose from. Reference Monte Carlo Counterfactual Regret Minimization MCCFR technical report","title":"Monte Carlo CFR"},{"location":"mathematics/CFR/MonteCarloCfr/#monte-carlo-cfr","text":"\u5728\u4e0a\u4e00\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6211\u4eec\u8bb2\u4e86\u7ecf\u5178\u7684 CFR \u7b97\u6cd5\u7684\u539f\u7406\u53ca\u5b9e\u73b0\u3002\u4f46\u662f\uff0c\u5b83\u4e5f\u6709\u4e24\u4e2a\u81f4\u547d\u7684\u7f3a\u9677\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5e94\u7528\u5230\u5b9e\u9645\u7684\u590d\u6742\u535a\u5f08\u4e2d\uff1a \u5b83\u9700\u8981\u904d\u5386\u6574\u4e2a\u6e38\u620f\u6811 \u5b83\u9700\u8981\u77e5\u9053\u5bf9\u624b\u7684\u7b56\u7565\uff0c\u8fd9\u5728\u5b9e\u9645\u60c5\u51b5\u4e0b\u5f88\u96be\u6ee1\u8db3 \u672c\u6587\u5c06\u4ecb\u7ecd\u57fa\u4e8e Monte Carlo \u7684\u6539\u8fdb\u7b97\u6cd5\u3002\u5168\u90e8\u4ee3\u7801\u53ef\u4ee5\u53c2\u8003\u6211\u7684 github \u9879\u76ee\uff1a cfr-kuhn .","title":"Monte Carlo CFR"},{"location":"mathematics/CFR/MonteCarloCfr/#definition","text":"\u6838\u5fc3\u601d\u60f3\u662f\uff0c\u5728\u907f\u514d\u904d\u5386\u6574\u4e2a\u6e38\u620f\u6811\u7684\u524d\u63d0\u4e0b\uff0c\u6bcf\u4e2a information set \u7684\u6bcf\u4e2a action \u7684 counterfactual regret \u671f\u671b\u503c\u4fdd\u6301\u4e0d\u53d8\u3002 \u4ee4 \\(\\mathcal{Q} = \\{Q_1, Q_2, ..., Q_r \\}\\) \u662f\u6240\u6709 terminal node \\(Z\\) \u7684\u5b50\u96c6\uff0c\u5176\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20 \\(Q_j\\) \u6211\u4eec\u79f0\u4e3a block \uff0c\u6bcf\u4e2a block \u53ef\u80fd\u5305\u542b\u591a\u4e2a terminal node \u3002\u4ee4 \\(q_j > 0\\) \u662f\u9009\u62e9 \\(Q_j\\) \u7684\u6982\u7387\uff0c\u6ee1\u8db3 \\(\\sum_{j=1}^r q_j = 1\\) \u3002\u6bcf\u6b21\u8fed\u4ee3\uff0c\u6211\u4eec\u90fd\u4f1a\u4ece\u8fd9\u4e9b blocks \u4e2d\u91c7\u6837\u4e00\u4e2a\u5e76\u4e14\u53ea\u8003\u8651\u5728\u8be5 block \u4e2d\u7684 terminal node \u3002 \u56de\u5fc6 CFR \u7ecf\u5178\u7248\u7684 counterfactual value \u8ba1\u7b97\u516c\u5f0f\uff1a \\[ v_i(\\sigma, I) = \\sum_{z \\in Z_I } \\pi_{-i}^{\\sigma} (z[I]) \\pi ^{\\sigma} (z[I], z) u_i (z) \\] \u5728 MCCFR \u4e2d\uff0c\u5bf9\u4e8e block \\(Q_j\\) \u7684 sampled counterfactual value \u53ef\u4ee5\u8868\u793a\u4e3a\uff1a \\[v_i(\\sigma, I | j) = \\sum_{z \\in Q_j \\cap Z_I} \\frac{1}{q(z)} \\pi_{-i}^\\sigma (z[I]) \\pi^\\sigma (z[I], z) u_i(z)\\] \u5176\u4e2d\uff1a \\(z\\) \u662f terminal node \\(z_I\\) \u8868\u793a information set \\(I\\) \u53ef\u4ee5\u5230\u8fbe\u7684 \\(z\\) \u7684\u5168\u96c6 \\(z[I]\\) \u8868\u793a\u67d0\u4e2a\u53ef\u4ee5\u5230\u8fbe \\(z\\) \u7684 information set \\(i\\) \u8868\u793a\u73a9\u5bb6 id \\(j\\) \u8868\u793a block id \\(q(z)\\) \u8868\u793a \\(z\\) \u6240\u5728 block \u88ab\u9009\u4e2d\u7684\u6982\u7387\u4e4b\u548c\uff08\u4e00\u4e2a terminal node \u53ef\u4ee5\u88ab\u5206\u5230\u591a\u4e2a block \u4e2d\uff09 \u6211\u4eec\u53ef\u4ee5\u8bc1\u660e counterfactual value \u548c sampled counterfactual value \u671f\u671b\u662f\u76f8\u7b49\u7684\u3002 \u8fd9\u5c31\u662f MCCFR \u7684\u57fa\u672c\u601d\u60f3\u4e86\u3002\u4e8b\u5b9e\u4e0a\uff0cCFR \u53ef\u4ee5\u770b\u4f5c MCCFR \u5728 \\(\\mathcal{Q} = \\{Z\\}\\) \u4e14 \\(q_1 = 1.0\\) \u7684\u7279\u4f8b\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u53ef\u4ee5__\u5c06\u6bcf\u4e00\u4e2a block \u9009\u4e3a chance node \u7684\u4e00\u4e2a\u5206\u652f__\u3002\u4ee5 Kuhn Poker \u4e3a\u4f8b\uff0c\u6211\u4eec\u5bf9\u4e8e\u624b\u724c 1\uff0c2\uff0c3 \u53ef\u4ee5\u5efa\u7acb 3 \u4e2a block\uff1a \\(\\{ Q_1, Q_2, Q_3 \\}\\) \uff0c\u5206\u522b\u5305\u542b\u624b\u724c\u4e3a 1\uff0c2\uff0c3 \u7684\u6240\u6709 terminal nodes\u3002\u4ece\u800c\uff0c\u6bcf\u4e2a block \u7684\u6982\u7387\u4e5f\u81ea\u7136\u4e3a \\(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\) \u3002\u8fd9\u5c31\u662f chance-sampled CFR\u3002","title":"Definition"},{"location":"mathematics/CFR/MonteCarloCfr/#outcome-sampling-mccfr","text":"\u5728 outcome-sampling MCCFR \u4e2d\uff0c\u6211\u4eec__\u628a\u6bcf\u4e2a terminal node \u4f5c\u4e3a\u4e00\u4e2a block__\u3002\u6bcf\u6b21\u8fed\u4ee3\uff0c\u6211\u4eec\u62bd\u53d6\u4e00\u4e2a terminal node \u5e76\u66f4\u65b0\u5b83\u7684\u524d\u7f00 information set\u3002\u4e00\u4e2a terminal node \u51fa\u73b0\u6982\u7387\u8d8a\u9ad8\uff0c\u6211\u4eec\u91c7\u6837\u6982\u7387\u4e5f\u8d8a\u9ad8\u3002\u56e0\u6b64\u6211\u4eec\u5fc5\u987b\u5f97\u5230\u6bcf\u4e2a terminal node j \u51fa\u73b0\u7684\u6982\u7387\u4f5c\u4e3a \\(q_j\\) \u3002\u5982\u4f55\u5f97\u5230\u5462\uff1f \u5728 CFR \u4e2d\uff0c\u6211\u4eec\u5728\u8ba1\u7b97 regret \u7684\u65f6\u5019\uff0c\u4f1a\u8ba1\u7b97\u5230\u8fbe\u6bcf\u4e2a history \u7684\u6982\u7387\u3002\u5bf9\u4e8e terminal node j\uff0c\u8fd9\u4e2a\u6982\u7387\u5176\u5b9e\u5c31\u662f\u91c7\u6837\u6982\u7387 \\(q_j\\) \u3002 CFR \u7b97\u6cd5\u5305\u542b\u4e86\u4e24\u4e2a\u65b9\u5411\u7684\u904d\u5386\uff1a \u524d\u5411\u904d\u5386\u3002\u7528\u4e8e\u8ba1\u7b97\u6bcf\u4e2a\u73a9\u5bb6\u4ece\u539f\u70b9\u5230\u8fbe\u5f53\u524d history \u7684\u6982\u7387 \\(\\pi_i^{\\sigma}(h)\\) \u540e\u5411\u904d\u5386\u3002\u7528\u4e8e\u8ba1\u7b97\u6bcf\u4e2a\u73a9\u5bb6\u4ece\u5f53\u524d history \u8fdb\u884c\u5230 terminal node \u7684\u6982\u7387 \\(\\pi_i^{\\sigma}(h,z)\\) \uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\uff0c\u8fd8\u4f1a\u8ba1\u7b97 sampled counterfactual regrets\u3002 \u540e\u5411\u904d\u5386\u4e2d\u8ba1\u7b97 sampled counterfactual regrets \u4f1a\u5206\u4e3a\u4e24\u79cd\u60c5\u51b5\u3002 \u5176\u4e00\u662f\u9009\u62e9\u4e86\u80fd\u901a\u5411\u5f53\u524d terminal node \\(z\\) \u7684 action\u3002\u5373\uff0c\u5728information set \\(I\\) \u91c7\u53d6\u4e86\u884c\u52a8 \\(a\\) \uff0c\u6b64\u65f6\u8ba1\u7b97\u65b9\u5f0f\u4e3a\uff1a \\[ \\begin{align} r(I, a) &= v_i(\\sigma_{I \\rightarrow a}, I) - v_i(\\sigma, I) \\\\ &= \\frac{\\pi_{-i}^\\sigma (z[I]) \\pi^\\sigma (z[I]a, z) u_i(z)}{q(z)} - \\frac{\\pi_{-i}^\\sigma (z[I]) \\pi^\\sigma (z[I], z) u_i(z)}{q(z)} \\\\ &= \\frac{\\pi_{-i}^\\sigma (z[I]) \\pi^\\sigma (z[I] a, z) u_i(z)}{q(z)} ( 1 - \\sigma(a|z[I])) \\end{align} \\] \u4e0a\u5f0f\u6bd4\u8f83\u96be\u4ee5\u7406\u89e3\u7684\u662f\uff1a \\[\\pi^\\sigma (z[I] a, z) \\cdot \\sigma(a|z[I]) = \\pi^\\sigma (z[I], z) \\] \u7ed3\u5408\u5b9a\u4e49\uff0c \\(\\pi^\\sigma (z[I] a, z)\\) \u662f\u4ece \\(I + a\\) \u5230\u8fbe \\(z\\) \u7684\u6982\u7387\uff0c\u76f8\u6bd4 \\(\\pi^\\sigma (z[I], z)\\) \u591a\u524d\u8fdb\u4e86\u4e00\u6b65\uff08\u9009\u62e9\u884c\u52a8 \\(a\\) \uff09\uff0c\u5728 \\(I\\) \u9009\u62e9 \\(a\\) \u7684\u6982\u7387\u662f \\(\\sigma(a|z[I])\\) \uff0c\u56e0\u6b64\uff0c\u5982\u679c\u4ece \\(I\\) \u5230 \\(z\\) \u7684\u6982\u7387\u4e3a \\(p\\) \uff0c\u7531\u4e8e \\(I + a\\) \u5c06\u9009\u62e9 \\(a\\) \u7684\u6982\u7387\u7531\u539f\u672c\u7684 \\(\\sigma(a|z[I])\\) \u53d8\u6210\u4e86 1.0\uff0c\u5b83\u5230 \\(z\\) \u7684\u6982\u7387\u5c31\u53d8\u6210\u4e86 \\(p / \\sigma(a|z[I])\\) \u3002 \u53e6\u4e00\u79cd\u60c5\u51b5\u662f\u9009\u62e9\u4e86\u5176\u4ed6\u884c\u52a8\uff0c\u6b64\u65f6 \\(I + a\\) \u4e0d\u662f \\(z\\) \u7684\u524d\u7f00\u3002\u6b64\u65f6\u7684 sampled counterfactual value \u662f\uff1a \\[ \\begin{align} r(I, a) &= 0 - v_i(\\sigma, I) \\\\ &= - \\frac{\\pi_{-i}^\\sigma (z[I]) \\pi^\\sigma (z[I] a, z) u_i(z)}{q(z)} \\sigma(a|z[I]) \\end{align} \\] \u9009\u53d6 0 \u662f\u56e0\u4e3a\u8be5\u52a8\u4f5c\u7684\u540e\u6094\u503c\u66f4\u65b0\u4e0d\u5f52 \\(z\\) \u7ba1\u3002\u5fc5\u7136\u6709\u522b\u7684 terminal node \u7684\u524d\u7f00\u662f \\(I + a\\) \uff0c\u5f53\u91c7\u6837\u5230\u8fd9\u4e9b terminal node \u7684\u65f6\u5019\u81ea\u7136\u4f1a\u66f4\u65b0\u3002 \u6211\u4eec\u4ee4\uff1a \\[ w_I = \\frac{\\pi_{-i}^\\sigma (z[I]) \\pi^\\sigma (z[I] a, z) u_i(z)}{q(z)} \\] \u7531\u4e8e\u5728 outcome sampling \u4e2d\uff0c\u6bcf\u4e2a terminal node \\(z\\) \u90fd\u5bf9\u5e94\u4e00\u4e2a block \\(Q\\) \uff0c\u56e0\u6b64 \\(q(z)\\) \u5c31\u662f\u9009\u4e2d \\(Q\\) \u7684\u6982\u7387\u3002\u7406\u8bba\u4e0a\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u91c7\u6837\u7684\u771f\u5b9e\u6027\uff0c\u8fd9\u4e2a\u6982\u7387\u5e94\u8be5\u4e0e\u6240\u6709\u73a9\u5bb6\u9075\u5faa\u7b56\u7565 \\(\\sigma\\) \u8fdb\u884c\u6e38\u620f\u5230\u8fbe \\(z\\) \u7684\u6982\u7387\u76f8\u540c\uff0c\u5373 \\[q(z) = \\pi^\\sigma(z)\\] \u6839\u636e\u5b9a\u4e49\uff0c \\(\\pi_{-i}^\\sigma (z)\\) \u662f\u5bf9\u624b\u5230\u8fbe \\(z\\) \u7684\u6982\u7387\uff0c\u5728\u8ba1\u7b97\u65f6\uff0c\u5047\u8bbe\u73a9\u5bb6 \\(i\\) \u4ee5 1.0 \u7684\u6982\u7387\u9009\u62e9\u52a8\u4f5c\uff0c\u800c \\(\\pi_i^\\sigma (z)\\) \u76f8\u53cd\u3002\u4e24\u8005\u7684\u4e58\u79ef\u5c31\u662f\u4ece\u5c40\u5916\u65c1\u89c2\u8005\u770b\u6765\u5230\u8fbe \\(z\\) \u7684\u6982\u7387\u3002\u56e0\u6b64\u6709\uff1a \\[ \\begin{align} q(z) &= \\pi^\\sigma(z) \\\\ &= \\pi^\\sigma(z[I]) \\cdot \\pi^\\sigma(z[I], z) \\\\ &= \\pi_i^\\sigma (z[I]) \\cdot \\pi_{-i}^\\sigma (z[I]) \\cdot \\pi^\\sigma(z[I], z) \\end{align} \\] \u5e26\u5165\u5f97\u5230\uff1a \\[\\begin{align} w_I &= \\frac{\\pi_{-i}^\\sigma (z[I]) \\pi^\\sigma (z[I] a, z) u_i(z)}{q(z)} \\\\ &= \\frac{\\pi^\\sigma (z[I] a, z) u_i(z)}{ \\pi_i^\\sigma (z[I]) \\cdot \\pi^\\sigma(z[I], z)} \\\\ &= \\frac{\\pi_i^\\sigma(z[I]a, z) u_i(z)}{\\pi_i^\\sigma(z)} \\\\ &= \\frac{u_i(z)}{ \\pi_i^\\sigma (z[I]) \\cdot \\sigma(a|z[I])} \\end{align}\\] \u6ce8\u610f\u5728\u8fd9\u4e2a\u8868\u8fbe\u5f0f\u4e2d\uff0c\u6211\u4eec\u4e0d\u518d\u9700\u8981\u5bf9\u624b\u7684\u4fe1\u606f \uff08\u53ea\u6709\u5e26\u4e0b\u6807 \\(i\\) \u7684\u90e8\u5206\u4e86\uff09\u3002\u6211\u4eec\u5728\u5316\u7b80\u4e2d\u523b\u610f\u6d88\u53bb\u4e86\u5bf9\u624b \\(-i\\) \u76f8\u5173\u7684\u4fe1\u606f\u3002 \u6bcf\u6b21\u8fed\u4ee3\u7d2f\u8ba1\u540e\u6094\u503c\u4f1a\u589e\u52a0\uff1a \\[ r(I,a) = \\begin{cases} w_I \\cdot (1 - \\sigma(a|z[I]) & \\text{if} ~ z(z[I]a) \\sqsubset z \\\\ -w_I \\cdot \\sigma(a|z[I]) & \\text{otherwise} \\end{cases}\\] \u8fd9\u6837\u8ba1\u7b97\u53ef\u4ee5\u4fdd\u8bc1\u7d2f\u8ba1\u540e\u6094\u503c\u7684\u671f\u671b\u662f\u4e0e\u4f20\u7edf cfr \u76f8\u540c\u7684\u3002 \u6bcf\u6b21\u8fed\u4ee3\uff0c\u6bcf\u4e2a\u52a8\u4f5c\u7684\u7d2f\u8ba1 strategy \u589e\u52a0\uff1a \\[s(I, a) = \\pi_i^\\sigma(z[I]) \\cdot \\sigma(a|z[I]) \\] \u6ce8\uff1a\u8bba\u6587\u4e2d\u8fd8\u589e\u52a0\u4e86 \\(t - c_I\\) \u6743\u91cd\uff0c\u5373\u672c\u6b21 iteration \u548c\u4e0a\u6b21\u8bbf\u95ee \\(I\\) \u7684 iteration \u4e4b\u5dee\uff0c\u4f46\u662f\u6211\u52a0\u4e0a\u6743\u91cd\u540e\u65e0\u6cd5\u5f97\u5230\u6b63\u786e\u7ed3\u679c\uff0c\u5c1a\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002","title":"Outcome-Sampling MCCFR"},{"location":"mathematics/CFR/MonteCarloCfr/#_1","text":"\u76f8\u6bd4 cfr\uff0cmccfr \u7684\u4e0d\u540c\u4e3b\u8981\u6709\uff1a 1. \u65e0\u9700\u5bf9\u624b\uff08-i\uff09\u7684\u4fe1\u606f 2. \u65e0\u9700\u904d\u5386\u6240\u6709\u884c\u52a8 3. \u8ba1\u7b97\u540e\u6094\u503c\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u9700\u8981\u589e\u52a0\u4e00\u4e2a\u57fa\u4e8e\u91c7\u6837\u6982\u7387\u7684\u6743\u91cd\u3002 4. \u9700\u8981\u5f15\u5165 epsilon \u4fdd\u8bc1\u63a2\u7d22\u6240\u6709 action fn mccfr ( & mut self , history : kuhn :: ActionHistory , cards : & Vec < i32 > , reach_probs : HashMap < i32 , f64 > , ) -> f64 { // current active player let player_id = ( history . 0. len () % 2 ) as i32 ; let opponent_id = 1 - player_id ; let maybe_payoff = kuhn :: get_payoff ( & history , cards ); if maybe_payoff . is_some () { let payoff = match player_id { 0 => maybe_payoff . unwrap (), 1 => - maybe_payoff . unwrap (), _ => panic! ( \"unexpected player id {}\" , player_id ), }; return payoff as f64 ; } // not the terminal node let info_set = InformationSet { action_history : history . clone (), hand_card : cards [ player_id as usize ], }; if self . cfr_info . contains_key ( & info_set ) == false { self . cfr_info . insert ( info_set . clone (), CfrNode :: new ( 0.06 )); } let action_probs = self . cfr_info . get ( & info_set ) . unwrap () . get_action_probability (); let chosen_action_id = sample ( & action_probs ); let chosen_action = kuhn :: Action :: from_int ( chosen_action_id ); let chosen_action_prob = action_probs [ chosen_action_id as usize ]; let mut next_history = history . clone (); next_history . 0. push ( chosen_action ); // modify reach prob for SELF (not opponent) // update history probability let mut next_reach_probs = reach_probs . clone (); * next_reach_probs . get_mut ( & player_id ). unwrap () *= chosen_action_prob ; // recursive call // final payoff of the terminal node let final_payoff = - self . mccfr ( next_history , cards , next_reach_probs ); // update regret value let node = self . cfr_info . get_mut ( & info_set ). unwrap (); for ( action_id , action_prob ) in action_probs . iter (). enumerate () { let action = kuhn :: Action :: from_int ( action_id ); // reach probability of SELF (not opponent) let weight = final_payoff / reach_probs [ & player_id ] / action_prob ; if action == chosen_action { node . cum_regrets [ action_id ] += weight * ( 1.0 - action_prob ); } else { node . cum_regrets [ action_id ] += - weight * action_prob ; } } // update strategy for ( action_id , action_prob ) in action_probs . iter (). enumerate () { node . cum_strategy [ action_id ] += action_prob * reach_probs [ & player_id ]; } return final_payoff ; }","title":"\u5b9e\u73b0\u7ec6\u8282"},{"location":"mathematics/CFR/MonteCarloCfr/#_2","text":"Explore with epsilon=0.06 . Average payoff = -0.05138 History Check Probability Bet Probability 1 0.805 0.195 1C 0.68 0.32 1B 0.97 0.03 1CB 0.97 0.03 2 0.97 0.03 2C 0.94 0.05 2B 0.56 0.44 2CB 0.47 0.53 3 0.422 0.578 3C 0.03 0.97 3B 0.03 0.97 3CB 0.03 0.97 We get 0.03 here because we choose epsilon = 0.06 and we have 2 actions to choose from.","title":"\u7ed3\u679c"},{"location":"mathematics/CFR/MonteCarloCfr/#reference","text":"Monte Carlo Counterfactual Regret Minimization MCCFR technical report","title":"Reference"},{"location":"mathematics/CFR/RegretMatchingAndBlottoGame/","text":"Regret Matching and Blotto Game 1 \u57fa\u672c\u6982\u5ff5 2000 \u5e74\uff0cHart \u548c Mas-Colell \u4ecb\u7ecd\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u535a\u5f08\u8bba\u7b97\u6cd5 regret matching\u3002\u535a\u5f08\u53cc\u65b9\u901a\u8fc7\uff1a \u8bb0\u5f55\u540e\u6094\u503c \u6839\u636e\u540e\u6094\u503c\u7684\u6bd4\u4f8b\u5730\u9009\u62e9\u4e0b\u4e00\u6b65\u884c\u52a8 \u8fbe\u5230\u7eb3\u4ec0\u5747\u8861 (Nash equilibrium)\u3002\u8fd9\u4e2a\u80fd\u5f88\u597d\u5730\u89e3\u51b3\u6b63\u5219\u5f62\u5f0f\u7684\u535a\u5f08\uff08normal-form game\uff09\uff0c\u4f46\u662f\u5bf9\u6269\u5c55\u5f62\u5f0f\u7684\u535a\u5f08\uff08extensive-form game\uff09\u4e0d\u9002\u7528\u3002 \u6240\u8c13\u6b63\u5219\u5f62\u5f0f\uff0c\u662f\u4e00\u79cd\u63cf\u8ff0\u535a\u5f08\u7684\u65b9\u5f0f\u3002\u6b63\u5219\u5f62\u5f0f\u7528 n \u7ef4\u77e9\u9635\u6765\u63cf\u8ff0\u535a\u5f08\uff0c\u800c\u6269\u5c55\u5f62\u5f0f\u4f7f\u7528\u56fe\u3002\u6b63\u5219\u5f62\u5f0f\u53ea\u80fd\u63cf\u8ff0\u73a9\u5bb6\u540c\u65f6\u884c\u52a8\u7684\u535a\u5f08\u3002 1.1 \u535a\u5f08\u7684\u6b63\u5219\u5f62\u5f0f\u63cf\u8ff0 \u6b63\u5219\u5f62\u5f0f\u7684\u77e9\u9635\u63cf\u8ff0\u6709\u4ee5\u4e0b\u51e0\u4e2a\u8981\u7d20\uff1a 1. \u73a9\u5bb6\u6570\u91cf n \u5373\u7ef4\u5ea6 2. \u7ef4\u5ea6 i \u4e0a\u7684\u503c\u662f\u73a9\u5bb6 i \u7684\u884c\u52a8 3. \u77e9\u9635\u7684\u5143\u7d20\u662f\u5956\u52b1( payoff ) \u4f8b\u5982\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u5982\u4e0b\u77e9\u9635\u6765\u63cf\u8ff0\u526a\u5200\u77f3\u5934\u5e03\u6e38\u620f\uff1a \u526a\u5200 \u77f3\u5934 \u5e03 \u526a\u5200 (0, 0) (-1, 1) (1, -1) \u77f3\u5934 (1, -1) (0, 0) (-1, 1) \u5e03 (-1, 1) (1, -1) (0, 0) \u5982\u679c\u77e9\u9635\u4e2d\u6240\u6709 payoff \u7684\u503c\u7684\u548c\u4e3a 0\uff0c\u5219\u79f0\u4e3a\u96f6\u548c\u535a\u5f08\u3002 1.2 \u73a9\u5bb6\u7b56\u7565 \u5982\u679c\u67d0\u4e2a\u73a9\u5bb6\u4ee5 100% \u7684\u6982\u7387\u91c7\u53d6\u4e00\u4e2a\u884c\u52a8 \uff08\u4f8b\u5982\u5fb7\u6251\u5168\u7a0b all in\uff09\uff0c\u79f0\u4e3a pure strategy\u3002\u5982\u679c\u4e00\u4e2a\u73a9\u5bb6\u53ef\u80fd\u91c7\u53d6\u591a\u79cd\u884c\u52a8\uff0c\u5c31\u79f0\u4e3a mixed strategy\u3002 \u6211\u4eec\u4f7f\u7528 \\(\\sigma\\) \u8868\u793a mixed strategy\uff0c \\(\\sigma_i(s)\\) \u8868\u793a\u73a9\u5bb6 \\(i\\) \u9009\u62e9\u884c\u52a8 \\(s\\) \u7684\u6982\u7387\uff0c \\(-i\\) \u8868\u793a \\(i\\) \u7684\u5bf9\u624b\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u6cd5\u8ba1\u7b97\u73a9\u5bb6\u7684\u671f\u671b payoff\uff1a \\[u_i(\\sigma_i, \\sigma_{-i}) = \\sum_{s \\in S_i}\\sum_{s' \\in S_{-i}} \\sigma_i(s) \\sigma_{-i}(s')u_i(s, s')\\] \u5176\u5b9e\u5c31\u662f\u52a0\u6743\u6c42\u548c\u3002 2 Regret Matching and Minimization Regret matching \u7b97\u6cd5\u53ea\u80fd\u7528\u4e8e\u6b63\u5219\u5f62\u5f0f\u7684\u535a\u5f08\u3002\u5176\u57fa\u672c\u601d\u60f3\u4e3a\u6839\u636e payoff \u5bf9\u4e4b\u524d\u7684\u884c\u52a8\u4f5c\u6c42\u53cd\u6094\u503c\u3002\u518d\u5229\u7528\u7d2f\u8ba1\u7684\u53cd\u6094\u503c\u6307\u5bfc\u4e0b\u4e00\u6b65\u884c\u52a8\u3002 \u4ee5\u521a\u624d\u7684\u77f3\u5934\u526a\u5200\u5e03\u6e38\u620f\u4e3a\u4f8b\uff0c\u6211\u4eec\u51fa\u4e86\u77f3\u5934\uff0c\u5bf9\u624b\u51fa\u4e86\u5e03\uff0c\u6211\u4eec\u8f93\u6389\u4e86 1 \u5757\u94b1\uff0cpayoff \u662f -1\u3002\u6211\u4eec\u5982\u679c\u51fa\u5e03\uff0c\u662f\u5e73\u5c40\uff0cpayoff \u662f 0\uff0c\u5982\u679c\u51fa\u526a\u5200\uff0c\u5c31\u8d62\u4e86\uff0cpayoff \u662f 1\u3002payoff \u5dee\u503c\u5373\u53cd\u6094\u503c\u5206\u522b\u662f (0\uff0c1\uff0c2)\u3002\u5c06\u5176 normalize\uff0c\u4e0b\u4e00\u6b21\u51fa\u77f3\u5934\u3001\u5e03\u3001\u526a\u5200\u7684\u6982\u7387\u5206\u522b\u662f (0\uff0c1/3\uff0c2/3)\u3002 \u5047\u8bbe\u7b2c\u4e8c\u6b21\u6211\u4eec\u51fa\u4e86\u526a\u5200\uff0c\u5bf9\u624b\u51fa\u4e86\u77f3\u5934\u3002\u6211\u4eec\u518d\u6b21\u8f93\u6389\u4e86 1 \u5757\u94b1\u3002\u8fd9\u6b21\u6211\u4eec\u5bf9\u77f3\u5934\u3001\u5e03\u3001\u526a\u5200\u7684\u53cd\u6094\u503c\u5206\u522b\u662f(1\uff0c2\uff0c0)\u3002\u7d2f\u52a0\u5230\u4e4b\u524d\u7684 (0\uff0c1\uff0c2) \u4e0a\u4e3a (1\uff0c3\uff0c2)\u3002\u4e0b\u4e00\u6b21\u51fa\u77f3\u5934\u3001\u5e03\u3001\u526a\u5200\u7684\u6982\u7387\u4e3a (1/6, 3/6, 2/6)\u3002 Exercise: Colonel Blotto Colonel Blotto and his arch-enemy, Boba Fett, are at war. Each commander has S soldiers in total, and each soldier can be assigned to one of N < S battlefields. Naturally, these commanders do not communicate and hence direct their soldiers independently. Any number of soldiers can be allocated to each battlefield, including zero. A commander claims a battlefield if they send more soldiers to the battlefield than their opponent. The commander\u2019s job is to break down his pool of soldiers into groups to which he assigned to each battlefield. The winning commander is the one who claims the most battlefields. For example, with (S,N) = (10,4) a Colonel Blotto may choose to play (2,2,2,4) while Boba Fett may choose to play (8,1,1,0). In this case, Colonel Blotto would win by claiming three of the four battlefields. The war ends in a draw if both commanders claim the same number of battlefields. \u8ba9\u4e24\u4e2a\u4f7f\u7528 regret matching \u7684\u73a9\u5bb6\u8fdb\u884c\u5bf9\u6218\uff0c\u9009\u5b9a S=5 \u548c N=3\uff0c\u627e\u5230 Nash Equilibrium\u3002 \u8981\u70b9\uff1a \u5217\u51fa\u6240\u6709\u5206\u914d\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u53ef\u80fd\u7684 action set\uff0c\u4ece 0 \u5f00\u59cb\u4f9d\u6b21\u7f16\u53f7 \u7528\u4e00\u4e2a 2 \u7ef4\u77e9\u9635\u5b58\u50a8 action \u5bf9 action \u7684\u80dc\u8d1f\u8868\uff0c\u5373\u5f97\u5230 blotto \u535a\u5f08\u7684\u77e9\u9635\u63cf\u8ff0 \u5404\u4e2a\u73a9\u5bb6\u6839\u636e\u5728\u5177\u6709\u6b63\u53cd\u6094\u503c\u7684 action \u4e2d\uff0c\u6839\u636e\u53cd\u6094\u503c\u968f\u673a\u9009\u62e9\u4e00\u4e2a action\u3002\u82e5\u4e0d\u5b58\u5728\u6b63\u53cd\u6094\u503c\u7684 action\uff08\u4f8b\u5982\u7b2c\u4e00\u8f6e\uff09\uff0c\u5219\u968f\u673a\u9009\u62e9\u521d\u59cb action\u3002 \u5404\u4e2a\u73a9\u5bb6\u5728\u6e38\u620f\u7ed3\u675f\u65f6\u5f97\u5230\u5bf9\u624b\u7684 action\uff0c\u5e76\u66f4\u65b0\u81ea\u5df1\u7684\u53cd\u6094\u503c \u4ee3\u7801\u5b9e\u73b0\u89c1 https://github.com/double-free/cfr \uff0c\u8fd0\u884c 10 \u4e07\u6b21\u7ed3\u679c\u5982\u4e0b\uff08\u7ed3\u679c\u5177\u6709\u4e00\u5b9a\u968f\u673a\u6027\uff09\u3002 \u53ef\u80fd\u7684\u5206\u914d\u65b9\u5f0f\uff1a [0, 0, 5], [0, 1, 4], [0, 2, 3], [0, 3, 2], [0, 4, 1], [0, 5, 0], [1, 0, 4], [1, 1, 3], [1, 2, 2], [1, 3, 1], [1, 4, 0], [2, 0, 3], [2, 1, 2], [2, 2, 1], [2, 3, 0], [3, 0, 2], [3, 1, 1], [3, 2, 0], [4, 0, 1], [4, 1, 0], [5, 0, 0], \u6bcf\u4e2a\u5206\u914d\u65b9\u5f0f\u5bf9\u5e94\u7684\u53cd\u6094\u503c\uff1a player 1: for opponent 0, regret sum for each action [-55002, -10612, -45, -41, -11250, -55644, -10426, 25, -11075, 127, -11068, 109, -11107, -11009, 207, -74, -92, 20, -11636, -11640, -56029] \u6700\u7ec8\u8fd8\u5177\u6709\u6b63\u53cd\u6094\u503c\u7684 action\uff1a player 1: for opponent 0, candidate strategy Allocation { soldiers: [1, 1, 3] } has positive regret 25 player 1: for opponent 0, candidate strategy Allocation { soldiers: [1, 3, 1] } has positive regret 127 player 1: for opponent 0, candidate strategy Allocation { soldiers: [2, 0, 3] } has positive regret 109 player 1: for opponent 0, candidate strategy Allocation { soldiers: [2, 3, 0] } has positive regret 207 player 1: for opponent 0, candidate strategy Allocation { soldiers: [3, 2, 0] } has positive regret 20 \u7ed3\u679c\u8fd8\u662f\u76f8\u5bf9\u7b26\u5408\u76f4\u89c9\u7684\u3002 Reference \u53cd\u4e8b\u5b9e\u540e\u6094\u6700\u5c0f\u5316 An Introduction to Counterfactual Regret Minimization Game Basics Monte Carlo Tree Search Counterfactual Regret Minimization","title":"Regret Matching and Blotto Game"},{"location":"mathematics/CFR/RegretMatchingAndBlottoGame/#regret-matching-and-blotto-game","text":"","title":"Regret Matching and Blotto Game"},{"location":"mathematics/CFR/RegretMatchingAndBlottoGame/#1","text":"2000 \u5e74\uff0cHart \u548c Mas-Colell \u4ecb\u7ecd\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u535a\u5f08\u8bba\u7b97\u6cd5 regret matching\u3002\u535a\u5f08\u53cc\u65b9\u901a\u8fc7\uff1a \u8bb0\u5f55\u540e\u6094\u503c \u6839\u636e\u540e\u6094\u503c\u7684\u6bd4\u4f8b\u5730\u9009\u62e9\u4e0b\u4e00\u6b65\u884c\u52a8 \u8fbe\u5230\u7eb3\u4ec0\u5747\u8861 (Nash equilibrium)\u3002\u8fd9\u4e2a\u80fd\u5f88\u597d\u5730\u89e3\u51b3\u6b63\u5219\u5f62\u5f0f\u7684\u535a\u5f08\uff08normal-form game\uff09\uff0c\u4f46\u662f\u5bf9\u6269\u5c55\u5f62\u5f0f\u7684\u535a\u5f08\uff08extensive-form game\uff09\u4e0d\u9002\u7528\u3002 \u6240\u8c13\u6b63\u5219\u5f62\u5f0f\uff0c\u662f\u4e00\u79cd\u63cf\u8ff0\u535a\u5f08\u7684\u65b9\u5f0f\u3002\u6b63\u5219\u5f62\u5f0f\u7528 n \u7ef4\u77e9\u9635\u6765\u63cf\u8ff0\u535a\u5f08\uff0c\u800c\u6269\u5c55\u5f62\u5f0f\u4f7f\u7528\u56fe\u3002\u6b63\u5219\u5f62\u5f0f\u53ea\u80fd\u63cf\u8ff0\u73a9\u5bb6\u540c\u65f6\u884c\u52a8\u7684\u535a\u5f08\u3002","title":"1 \u57fa\u672c\u6982\u5ff5"},{"location":"mathematics/CFR/RegretMatchingAndBlottoGame/#11","text":"\u6b63\u5219\u5f62\u5f0f\u7684\u77e9\u9635\u63cf\u8ff0\u6709\u4ee5\u4e0b\u51e0\u4e2a\u8981\u7d20\uff1a 1. \u73a9\u5bb6\u6570\u91cf n \u5373\u7ef4\u5ea6 2. \u7ef4\u5ea6 i \u4e0a\u7684\u503c\u662f\u73a9\u5bb6 i \u7684\u884c\u52a8 3. \u77e9\u9635\u7684\u5143\u7d20\u662f\u5956\u52b1( payoff ) \u4f8b\u5982\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u5982\u4e0b\u77e9\u9635\u6765\u63cf\u8ff0\u526a\u5200\u77f3\u5934\u5e03\u6e38\u620f\uff1a \u526a\u5200 \u77f3\u5934 \u5e03 \u526a\u5200 (0, 0) (-1, 1) (1, -1) \u77f3\u5934 (1, -1) (0, 0) (-1, 1) \u5e03 (-1, 1) (1, -1) (0, 0) \u5982\u679c\u77e9\u9635\u4e2d\u6240\u6709 payoff \u7684\u503c\u7684\u548c\u4e3a 0\uff0c\u5219\u79f0\u4e3a\u96f6\u548c\u535a\u5f08\u3002","title":"1.1 \u535a\u5f08\u7684\u6b63\u5219\u5f62\u5f0f\u63cf\u8ff0"},{"location":"mathematics/CFR/RegretMatchingAndBlottoGame/#12","text":"\u5982\u679c\u67d0\u4e2a\u73a9\u5bb6\u4ee5 100% \u7684\u6982\u7387\u91c7\u53d6\u4e00\u4e2a\u884c\u52a8 \uff08\u4f8b\u5982\u5fb7\u6251\u5168\u7a0b all in\uff09\uff0c\u79f0\u4e3a pure strategy\u3002\u5982\u679c\u4e00\u4e2a\u73a9\u5bb6\u53ef\u80fd\u91c7\u53d6\u591a\u79cd\u884c\u52a8\uff0c\u5c31\u79f0\u4e3a mixed strategy\u3002 \u6211\u4eec\u4f7f\u7528 \\(\\sigma\\) \u8868\u793a mixed strategy\uff0c \\(\\sigma_i(s)\\) \u8868\u793a\u73a9\u5bb6 \\(i\\) \u9009\u62e9\u884c\u52a8 \\(s\\) \u7684\u6982\u7387\uff0c \\(-i\\) \u8868\u793a \\(i\\) \u7684\u5bf9\u624b\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u6cd5\u8ba1\u7b97\u73a9\u5bb6\u7684\u671f\u671b payoff\uff1a \\[u_i(\\sigma_i, \\sigma_{-i}) = \\sum_{s \\in S_i}\\sum_{s' \\in S_{-i}} \\sigma_i(s) \\sigma_{-i}(s')u_i(s, s')\\] \u5176\u5b9e\u5c31\u662f\u52a0\u6743\u6c42\u548c\u3002","title":"1.2 \u73a9\u5bb6\u7b56\u7565"},{"location":"mathematics/CFR/RegretMatchingAndBlottoGame/#2-regret-matching-and-minimization","text":"Regret matching \u7b97\u6cd5\u53ea\u80fd\u7528\u4e8e\u6b63\u5219\u5f62\u5f0f\u7684\u535a\u5f08\u3002\u5176\u57fa\u672c\u601d\u60f3\u4e3a\u6839\u636e payoff \u5bf9\u4e4b\u524d\u7684\u884c\u52a8\u4f5c\u6c42\u53cd\u6094\u503c\u3002\u518d\u5229\u7528\u7d2f\u8ba1\u7684\u53cd\u6094\u503c\u6307\u5bfc\u4e0b\u4e00\u6b65\u884c\u52a8\u3002 \u4ee5\u521a\u624d\u7684\u77f3\u5934\u526a\u5200\u5e03\u6e38\u620f\u4e3a\u4f8b\uff0c\u6211\u4eec\u51fa\u4e86\u77f3\u5934\uff0c\u5bf9\u624b\u51fa\u4e86\u5e03\uff0c\u6211\u4eec\u8f93\u6389\u4e86 1 \u5757\u94b1\uff0cpayoff \u662f -1\u3002\u6211\u4eec\u5982\u679c\u51fa\u5e03\uff0c\u662f\u5e73\u5c40\uff0cpayoff \u662f 0\uff0c\u5982\u679c\u51fa\u526a\u5200\uff0c\u5c31\u8d62\u4e86\uff0cpayoff \u662f 1\u3002payoff \u5dee\u503c\u5373\u53cd\u6094\u503c\u5206\u522b\u662f (0\uff0c1\uff0c2)\u3002\u5c06\u5176 normalize\uff0c\u4e0b\u4e00\u6b21\u51fa\u77f3\u5934\u3001\u5e03\u3001\u526a\u5200\u7684\u6982\u7387\u5206\u522b\u662f (0\uff0c1/3\uff0c2/3)\u3002 \u5047\u8bbe\u7b2c\u4e8c\u6b21\u6211\u4eec\u51fa\u4e86\u526a\u5200\uff0c\u5bf9\u624b\u51fa\u4e86\u77f3\u5934\u3002\u6211\u4eec\u518d\u6b21\u8f93\u6389\u4e86 1 \u5757\u94b1\u3002\u8fd9\u6b21\u6211\u4eec\u5bf9\u77f3\u5934\u3001\u5e03\u3001\u526a\u5200\u7684\u53cd\u6094\u503c\u5206\u522b\u662f(1\uff0c2\uff0c0)\u3002\u7d2f\u52a0\u5230\u4e4b\u524d\u7684 (0\uff0c1\uff0c2) \u4e0a\u4e3a (1\uff0c3\uff0c2)\u3002\u4e0b\u4e00\u6b21\u51fa\u77f3\u5934\u3001\u5e03\u3001\u526a\u5200\u7684\u6982\u7387\u4e3a (1/6, 3/6, 2/6)\u3002","title":"2 Regret Matching and Minimization"},{"location":"mathematics/CFR/RegretMatchingAndBlottoGame/#exercise-colonel-blotto","text":"Colonel Blotto and his arch-enemy, Boba Fett, are at war. Each commander has S soldiers in total, and each soldier can be assigned to one of N < S battlefields. Naturally, these commanders do not communicate and hence direct their soldiers independently. Any number of soldiers can be allocated to each battlefield, including zero. A commander claims a battlefield if they send more soldiers to the battlefield than their opponent. The commander\u2019s job is to break down his pool of soldiers into groups to which he assigned to each battlefield. The winning commander is the one who claims the most battlefields. For example, with (S,N) = (10,4) a Colonel Blotto may choose to play (2,2,2,4) while Boba Fett may choose to play (8,1,1,0). In this case, Colonel Blotto would win by claiming three of the four battlefields. The war ends in a draw if both commanders claim the same number of battlefields. \u8ba9\u4e24\u4e2a\u4f7f\u7528 regret matching \u7684\u73a9\u5bb6\u8fdb\u884c\u5bf9\u6218\uff0c\u9009\u5b9a S=5 \u548c N=3\uff0c\u627e\u5230 Nash Equilibrium\u3002 \u8981\u70b9\uff1a \u5217\u51fa\u6240\u6709\u5206\u914d\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u53ef\u80fd\u7684 action set\uff0c\u4ece 0 \u5f00\u59cb\u4f9d\u6b21\u7f16\u53f7 \u7528\u4e00\u4e2a 2 \u7ef4\u77e9\u9635\u5b58\u50a8 action \u5bf9 action \u7684\u80dc\u8d1f\u8868\uff0c\u5373\u5f97\u5230 blotto \u535a\u5f08\u7684\u77e9\u9635\u63cf\u8ff0 \u5404\u4e2a\u73a9\u5bb6\u6839\u636e\u5728\u5177\u6709\u6b63\u53cd\u6094\u503c\u7684 action \u4e2d\uff0c\u6839\u636e\u53cd\u6094\u503c\u968f\u673a\u9009\u62e9\u4e00\u4e2a action\u3002\u82e5\u4e0d\u5b58\u5728\u6b63\u53cd\u6094\u503c\u7684 action\uff08\u4f8b\u5982\u7b2c\u4e00\u8f6e\uff09\uff0c\u5219\u968f\u673a\u9009\u62e9\u521d\u59cb action\u3002 \u5404\u4e2a\u73a9\u5bb6\u5728\u6e38\u620f\u7ed3\u675f\u65f6\u5f97\u5230\u5bf9\u624b\u7684 action\uff0c\u5e76\u66f4\u65b0\u81ea\u5df1\u7684\u53cd\u6094\u503c \u4ee3\u7801\u5b9e\u73b0\u89c1 https://github.com/double-free/cfr \uff0c\u8fd0\u884c 10 \u4e07\u6b21\u7ed3\u679c\u5982\u4e0b\uff08\u7ed3\u679c\u5177\u6709\u4e00\u5b9a\u968f\u673a\u6027\uff09\u3002 \u53ef\u80fd\u7684\u5206\u914d\u65b9\u5f0f\uff1a [0, 0, 5], [0, 1, 4], [0, 2, 3], [0, 3, 2], [0, 4, 1], [0, 5, 0], [1, 0, 4], [1, 1, 3], [1, 2, 2], [1, 3, 1], [1, 4, 0], [2, 0, 3], [2, 1, 2], [2, 2, 1], [2, 3, 0], [3, 0, 2], [3, 1, 1], [3, 2, 0], [4, 0, 1], [4, 1, 0], [5, 0, 0], \u6bcf\u4e2a\u5206\u914d\u65b9\u5f0f\u5bf9\u5e94\u7684\u53cd\u6094\u503c\uff1a player 1: for opponent 0, regret sum for each action [-55002, -10612, -45, -41, -11250, -55644, -10426, 25, -11075, 127, -11068, 109, -11107, -11009, 207, -74, -92, 20, -11636, -11640, -56029] \u6700\u7ec8\u8fd8\u5177\u6709\u6b63\u53cd\u6094\u503c\u7684 action\uff1a player 1: for opponent 0, candidate strategy Allocation { soldiers: [1, 1, 3] } has positive regret 25 player 1: for opponent 0, candidate strategy Allocation { soldiers: [1, 3, 1] } has positive regret 127 player 1: for opponent 0, candidate strategy Allocation { soldiers: [2, 0, 3] } has positive regret 109 player 1: for opponent 0, candidate strategy Allocation { soldiers: [2, 3, 0] } has positive regret 207 player 1: for opponent 0, candidate strategy Allocation { soldiers: [3, 2, 0] } has positive regret 20 \u7ed3\u679c\u8fd8\u662f\u76f8\u5bf9\u7b26\u5408\u76f4\u89c9\u7684\u3002","title":"Exercise: Colonel Blotto"},{"location":"mathematics/CFR/RegretMatchingAndBlottoGame/#reference","text":"\u53cd\u4e8b\u5b9e\u540e\u6094\u6700\u5c0f\u5316 An Introduction to Counterfactual Regret Minimization Game Basics Monte Carlo Tree Search Counterfactual Regret Minimization","title":"Reference"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/","text":"ESL 3: Linear Methods for Regression \u4e00\u4e2a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u5047\u8bbe\u56de\u5f52\u51fd\u6570 E(Y|X) \u5bf9\u4e8e\u8f93\u5165 X \u662f\u7ebf\u6027\u7684\u3002 \u5b83\u7684\u4f18\u52bf\u5728\u4e8e\uff1a \u7b80\u5355 \u80fd\u591f\u8868\u793a\u6bcf\u4e2a\u8f93\u5165\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd \u8f93\u5165\u53ef\u4ee5\u8fdb\u884c\u53d8\u6362 \u4ed6\u4eec\u6709\u65f6\u5019\u6bd4\u590d\u6742\u7684\u65b9\u6cd5\u66f4\u7cbe\u51c6\uff0c\u5c24\u5176\u662f\u5728\u6837\u672c\u6570\u91cf\u5c11\u3001\u4f4e\u4fe1\u566a\u6bd4\u6216\u8005\u7a00\u758f\u77e9\u9635\u7684\u60c5\u5f62\u3002 3.2 Linear Regression Models and Least Squares \\(p\\) \u7ef4\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u5f62\u5f0f\u5982\u4e0b\uff1a \\[f(X) = \\beta_0 + \\sum_{j=1}^p X_j \\beta_j\\] \u6211\u4eec\u9700\u8981\u4f30\u8ba1\u4e00\u7ec4\u53c2\u6570 \\(\\beta\\) \uff0c\u4f7f\u6b8b\u5dee\u5e73\u65b9\u548c\uff08Residual Sum of Squares\uff09\u6700\u5c0f\uff1a \\[\\begin{align} \\text{RSS}(\\beta) &= (\\textbf{y} - \\textbf{X}\\beta )^T(\\textbf{y} - \\textbf{X}\\beta ) \\\\ &= \\textbf{y}^T\\textbf{y} - \\textbf{y}^T\\textbf{X}\\beta - \\beta^T\\textbf{X}^T\\textbf{y} + \\beta^T\\textbf{X}^T\\textbf{X}\\beta \\end{align}\\] \u5176\u4e2d\uff0c \\(\\textbf{X}\\) \u662f\u4e00\u4e2a \\(N \\times (p+1)\\) \u77e9\u9635\uff0c \\(\\textbf{y}\\) \u662f \\(N \\times 1\\) \u89c2\u6d4b\u503c\u3002 \u5bf9 \\(\\beta\\) \u6c42\u5bfc\u53ef\u4ee5\u5f97\u5230\uff1a \\[ \\frac{\\partial \\text{RSS}(\\beta)}{\\partial \\beta} = -2 \\textbf{X}^T\\textbf{y} + 2\\textbf{X}^T\\textbf{X} \\beta\\] \u7531\u4e8e\u4e8c\u9636\u5bfc\u6570\u6b63\u5b9a\uff0c\u4ee4\u4e00\u9636\u5bfc\u6570\u4e3a 0 \u5411\u91cf\uff0c\u5f97\u51fa\u6781\u503c\u70b9\uff08\u5373\u4f30\u8ba1\u503c\uff09\uff1a \\[ \\hat{\\beta}= (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}\\] \\[\\hat{\\textbf{y}} = \\textbf{X} \\hat{\\beta} = \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}\\] \u6211\u4eec\u79f0 \\(\\textbf{H} = \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\) \u4e3a\u4f30\u8ba1\u77e9\u9635\uff08\"hat\" matrix\uff09\uff0c\u5b83\u6ee1\u8db3\u5bf9\u79f0\u6027\u548c\u5e42\u7b49\u6027\uff1a \\[\\textbf{H}^T = \\textbf{H}\\] \\[\\textbf{H}^T\\textbf{H} = \\textbf{H}\\] \u5f53 \\(\\textbf{X}\\) \u4e2d\u67d0\u4e9b\u5217\u7ebf\u6027\u76f8\u5173\uff08\u5373\u975e\u6ee1\u79e9\u77e9\u9635\uff09\u65f6\uff0c \\((\\textbf{X}^T\\textbf{X})\\) \u662f\u5947\u5f02\u77e9\u9635\uff0c\u5b83\u53ea\u80fd\u6c42\u5e7f\u4e49\u9006\u77e9\u9635\uff0c\u4e0d\u6b62\u4e00\u4e2a\u89e3\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5c06\u5197\u4f59\u7684\u8f93\u5165\u5254\u9664\u6389\uff0c\u5927\u90e8\u5206\u6c42\u89e3\u8f6f\u4ef6\u90fd\u5b9e\u73b0\u4e86\u8fd9\u4e2a\u529f\u80fd\u3002 \u4f30\u8ba1\u53c2\u6570\u7684\u7edf\u8ba1\u7279\u6027 \u4e3a\u4e86\u786e\u5b9a\u4f30\u8ba1\u7684\u53c2\u6570 \\(\\hat{\\beta}\\) \u7684\u7edf\u8ba1\u7279\u6027\uff0c\u6211\u4eec\u5047\u8bbe\uff1a \u6bcf\u4e2a\u89c2\u6d4b\u503c \\(y_i\\) \u76f8\u4e92\u72ec\u7acb \\(y_i\\) \u6709\u56fa\u5b9a\u7684\u566a\u58f0 \\(\\varepsilon \\sim N(0, \\sigma^2)\\) \u90a3\u4e48\u4f30\u8ba1\u503c \\(\\hat{\\beta}\\) \u7684\u65b9\u5dee\u4e3a\uff1a \\[ \\text{Var}(\\hat{\\beta}) = (\\textbf{X}^T\\textbf{X})^{-1} \\sigma^2\\] where: \\[\\hat{\\sigma}^2 = \\frac{\\text{RSS}}{N-p-1}= \\frac{1}{N-p-1} \\sum_{i=1}^{N} (y_i-\\hat{y})^2\\] \u8bc1\u660e N \u4e2a y \u7684\u89c2\u6d4b\u503c\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a \\[ \\textbf{y} = \\textbf{X}\\beta + \\varepsilon \\] \u5176\u4e2d \\(\\varepsilon\\) \u662f \\(N \\times 1\\) \u7684\u566a\u58f0\u3002\u56e0\u6b64\u6709\uff1a \\[\\begin{align} \\hat{\\beta} &= (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y} \\\\ &= \\beta + (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\varepsilon \\end{align}\\] \u65e0\u504f\u6027\uff08\u671f\u671b\u503c\u4e3a \\(\\beta\\) \uff09\uff1a \\[E(\\hat{\\beta}) = \\beta + (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T E(\\varepsilon) = \\beta\\] \u534f\u65b9\u5dee\u77e9\u9635\uff08\u6ce8\u610f\u662f \\(\\beta \\beta^T\\) \u800c\u975e \\(\\beta^T \\beta\\) \uff0c\u662f\u4e00\u4e2a\u77e9\u9635\uff09\uff1a \\[\\begin{align} \\text{Var}(\\hat{\\beta}) &= E[(\\beta - \\hat{\\beta})(\\beta - \\hat{\\beta})^T] \\\\ &=E[(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\varepsilon\\varepsilon^T\\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}] \\\\ &= (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T E(\\varepsilon\\varepsilon^T) \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1} \\\\ &= \\sigma^2 (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T \\textbf{I} \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1} \\\\ &= \\sigma^2 (\\textbf{X}^T\\textbf{X})^{-1} \\end{align}\\] \u53ef\u4ee5\u5f97\u5230\uff1a \\[ \\hat{\\beta} \\sim N(\\beta, \\sigma^2 (\\textbf{X}^T\\textbf{X})^{-1})\\] \u4e0b\u9762\u6765\u786e\u5b9a \\(\\sigma^2\\) \u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u89c2\u6d4b\u503c \\(y\\) \u548c\u9884\u6d4b\u503c \\(\\hat{y}\\) \u7684\u5dee\u6765\u5f97\u5230\u566a\u58f0 \\(\\varepsilon\\) \u3002 \\[\\begin{align} y - \\hat{y} &= \\textbf{X}\\beta + \\varepsilon -\\textbf{X}\\hat{\\beta} \\\\ &= \\textbf{X}\\beta + \\varepsilon - \\textbf{X}(\\beta + (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\varepsilon) \\\\ &= (\\textbf{I -H} )\\varepsilon \\end{align}\\] \\[\\begin{align} \\sum_{i=1}^N(y_i - \\hat{y_i})^2 &= (y - \\hat{y})^T (y - \\hat{y}) \\\\ &= \\varepsilon^T(\\textbf{I - H}) \\varepsilon \\\\ &= \\sum_{k =1}^N \\varepsilon_k^2- \\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij} \\end{align}\\] \u5176\u671f\u671b\u503c\u4e3a\uff1a \\[\\begin{align} E[\\sum_{i=1}^N(y_i - \\hat{y_i})^2] &= E[\\sum_{k =1}^N \\varepsilon_k^2- \\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij} ] \\\\ &= N\\sigma^2 - E(\\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij}) \\end{align}\\] \u7531\u4e8e \\(\\varepsilon_i, \\varepsilon_j\\) \u662f\u72ec\u7acb\u7684\uff0c\u5f53 \\(i \\neq j\\) \u65f6\uff1a \\[\\text{Cov}(\\varepsilon_i, \\varepsilon_j) = E(\\varepsilon_i \\varepsilon_j) - E(\\varepsilon_i)E(\\varepsilon_j) = 0\\] \u56e0\u6b64\uff1a \\[\\begin{align} E[\\sum_{i=1}^N(y_i - \\hat{y_i})^2] &= N\\sigma^2 - E(\\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij}) \\\\ &= N\\sigma^2 - E(\\sum_{i=1}^{N}\\varepsilon_i^2H_{ii}) \\\\ &= \\sigma^2[N - \\text{trace}(\\textbf{H})] \\end{align}\\] \u8fd9\u91cc\u518d\u5229\u7528\u516c\u5f0f\uff1a \\[\\text{trace}(ABC) = \\text{trace}(CAB)\\] \u5f97\u5230\uff1a \\[\\begin{align} E[\\sum_{i=1}^N(y_i - \\hat{y_i})^2] &= \\sigma^2[N - \\text{trace}(\\textbf{H})] \\\\ &= \\sigma^2[N - \\text{trace}(\\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T)] \\\\ &= \\sigma^2[N - \\text{trace}(\\textbf{X}^T \\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1}_{(p+1) \\times (p+1)})] \\\\ &= \\sigma^2[N - \\text{trace}(\\textbf{I}_{(p+1) \\times (p+1)})] \\\\ &= \\sigma^2(N - p -1) \\end{align}\\] \u56e0\u6b64\uff0c\u5bf9 \\(\\sigma^2\\) \u7684\u65e0\u504f\u4f30\u8ba1\u5c31\u662f\uff1a \\[\\hat{\\sigma}^2 = \\frac{1}{N-p-1} \\sum_{i=1}^{N} (y_i-\\hat{y})^2\\] \u6a21\u578b\u8bef\u5dee\u7684\u7edf\u8ba1\u7279\u6027 \u7531\u4e8e\u6211\u4eec\u5bf9\u7b2c i \u4e2a\u6837\u672c\u7684\u566a\u58f0 \\(\\varepsilon_i\\) \u65e0\u504f\u4f30\u8ba1\u5c31\u662f \\(\\hat{\\varepsilon_i} = y_i - \\hat{y_i}\\) \uff0c\u6211\u4eec\u8ba1\u7b97\u5176\u65b9\u5dee\uff1a \\[\\begin{align} \\text{Var}(\\hat{\\varepsilon}) &= \\text{Var}(\\textbf{y} - \\hat{\\textbf{y}}) \\\\ &= \\text{Var}[(\\textbf{I} - \\textbf{H}){\\varepsilon}] \\end{align}\\] \u7531\u4e8e \\(D(AX) = AD(X)A^T\\) \uff1a \\[\\begin{align} \\text{Var}(\\hat{\\varepsilon}) &= \\text{Var}[(\\textbf{I} - \\textbf{H}){\\varepsilon}] \\\\ &= (\\textbf{I} - \\textbf{H}) \\text{Var}(\\varepsilon) (\\textbf{I} - \\textbf{H}) \\end{align}\\] \u7531\u4e8e \\(\\varepsilon \\sim N(0, \\sigma^2)\\) \uff0c\u56e0\u6b64\uff1a \\[\\text{Var}(\\varepsilon) = \\sigma^2 \\textbf{I}_{N \\times N}\\] \u800c \\(\\textbf{H} = \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\) \u6ee1\u8db3\u5bf9\u79f0\u6027\u548c\u5e42\u7b49\u6027\uff1a \\[\\textbf{H}^T = \\textbf{H}\\] \\[\\textbf{H}^T\\textbf{H} = \\textbf{H}\\] \u56e0\u6b64\u6709\u7ed3\u8bba\uff1a \\[\\text{Var}(\\hat{\\varepsilon}) = \\sigma^2 (\\textbf{I} - \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T)\\] \u663e\u8457\u6027\u5206\u6790 \u5f53\u6211\u4eec\u5224\u65ad\u54ea\u4e9b\u53c2\u6570\u53ef\u4ee5\u5ffd\u7565\u4ee5\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 F-statistic \u8fdb\u884c\u663e\u8457\u6027\u5206\u6790\u3002\u5047\u8bbe\u6211\u4eec\u5c06 \\(\\beta\\) \u7ef4\u5ea6\u4ece \\(p_1 + 1\\) \u964d\u4f4e\u5230 \\(p_0 + 1\\) \uff1a \\[ F = \\frac{(\\text{RSS}_0 - \\text{RSS}_1) / (p_1 - p_0)}{\\text{RSS}_1 / (N- p_1 -1)} \\] F-statistic \u63cf\u8ff0\u4e86\u6bcf\u4e2a\u88ab\u5ffd\u7565\u7684\u53c2\u6570\u5bf9 RSS \u7684\u5e73\u5747\u8d21\u732e\uff0c\u7528 \\(\\hat{\\sigma}^2\\) \u8fdb\u884c\u4e86 normalize\u3002 \u5f53 \\(p_1 - p_0 =1\\) \u5373\u4ec5\u53bb\u6389\u4e00\u4e2a\u53c2\u6570\u65f6\uff08\u5047\u8bbe \\(\\beta_j = 0\\) \uff09\uff0c\u8be5\u516c\u5f0f\u53ef\u4ee5\u7b80\u5316\u4e3a\u5bf9\u5e94\u7684 z-score \u7684\u5e73\u65b9\uff0c\u5176\u4e2d z-score \u4e3a\uff1a \\[ z_j = \\frac{\\hat{\\beta}_j}{\\hat{\\sigma} \\sqrt{v_j} }\\] where: \\[\\hat{\\sigma}^2 =\\frac{\\text{RSS}_1}{N-p-1} =\\frac{1}{N-p-1} \\sum_{i=1}^{N} (y_i-\\hat{y})^2\\] \\[v_j = (\\textbf{X}^T\\textbf{X})^{-1}_{jj}\\] \u8bc1\u660e \u8fd9\u4e2a\u8bc1\u660e\u540c\u65f6\u4e5f\u662f\u4e60\u9898 3.1 Ex. 3.1 Show that the F statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding z-score (3.12). \u5b9e\u9645\u4e0a\u6211\u4eec\u9700\u8981\u8bc1\u660e\uff0c\u5728\u53bb\u6389\u6a21\u578b\u7684\u7b2c j \u4e2a\u53c2\u6570\u540e\uff1a \\[ \\text{RSS}_0 - \\text{RSS}_1 = \\frac{\\hat{\\beta}_j^2}{v_j} \\] \u4e0a\u5f0f\u4e2d\u552f\u4e00\u672a\u77e5\u7684\u5c31\u662f \\(\\text{RSS}_0\\) \uff0c\u5b83\u5b9e\u8d28\u4e0a\u662f\u6c42\u4e00\u4e2a\u5e26\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff1a \\[\\begin{align} \\min_{\\beta \\in \\mathbb{R}^{(p+1) \\times 1}} (\\textbf{y} - \\textbf{X}\\beta)^T(\\textbf{y}-\\textbf{X}\\beta) \\\\ \\text{s.t.} ~\\beta_j = 0 \\end{align}\\] \u6211\u4eec\u53ef\u4ee5\u7528\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u6cd5\u6765\u89e3\u51b3\u3002 \\[L(\\beta, \\lambda) = (\\textbf{y} - \\textbf{X}\\beta)^T(\\textbf{y}-\\textbf{X}\\beta) + \\lambda e_j^T \\beta \\] \u5bf9 \\(\\beta\\) \u6c42\u5bfc\uff0c\u5e76\u4ee4\u5bfc\u6570\u4e3a 0\uff0c\u6709\uff1a \\[\\frac{\\partial L(\\beta, \\lambda)}{\\partial \\beta} = - 2\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\beta) + \\lambda e_j = 0\\] \u89e3\u51fa\uff1a \\[\\begin{align} \\beta_0 &= (\\textbf{X}^T\\textbf{X})^{-1} \\textbf{X}^T\\textbf{y} - \\frac{\\lambda}{2}(\\textbf{X}^T \\textbf{X})^{-1} e_j \\\\ &= \\hat{\\beta}- \\frac{\\lambda}{2}(\\textbf{X}^T \\textbf{X})^{-1} e_j \\end{align}\\] \u7b49\u5f0f\u4e24\u8fb9\u4e58\u4ee5 \\(e_j^T\\) \uff0c\u5e76\u5e26\u5165 \\(\\beta_j = 0\\) \uff0c\u6709\uff1a \\[\\begin{align} e_j^T\\beta_0 = 0 &= e_j^T \\hat{\\beta} + \\frac{\\lambda}{2} e_j^T(\\textbf{X}^T \\textbf{X})^{-1} e_j \\\\ &= \\hat{\\beta}_j + \\frac{\\lambda}{2}v_j \\end{align}\\] \u56e0\u6b64\u6709\uff1a \\[\\lambda = - \\frac{2\\hat{\\beta}_j}{v_j}\\] \u5e26\u5165\u53ef\u5f97\uff1a \\[\\begin{align} \\text{RSS}_0 &= (\\textbf{y} - \\textbf{X}\\beta_0)^T(\\textbf{y}-\\textbf{X}\\beta_0) \\\\ &= (\\textbf{y} - \\textbf{X}\\hat{\\beta} + \\frac{\\lambda}{2}\\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1} e_j)^T(\\textbf{y}-\\textbf{X}\\hat{\\beta} + \\frac{\\lambda}{2}\\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1} e_j) \\\\ &= \\text{RSS}_1 + \\frac{\\lambda}{2} [e_j^T(\\textbf{X}^T \\textbf{X})^{-1}\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\hat{\\beta}) + (\\textbf{y} - \\textbf{X}\\hat{\\beta})^T \\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1} e_j)] \\\\ &~~~~ + \\frac{\\lambda^2}{4}e_j^T (\\textbf{X}^T \\textbf{X})^{-1} e_j \\\\ &= \\text{RSS}_1 + \\frac{\\lambda^2}{4}e_j^T (\\textbf{X}^T \\textbf{X})^{-1} e_j \\\\ &= \\text{RSS}_1 + \\frac{\\hat{\\beta}_j^2}{v_j} \\end{align}\\] \u5176\u4e2d\uff0c\u4e2d\u95f4\u9879\u53ef\u4ee5\u6d88\u53bb\u7684\u539f\u56e0\u662f\uff1a \\[\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\hat{\\beta}) = \\textbf{X}^T[\\textbf{y} - \\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}] = 0 \\] \u76f4\u89c2\u7406\u89e3\uff0c \\(\\textbf{X}\\) \u548c \\(\\textbf{y} - \\textbf{X}\\hat{\\beta}\\) \u662f\u6b63\u4ea4\u7684\uff0c\u56e0\u4e3a \\(\\textbf{X}\\hat{\\beta}\\) \u6b63\u662f \\(\\textbf{y}\\) \u5728 \\(\\textbf{X}\\) \u6240\u5728\u5e73\u9762\u4e0a\u7684\u6295\u5f71\u3002 3.2.2 The Gauss\u2013Markov Theorem \u6700\u5c0f\u4e8c\u4e58\u6cd5\u5f97\u51fa\u7684 \\(\\beta\\) \u5728\u6240\u6709\u7ebf\u6027 \u65e0\u504f \u4f30\u8ba1\u4e2d\u5747\u65b9\u8bef\u5dee\u6700\u5c0f\u3002\u5f53\u7136\uff0c\u5982\u679c\u6211\u4eec\u613f\u610f\u4e3a\u4e86\u8fdb\u4e00\u6b65\u51cf\u5c0f\u8bef\u5dee\u5f15\u5165\u4e00\u70b9 bias\uff0c\u5b8c\u5168\u53ef\u80fd\u627e\u5230\u4e00\u4e2a\u66f4\u5c0f\u5747\u65b9\u8bef\u5dee\u7684 \u6709\u504f \u4f30\u8ba1\u3002 the least squares estimates of the parameters \u03b2 have the smallest variance among all linear unbiased estimates \u73b0\u5728\u6211\u4eec\u6765\u8bc1\u660e\u8fd9\u4e2a\u7ed3\u8bba\u3002\u5bf9\u4e8e\u7ebf\u6027\u4f30\u8ba1\uff1a \\[\\textbf{y} = \\textbf{X}\\beta\\] \\(\\textbf{y}\\) \u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u90fd\u53ef\u4ee5\u770b\u4f5c \\(\\textbf{X}\\) \u4e2d\u7684\u4e00\u884c\u4e0e\u5411\u91cf \\(\\beta\\) \u7684\u7ebf\u6027\u7ec4\u5408\u3002 \u65e0\u504f\u6027 \u90a3\u4e48\uff0c\u9488\u5bf9\u65e0\u504f\u6027\uff0c\u6211\u4eec\u9700\u8981\u8bc1\u660e\u6700\u5c0f\u4e8c\u4e58\u6cd5\u4f30\u8ba1\u51fa\u7684 \\(\\hat{\\beta}\\) \u6ee1\u8db3\uff1a \\[ E(\\alpha^T \\hat{\\beta}) = \\alpha^T\\beta\\] \u5176\u4e2d \\(\\alpha\\) \u662f\u4efb\u610f\u5411\u91cf\u3002 \\[\\begin{align} E(\\alpha^T \\hat{\\beta}) &= E(\\alpha^T (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}) \\\\ &= E(\\alpha^T (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{X} \\beta) \\\\ &= \\alpha^T \\beta \\end{align} \\] \u5747\u65b9\u8bef\u5dee\u6700\u5c0f Gauss\u2013Markov theorem \u6307\u51fa\uff0c\u5982\u679c\u8fd8\u5b58\u5728\u5176\u4ed6\u7ebf\u6027\u4f30\u8ba1 \\(c^T \\textbf{y}\\) \u6ee1\u8db3\uff1a \\[ E(c^T \\textbf{y}) = \\alpha^T\\beta\\] \u90a3\u4e48\u5fc5\u7136\u6709\uff1a \\[\\text{Var}(\\alpha^T \\hat{\\beta}) \\leq \\text{Var}(c^T \\textbf{y})\\] \u8bc1\u660e\uff1a TBD 3.3 Subset Selection \u6700\u5c0f\u4e8c\u4e58\u6cd5\u7684\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a \u9884\u6d4b\u7cbe\u5ea6\u3002\u867d\u7136\u5b83\u662f\u65e0\u504f\u7684\uff0c\u4f46\u662f\u65b9\u5dee\u5f88\u5927\u3002\u5982\u679c\u6211\u4eec\u5ffd\u7565\u4e00\u90e8\u5206\u6a21\u578b\u53c2\u6570\uff0c\u867d\u7136\u4f1a\u53d8\u6210\u6709\u504f\u4f30\u8ba1\uff0c\u4f46\u662f\u53ef\u80fd\u4f1a\u6781\u5927\u63d0\u9ad8\u7cbe\u5ea6\u3002 \u53ef\u89e3\u91ca\u6027\uff08\u5373\u6a21\u578b\u590d\u6742\u5ea6\uff09\u3002\u5f53\u6a21\u578b\u53c2\u6570\u5f88\u591a\u65f6\uff0c\u6211\u4eec\u60f3\u53bb\u786e\u5b9a\u4e00\u5c0f\u90e8\u5206\u5177\u6709\u6700\u5927\u5f71\u54cd\u7684\u6a21\u578b\u53c2\u6570\uff0c\u4e3a\u6b64\u6211\u4eec\u613f\u610f\u727a\u7272\u4e00\u90e8\u5206\u65e0\u5173\u7d27\u8981\u7684\u53c2\u6570\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u9009\u53d6\u53d8\u91cf\u5b50\u96c6\uff0c\u5373\u201cmodel selection\u201d\u3002 3.3.1 Best-Subset Selection \u6700\u4f73\u5b50\u96c6\u662f\u6307\u4ece\u6240\u6709\u5177\u6709 \\(k (k <= p)\\) \u4e2a\u53d8\u91cf\u7684\u5b50\u96c6\u4e2d\uff0cRSS \u6700\u5c0f\u7684\u90a3\u4e2a\u3002 \u5f53\u7136\uff0c\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u5c31\u662f\u4ece\u904d\u5386\u6240\u6709\u7684\u7ec4\u5408\u3002\u8fd9\u6837\u505a\u7684\u590d\u6742\u5ea6\u662f \\(2^p\\) \uff0c\u53ea\u9002\u7528\u4e8e\u5c0f\u89c4\u6a21\u7684\u95ee\u9898\u3002 3.3.2 Forward- and Backward-Stepwise Selection \u201c\u524d\u5411\u9010\u6b65\u9009\u62e9\u201d\u662f\u4e00\u79cd\u8d2a\u5fc3\u7b97\u6cd5\u3002\u5b83\u6309\u987a\u5e8f\u52a0\u5165\u6700\u80fd\u63d0\u9ad8\u62df\u5408\u5ea6\u7684\u53c2\u6570\u3002\u5b83\u867d\u7136\u4e0d\u4e00\u5b9a\u627e\u5230\u6700\u4f18\u89e3\uff0c\u4f46\u662f\u5b83\u4f18\u52bf\u5728\u4e8e\uff1a \u8fd0\u7b97\u91cf\u5c0f\u3002\u5f53\u7ef4\u5ea6 \\(p >= 40\\) \u65f6\uff0c\u51e0\u4e4e\u65e0\u6cd5\u7b97\u51fa\u6700\u4f18\u89e3\u3002\u4f46\u662f\u4f9d\u65e7\u53ef\u4ee5\u7528 forward stepwise selection \uff08\u5373\u4f7f\u7ef4\u5ea6 p \u5927\u4e8e\u6837\u672c\u6570 N\uff09\u3002 \u65b9\u5dee\u5c0f\u3002\u6700\u4f18\u5b50\u96c6\u65b9\u5dee\u6bd4 forward stepwise selection \u5927\uff0c\u867d\u7136\u540e\u8005\u53ef\u80fd\u4f1a\u6709\u4e00\u5b9a\u7684 bias\u3002 \u90a3\u4e48\u5982\u4f55\u9009\u62e9\u201c\u6700\u80fd\u63d0\u9ad8\u62df\u5408\u5ea6\u201c\u7684\u53c2\u6570\u5462\uff1f\u6211\u4eec\u5728\u4e4b\u524d\u201c\u663e\u8457\u6027\u5206\u6790\u201d\u4e2d\u5df2\u7ecf\u8bc1\u660e\u4e86\uff0c\u53bb\u6389\u4e00\u4e2a\u53c2\u6570\u5bf9\u6b8b\u5dee\u7684\u5f71\u54cd\u4e3a\u5176 z-score \u7684\u5e73\u65b9\u3002\u90a3\u4e48\uff0c\u6211\u4eec\u76f4\u63a5 \u4ece z-score \u6700\u5927\u7684\u53c2\u6570\u5f00\u59cb\u4f9d\u6b21\u52a0\u5165 \u5373\u53ef\u3002\u7b2c \\(j\\) \u4e2a\u53c2\u6570\u7684 z-score \u53ef\u4ee5\u7531\u4e8e\u4e0b\u5f0f\u8ba1\u7b97\uff1a \\[ z_j = \\frac{\\hat{\\beta}_j}{\\hat{\\sigma} \\sqrt{v_j} }\\] where: \\[\\hat{\\sigma}^2 =\\frac{\\text{RSS}_1}{N-p-1} =\\frac{1}{N-p-1} \\sum_{i=1}^{N} (y_i-\\hat{y})^2\\] \\[v_j = (\\textbf{X}^T\\textbf{X})^{-1}_{jj}\\] \u201c\u540e\u5411\u9010\u6b65\u9009\u62e9\u201d \u4e0e \u201c\u524d\u5411\u9010\u6b65\u9009\u62e9\u201c\u76f8\u53cd\u3002\u5b83\u4ece\u5168\u96c6\u5f00\u59cb\uff0c\u4f9d\u6b21\u53bb\u6389\u6700\u65e0\u5173\u7d27\u8981\u7684\u53d8\u91cf\uff08z-score \u6700\u5c0f\u7684\uff09\u3002\u5b83\u53ea\u80fd\u7528\u4e8e\u6837\u672c\u6570 N \u5927\u4e8e\u7ef4\u5ea6 p \u7684\u60c5\u5f62\u3002 3.4 Shrinkage Methods Subset selection \u786e\u5b9e\u53ef\u4ee5\u5e2e\u6211\u4eec\u7b80\u5316\u6a21\u578b\uff0c\u5e76\u4e14\u8fd8\u53ef\u80fd\u964d\u4f4e\u8bef\u5dee\u3002\u4f46\u662f\uff0c\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u79bb\u6563\u7684\u8fc7\u7a0b\uff08\u53c2\u6570\u8981\u4e48\u88ab\u4e22\u5f03\u8981\u4e48\u88ab\u4fdd\u7559\uff0c\u6ca1\u6709\u4e2d\u95f4\u72b6\u6001\uff09\uff0c\u5b83\u901a\u5e38\u5177\u6709\u8f83\u5927\u7684\u65b9\u5dee\u3002Shrinkage methods \u66f4\u52a0\u8fde\u7eed\uff0c\u56e0\u6b64\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u3002 3.4.1 Ridge Regression Ridge Regression \u901a\u8fc7\u7ed9\u53c2\u6570\u6570\u91cf\u589e\u52a0\u4e00\u4e2a\u60e9\u7f5a\u9879\u6765\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u3002\u5b83\u7684\u4f18\u5316\u76ee\u6807\uff1a \\[\\hat{\\beta} = \\mathop{\\arg \\min}_{\\beta} \\sum_{i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\\] \u8fd9\u91cc\u7684 \\(\\lambda\\) \u63a7\u5236\u6a21\u578b\u201c\u7f29\u5c0f\u201d\u7684\u7a0b\u5ea6\uff0c \\(\\lambda\\) \u8d8a\u5927\uff0c\u5f97\u5230\u7684\u6a21\u578b\u590d\u6742\u5ea6\u8d8a\u4f4e\u3002 \u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c \u60e9\u7f5a\u9879\u4e2d\u4e0d\u5305\u542b\u5e38\u6570\u9879 \\(\\beta_0\\) \uff0c\u5426\u5219\u6a21\u578b\u4e0d\u7a33\u5b9a\u3002\u5f53\u9009\u53d6 \\(y_i = y_i + c\\) \u65f6\uff0c\u9884\u6d4b\u503c \\(\\hat{y}_i\\) \u7684\u53d8\u5316\u91cf\u4e0d\u662f \\(c\\) \u3002 \u4e0e\u7ecf\u5178\u7684 Linear Regression \u4e0d\u540c\uff0cRidge Regression \u8981\u6c42\u8f93\u5165 \\(\\textbf{X}, \\textbf{y}\\) \u662f\u7ecf\u8fc7\u4e86 \u4e2d\u5fc3\u5316 (centering) \u7684\u3002\u5e76\u4e14\uff0c\u8fd9\u91cc\u7684\u6a21\u578b\u53c2\u6570 \\(\\beta\\) \u662f \\(p\\) \u7ef4\u800c\u4e0d\u662f \\(p+1\\) \u7ef4\u7684\u3002 \u4e0b\u9762\u6211\u4eec\u6765\u8bc1\u660e\u8fd9\u4e00\u70b9\u3002 \\(\\beta_0\\) \u7531\u4e8e\u4e0d\u542b \\(\\lambda\\) \uff0c\u53ef\u4ee5\u5355\u72ec\u4f18\u5316\u3002\u6211\u4eec\u5148\u5bf9 \\(\\beta_0\\) \u6c42\u5bfc\uff0c\u5e76\u4ee4\u5bfc\u6570\u4e3a0: \\[\\sum_{i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j) = 0\\] \u5f97\u5230\uff1a \\[\\beta_0 = \\frac{1}{N}(\\sum_{i=1}^N y_i - \\sum_{i=1}^N \\sum_{j=1}^{p} x_{ij}\\beta_j) \\] \u4ee4 \\(\\overline{x_j} = \\frac{1}{N} \\sum_{i=1}^N x_{ij}\\) \uff0c\u6709\uff1a \\[\\beta_0 = \\frac{1}{N}\\sum_{i=1}^N y_i - \\sum_{j=1}^{p} \\overline{x_{j}} \\beta_j \\] \u6211\u4eec\u4ee5\u4e0b\u7684\u53d8\u5f62\u4e3b\u8981\u662f\u4e3a\u4e86\u5c06\u4f18\u5316\u76ee\u6807\u51fd\u6570\u5199\u6210\u77e9\u9635\u4e58\u6cd5\u5f62\u5f0f\uff0c\u4ee5\u8fdb\u884c\u8fd0\u7b97\u3002 \\[\\begin{align} \\hat{\\beta} &= \\mathop{\\arg \\min}_{\\beta} \\sum_{i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\\\ &= \\mathop{\\arg \\min}_{\\beta} \\sum_{i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p \\overline{x_j}\\beta_j - \\sum_{j=1}^p (x_{ij} - \\overline{x_j}) \\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\end{align}\\] \u73b0\u5728\u6211\u4eec\u4ee4\uff1a \\[\\begin{align} \\beta_0^c &= \\beta_0 + \\sum_{j=1}^p \\overline{x_j}\\beta_j =\\frac{1}{N} \\sum_{i=1}^N y_{i} \\\\ \\beta_j^c&= \\beta_j & (j>=1) \\end{align}\\] \u53ef\u4ee5\u5f97\u51fa\uff1a \\[\\begin{align} \\hat{\\beta} &= \\mathop{\\arg \\min}_{\\beta^c} \\sum_{i=1}^N(y_i - \\beta_0^c - \\sum_{j=1}^p (x_{ij} - \\overline{x_j}) \\beta_j^c)^2 + \\lambda \\sum_{j=1}^p {\\beta_j^c}^2 \\end{align}\\] \u6211\u4eec\u518d\u4ee4\uff1a \\[\\begin{align} y_i^c &= y_i - \\beta_0^c = y_i - \\frac{1}{N} \\sum_{i=1}^N y_i \\\\ x_{ij}^c&= x_{ij} - \\overline{x_j} & (j >=1) \\end{align}\\] \u6709\uff1a \\[\\begin{align} \\hat{\\beta} &= \\mathop{\\arg \\min}_{\\beta^c} \\sum_{i=1}^N(y_i^c - \\sum_{j=1}^p (x_{ij}^c \\beta_j^c)^2) + \\lambda \\sum_{j=1}^p {\\beta_j^c}^2 \\\\ &=\\mathop{\\arg \\min}_{\\beta} (\\textbf{y} - \\textbf{X}\\beta)^T(\\textbf{y} - \\textbf{X}\\beta) + \\lambda(\\beta^T\\beta) \\end{align}\\] \u5176\u4e2d\uff0c \\(\\textbf{X}, \\textbf{y}, \\beta\\) \u90fd\u7ecf\u8fc7\u4e86\u4e2d\u5fc3\u5316\uff0c\u5e76\u4e14\u662f \\(p\\) \u7ef4\u7684\u3002 \u8be5\u5f0f\u5bf9 \\(\\beta\\) \u6c42\u5bfc\u5e76\u4ee4\u5bfc\u6570\u4e3a 0\uff0c\u6709\uff1a \\[ -\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\beta) + \\lambda \\beta = 0\\] \u89e3\u5f97\uff1a \\[ \\beta = (\\textbf{X}^T\\textbf{X} + \\lambda \\textbf{I})^{-1} \\textbf{X}^T \\textbf{y}\\] \u6211\u4eec\u770b\u5230\uff0c\u5373\u4f7f \\(\\textbf{X}^T\\textbf{X}\\) \u662f\u975e\u6ee1\u79e9\u7684\uff0c\u7531\u4e8e\u591a\u52a0\u4e86\u4e00\u4e2a \\(\\lambda \\textbf{I}\\) \uff0c\u5b83\u4ecd\u662f\u4e00\u4e2a\u53ef\u9006\u77e9\u9635\u3002\u8fd9\u4e5f\u662f ridge regression \u7684\u53e6\u4e00\u4e2a\u4f18\u52bf\u3002 Ridge Regression and SVD \u5947\u5f02\u503c\u5206\u89e3 (singular value decomposition, SVD) \u5c06\u4e00\u4e2a\u77e9\u9635\u5206\u89e3\u4e3a\u4e09\u4e2a\u77e9\u9635\u7684\u4e58\u79ef\uff1a \\[ \\textbf{X}_{N \\times p} = \\textbf{U}_{N \\times N} \\mathbf{\\Sigma}_{N \\times p} \\textbf{V}^T_{p \\times p} \\] \u5176\u4e2d\uff1a \\(\\textbf{U}_{N \\times N}\\) \u662f\u4e00\u4e2a\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635\uff0c\u5728 \\(\\mathbb{R}^{N \\times N}\\) \u7a7a\u95f4\u3002\u5b83\u4ee3\u8868\u4e86\u65cb\u8f6c(rotation) \\(\\mathbf{\\Sigma}_{N \\times p}\\) \u662f\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635\uff0c\u4f46\u662f\u4e0d\u4e00\u5b9a\u662f\u65b9\u9635\u3002\u5b83\u4ee3\u8868\u62c9\u4f38(scaling) \\(\\textbf{V}^T_{p \\times p}\\) \u662f\u4e00\u4e2a\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635\uff0c\u5728 \\(\\mathbb{R}^{p \\times p}\\) \u7a7a\u95f4\u3002\u5b83\u4ee3\u8868\u65cb\u8f6c(rotation) \u5bf9\u4e8e\u666e\u901a\u7684\u7ebf\u6027\u56de\u5f52\uff0c\u6709\uff1a \\[\\begin{align} \\hat{y} = \\textbf{H}y &= \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^Ty \\\\ &= \\textbf{U}\\mathbf{\\Sigma}\\textbf{V}^T(\\textbf{V}\\mathbf{\\Sigma}^T\\mathbf{\\Sigma}\\textbf{V}^T)^{-1} \\textbf{V}\\mathbf{\\Sigma}^T\\textbf{U}^T y \\\\ &= \\textbf{U}\\mathbf{\\Sigma} (\\mathbf{\\Sigma}^T\\mathbf{\\Sigma})^{-1} \\mathbf{\\Sigma}^T\\textbf{U}^T y \\\\ &= \\textbf{U}\\textbf{U}^T y \\end{align}\\] \u800c\u5bf9\u4e8e ridge regression\uff0c\u6709\uff1a \\[\\begin{align} \\hat{y} &= \\textbf{X}(\\textbf{X}^T\\textbf{X} + \\lambda \\textbf{I})^{-1} \\textbf{X}^T \\textbf{y} \\\\ &= \\textbf{U}\\mathbf{\\Sigma}(\\mathbf{\\Sigma}^T\\mathbf{\\Sigma} + \\lambda \\textbf{I})^{-1} \\mathbf{\\Sigma}^T\\textbf{U}^T y \\end{align}\\] \u5047\u8bbe SVD \u5206\u89e3\u7684\u5947\u5f02\u503c\u4e3a \\(\\sigma_1, \\sigma_2, ... , \\sigma_p\\) \uff0c\u6211\u4eec\u6709\uff1a \\[\\begin{align} \\hat{y} &= \\textbf{U}\\mathbf{\\Sigma}(\\mathbf{\\Sigma}^T\\mathbf{\\Sigma} + \\lambda \\textbf{I})^{-1} \\mathbf{\\Sigma}^T\\textbf{U}^T y \\\\ &= \\sum_{j=1}^p \\textbf{u}_j \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda} \\textbf{u}_j^T \\textbf{y} \\end{align}\\] \u5176\u4e2d \\(\\textbf{u}_j\\) \u8868\u793a\u77e9\u9635 \\(\\textbf{U}\\) \u7684\u7b2c \\(j\\) \u5217\u3002 \u56e0\u6b64\uff0c\u4ece\u76f4\u89c2\u610f\u4e49\u4e0a\u7406\u89e3\uff0cridge regression \u76f8\u6bd4\u666e\u901a\u7684 regression \u5c31\u662f\u5bf9 \\(\\textbf{U}\\) \u7684\u6bcf\u4e00\u5217\u9644\u52a0\u4e86\u4e00\u4e2a\u7cfb\u6570 \\(\\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda} \\leq 1\\) \u3002\u8fd9\u4e2a\u7cfb\u6570\u4e0e\u8be5\u5217\u5bf9\u5e94\u7684\u5947\u5f02\u503c\u76f8\u5173\u3002\u800c\u6211\u4eec\u5728 SVD \u5b9a\u4e49\u4e2d\u77e5\u9053 \\(\\sigma_j\\) \u4ee3\u8868\u4e86\u5728 \\(\\textbf{u}_j\\) \u65b9\u5411\u7684\u7f29\u653e\u7cfb\u6570\u3002\u663e\u7136\uff0c \\(\\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda}\\) \u5728 \\(\\sigma_j\\) \u8d8a\u5c0f\u65f6\uff0cshrinkage \u8d8a\u5927\u3002\u56e0\u6b64\uff0c\u76f4\u89c2\u7406\u89e3\uff0cridge regression \u4f1a\u503e\u5411\u4e8e\u5ffd\u7565\u8f93\u5165 \\(\\textbf{X}\\) \u65b9\u5dee\u8f83\u5c0f\u7684\u65b9\u5411\u3002 the small singular values correspond to directions in the column space of X having small variance, and ridge regression shrinks these directions the most. \u8fd9\u662f\u4e2a\u6bd4\u8f83\u5408\u7406\u7684\u5047\u8bbe\uff0c\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5bf9\u4e8e\u6837\u672c\u4e2d\u51e0\u4e4e\u4e00\u6837\u7684\u8f93\u5165\u53c2\u6570\u5e76\u4e0d\u662f\u5f88\u5173\u5fc3. Reference ESL solution ESL Chinese Simple Linear Regression Proofs involving ordinary least squares","title":"ESL 3: Linear Methods for Regression"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#esl-3-linear-methods-for-regression","text":"\u4e00\u4e2a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u5047\u8bbe\u56de\u5f52\u51fd\u6570 E(Y|X) \u5bf9\u4e8e\u8f93\u5165 X \u662f\u7ebf\u6027\u7684\u3002 \u5b83\u7684\u4f18\u52bf\u5728\u4e8e\uff1a \u7b80\u5355 \u80fd\u591f\u8868\u793a\u6bcf\u4e2a\u8f93\u5165\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd \u8f93\u5165\u53ef\u4ee5\u8fdb\u884c\u53d8\u6362 \u4ed6\u4eec\u6709\u65f6\u5019\u6bd4\u590d\u6742\u7684\u65b9\u6cd5\u66f4\u7cbe\u51c6\uff0c\u5c24\u5176\u662f\u5728\u6837\u672c\u6570\u91cf\u5c11\u3001\u4f4e\u4fe1\u566a\u6bd4\u6216\u8005\u7a00\u758f\u77e9\u9635\u7684\u60c5\u5f62\u3002","title":"ESL 3: Linear Methods for Regression"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#32-linear-regression-models-and-least-squares","text":"\\(p\\) \u7ef4\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u5f62\u5f0f\u5982\u4e0b\uff1a \\[f(X) = \\beta_0 + \\sum_{j=1}^p X_j \\beta_j\\] \u6211\u4eec\u9700\u8981\u4f30\u8ba1\u4e00\u7ec4\u53c2\u6570 \\(\\beta\\) \uff0c\u4f7f\u6b8b\u5dee\u5e73\u65b9\u548c\uff08Residual Sum of Squares\uff09\u6700\u5c0f\uff1a \\[\\begin{align} \\text{RSS}(\\beta) &= (\\textbf{y} - \\textbf{X}\\beta )^T(\\textbf{y} - \\textbf{X}\\beta ) \\\\ &= \\textbf{y}^T\\textbf{y} - \\textbf{y}^T\\textbf{X}\\beta - \\beta^T\\textbf{X}^T\\textbf{y} + \\beta^T\\textbf{X}^T\\textbf{X}\\beta \\end{align}\\] \u5176\u4e2d\uff0c \\(\\textbf{X}\\) \u662f\u4e00\u4e2a \\(N \\times (p+1)\\) \u77e9\u9635\uff0c \\(\\textbf{y}\\) \u662f \\(N \\times 1\\) \u89c2\u6d4b\u503c\u3002 \u5bf9 \\(\\beta\\) \u6c42\u5bfc\u53ef\u4ee5\u5f97\u5230\uff1a \\[ \\frac{\\partial \\text{RSS}(\\beta)}{\\partial \\beta} = -2 \\textbf{X}^T\\textbf{y} + 2\\textbf{X}^T\\textbf{X} \\beta\\] \u7531\u4e8e\u4e8c\u9636\u5bfc\u6570\u6b63\u5b9a\uff0c\u4ee4\u4e00\u9636\u5bfc\u6570\u4e3a 0 \u5411\u91cf\uff0c\u5f97\u51fa\u6781\u503c\u70b9\uff08\u5373\u4f30\u8ba1\u503c\uff09\uff1a \\[ \\hat{\\beta}= (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}\\] \\[\\hat{\\textbf{y}} = \\textbf{X} \\hat{\\beta} = \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}\\] \u6211\u4eec\u79f0 \\(\\textbf{H} = \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\) \u4e3a\u4f30\u8ba1\u77e9\u9635\uff08\"hat\" matrix\uff09\uff0c\u5b83\u6ee1\u8db3\u5bf9\u79f0\u6027\u548c\u5e42\u7b49\u6027\uff1a \\[\\textbf{H}^T = \\textbf{H}\\] \\[\\textbf{H}^T\\textbf{H} = \\textbf{H}\\] \u5f53 \\(\\textbf{X}\\) \u4e2d\u67d0\u4e9b\u5217\u7ebf\u6027\u76f8\u5173\uff08\u5373\u975e\u6ee1\u79e9\u77e9\u9635\uff09\u65f6\uff0c \\((\\textbf{X}^T\\textbf{X})\\) \u662f\u5947\u5f02\u77e9\u9635\uff0c\u5b83\u53ea\u80fd\u6c42\u5e7f\u4e49\u9006\u77e9\u9635\uff0c\u4e0d\u6b62\u4e00\u4e2a\u89e3\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5c06\u5197\u4f59\u7684\u8f93\u5165\u5254\u9664\u6389\uff0c\u5927\u90e8\u5206\u6c42\u89e3\u8f6f\u4ef6\u90fd\u5b9e\u73b0\u4e86\u8fd9\u4e2a\u529f\u80fd\u3002","title":"3.2 Linear Regression Models and Least Squares"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#_1","text":"\u4e3a\u4e86\u786e\u5b9a\u4f30\u8ba1\u7684\u53c2\u6570 \\(\\hat{\\beta}\\) \u7684\u7edf\u8ba1\u7279\u6027\uff0c\u6211\u4eec\u5047\u8bbe\uff1a \u6bcf\u4e2a\u89c2\u6d4b\u503c \\(y_i\\) \u76f8\u4e92\u72ec\u7acb \\(y_i\\) \u6709\u56fa\u5b9a\u7684\u566a\u58f0 \\(\\varepsilon \\sim N(0, \\sigma^2)\\) \u90a3\u4e48\u4f30\u8ba1\u503c \\(\\hat{\\beta}\\) \u7684\u65b9\u5dee\u4e3a\uff1a \\[ \\text{Var}(\\hat{\\beta}) = (\\textbf{X}^T\\textbf{X})^{-1} \\sigma^2\\] where: \\[\\hat{\\sigma}^2 = \\frac{\\text{RSS}}{N-p-1}= \\frac{1}{N-p-1} \\sum_{i=1}^{N} (y_i-\\hat{y})^2\\]","title":"\u4f30\u8ba1\u53c2\u6570\u7684\u7edf\u8ba1\u7279\u6027"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#_2","text":"N \u4e2a y \u7684\u89c2\u6d4b\u503c\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a \\[ \\textbf{y} = \\textbf{X}\\beta + \\varepsilon \\] \u5176\u4e2d \\(\\varepsilon\\) \u662f \\(N \\times 1\\) \u7684\u566a\u58f0\u3002\u56e0\u6b64\u6709\uff1a \\[\\begin{align} \\hat{\\beta} &= (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y} \\\\ &= \\beta + (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\varepsilon \\end{align}\\] \u65e0\u504f\u6027\uff08\u671f\u671b\u503c\u4e3a \\(\\beta\\) \uff09\uff1a \\[E(\\hat{\\beta}) = \\beta + (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T E(\\varepsilon) = \\beta\\] \u534f\u65b9\u5dee\u77e9\u9635\uff08\u6ce8\u610f\u662f \\(\\beta \\beta^T\\) \u800c\u975e \\(\\beta^T \\beta\\) \uff0c\u662f\u4e00\u4e2a\u77e9\u9635\uff09\uff1a \\[\\begin{align} \\text{Var}(\\hat{\\beta}) &= E[(\\beta - \\hat{\\beta})(\\beta - \\hat{\\beta})^T] \\\\ &=E[(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\varepsilon\\varepsilon^T\\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}] \\\\ &= (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T E(\\varepsilon\\varepsilon^T) \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1} \\\\ &= \\sigma^2 (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T \\textbf{I} \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1} \\\\ &= \\sigma^2 (\\textbf{X}^T\\textbf{X})^{-1} \\end{align}\\] \u53ef\u4ee5\u5f97\u5230\uff1a \\[ \\hat{\\beta} \\sim N(\\beta, \\sigma^2 (\\textbf{X}^T\\textbf{X})^{-1})\\] \u4e0b\u9762\u6765\u786e\u5b9a \\(\\sigma^2\\) \u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u89c2\u6d4b\u503c \\(y\\) \u548c\u9884\u6d4b\u503c \\(\\hat{y}\\) \u7684\u5dee\u6765\u5f97\u5230\u566a\u58f0 \\(\\varepsilon\\) \u3002 \\[\\begin{align} y - \\hat{y} &= \\textbf{X}\\beta + \\varepsilon -\\textbf{X}\\hat{\\beta} \\\\ &= \\textbf{X}\\beta + \\varepsilon - \\textbf{X}(\\beta + (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\varepsilon) \\\\ &= (\\textbf{I -H} )\\varepsilon \\end{align}\\] \\[\\begin{align} \\sum_{i=1}^N(y_i - \\hat{y_i})^2 &= (y - \\hat{y})^T (y - \\hat{y}) \\\\ &= \\varepsilon^T(\\textbf{I - H}) \\varepsilon \\\\ &= \\sum_{k =1}^N \\varepsilon_k^2- \\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij} \\end{align}\\] \u5176\u671f\u671b\u503c\u4e3a\uff1a \\[\\begin{align} E[\\sum_{i=1}^N(y_i - \\hat{y_i})^2] &= E[\\sum_{k =1}^N \\varepsilon_k^2- \\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij} ] \\\\ &= N\\sigma^2 - E(\\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij}) \\end{align}\\] \u7531\u4e8e \\(\\varepsilon_i, \\varepsilon_j\\) \u662f\u72ec\u7acb\u7684\uff0c\u5f53 \\(i \\neq j\\) \u65f6\uff1a \\[\\text{Cov}(\\varepsilon_i, \\varepsilon_j) = E(\\varepsilon_i \\varepsilon_j) - E(\\varepsilon_i)E(\\varepsilon_j) = 0\\] \u56e0\u6b64\uff1a \\[\\begin{align} E[\\sum_{i=1}^N(y_i - \\hat{y_i})^2] &= N\\sigma^2 - E(\\sum_{i, j = 1}^N \\varepsilon_i \\varepsilon_j H_{ij}) \\\\ &= N\\sigma^2 - E(\\sum_{i=1}^{N}\\varepsilon_i^2H_{ii}) \\\\ &= \\sigma^2[N - \\text{trace}(\\textbf{H})] \\end{align}\\] \u8fd9\u91cc\u518d\u5229\u7528\u516c\u5f0f\uff1a \\[\\text{trace}(ABC) = \\text{trace}(CAB)\\] \u5f97\u5230\uff1a \\[\\begin{align} E[\\sum_{i=1}^N(y_i - \\hat{y_i})^2] &= \\sigma^2[N - \\text{trace}(\\textbf{H})] \\\\ &= \\sigma^2[N - \\text{trace}(\\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T)] \\\\ &= \\sigma^2[N - \\text{trace}(\\textbf{X}^T \\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1}_{(p+1) \\times (p+1)})] \\\\ &= \\sigma^2[N - \\text{trace}(\\textbf{I}_{(p+1) \\times (p+1)})] \\\\ &= \\sigma^2(N - p -1) \\end{align}\\] \u56e0\u6b64\uff0c\u5bf9 \\(\\sigma^2\\) \u7684\u65e0\u504f\u4f30\u8ba1\u5c31\u662f\uff1a \\[\\hat{\\sigma}^2 = \\frac{1}{N-p-1} \\sum_{i=1}^{N} (y_i-\\hat{y})^2\\]","title":"\u8bc1\u660e"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#_3","text":"\u7531\u4e8e\u6211\u4eec\u5bf9\u7b2c i \u4e2a\u6837\u672c\u7684\u566a\u58f0 \\(\\varepsilon_i\\) \u65e0\u504f\u4f30\u8ba1\u5c31\u662f \\(\\hat{\\varepsilon_i} = y_i - \\hat{y_i}\\) \uff0c\u6211\u4eec\u8ba1\u7b97\u5176\u65b9\u5dee\uff1a \\[\\begin{align} \\text{Var}(\\hat{\\varepsilon}) &= \\text{Var}(\\textbf{y} - \\hat{\\textbf{y}}) \\\\ &= \\text{Var}[(\\textbf{I} - \\textbf{H}){\\varepsilon}] \\end{align}\\] \u7531\u4e8e \\(D(AX) = AD(X)A^T\\) \uff1a \\[\\begin{align} \\text{Var}(\\hat{\\varepsilon}) &= \\text{Var}[(\\textbf{I} - \\textbf{H}){\\varepsilon}] \\\\ &= (\\textbf{I} - \\textbf{H}) \\text{Var}(\\varepsilon) (\\textbf{I} - \\textbf{H}) \\end{align}\\] \u7531\u4e8e \\(\\varepsilon \\sim N(0, \\sigma^2)\\) \uff0c\u56e0\u6b64\uff1a \\[\\text{Var}(\\varepsilon) = \\sigma^2 \\textbf{I}_{N \\times N}\\] \u800c \\(\\textbf{H} = \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\) \u6ee1\u8db3\u5bf9\u79f0\u6027\u548c\u5e42\u7b49\u6027\uff1a \\[\\textbf{H}^T = \\textbf{H}\\] \\[\\textbf{H}^T\\textbf{H} = \\textbf{H}\\] \u56e0\u6b64\u6709\u7ed3\u8bba\uff1a \\[\\text{Var}(\\hat{\\varepsilon}) = \\sigma^2 (\\textbf{I} - \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T)\\]","title":"\u6a21\u578b\u8bef\u5dee\u7684\u7edf\u8ba1\u7279\u6027"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#_4","text":"\u5f53\u6211\u4eec\u5224\u65ad\u54ea\u4e9b\u53c2\u6570\u53ef\u4ee5\u5ffd\u7565\u4ee5\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 F-statistic \u8fdb\u884c\u663e\u8457\u6027\u5206\u6790\u3002\u5047\u8bbe\u6211\u4eec\u5c06 \\(\\beta\\) \u7ef4\u5ea6\u4ece \\(p_1 + 1\\) \u964d\u4f4e\u5230 \\(p_0 + 1\\) \uff1a \\[ F = \\frac{(\\text{RSS}_0 - \\text{RSS}_1) / (p_1 - p_0)}{\\text{RSS}_1 / (N- p_1 -1)} \\] F-statistic \u63cf\u8ff0\u4e86\u6bcf\u4e2a\u88ab\u5ffd\u7565\u7684\u53c2\u6570\u5bf9 RSS \u7684\u5e73\u5747\u8d21\u732e\uff0c\u7528 \\(\\hat{\\sigma}^2\\) \u8fdb\u884c\u4e86 normalize\u3002 \u5f53 \\(p_1 - p_0 =1\\) \u5373\u4ec5\u53bb\u6389\u4e00\u4e2a\u53c2\u6570\u65f6\uff08\u5047\u8bbe \\(\\beta_j = 0\\) \uff09\uff0c\u8be5\u516c\u5f0f\u53ef\u4ee5\u7b80\u5316\u4e3a\u5bf9\u5e94\u7684 z-score \u7684\u5e73\u65b9\uff0c\u5176\u4e2d z-score \u4e3a\uff1a \\[ z_j = \\frac{\\hat{\\beta}_j}{\\hat{\\sigma} \\sqrt{v_j} }\\] where: \\[\\hat{\\sigma}^2 =\\frac{\\text{RSS}_1}{N-p-1} =\\frac{1}{N-p-1} \\sum_{i=1}^{N} (y_i-\\hat{y})^2\\] \\[v_j = (\\textbf{X}^T\\textbf{X})^{-1}_{jj}\\]","title":"\u663e\u8457\u6027\u5206\u6790"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#_5","text":"\u8fd9\u4e2a\u8bc1\u660e\u540c\u65f6\u4e5f\u662f\u4e60\u9898 3.1 Ex. 3.1 Show that the F statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding z-score (3.12). \u5b9e\u9645\u4e0a\u6211\u4eec\u9700\u8981\u8bc1\u660e\uff0c\u5728\u53bb\u6389\u6a21\u578b\u7684\u7b2c j \u4e2a\u53c2\u6570\u540e\uff1a \\[ \\text{RSS}_0 - \\text{RSS}_1 = \\frac{\\hat{\\beta}_j^2}{v_j} \\] \u4e0a\u5f0f\u4e2d\u552f\u4e00\u672a\u77e5\u7684\u5c31\u662f \\(\\text{RSS}_0\\) \uff0c\u5b83\u5b9e\u8d28\u4e0a\u662f\u6c42\u4e00\u4e2a\u5e26\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff1a \\[\\begin{align} \\min_{\\beta \\in \\mathbb{R}^{(p+1) \\times 1}} (\\textbf{y} - \\textbf{X}\\beta)^T(\\textbf{y}-\\textbf{X}\\beta) \\\\ \\text{s.t.} ~\\beta_j = 0 \\end{align}\\] \u6211\u4eec\u53ef\u4ee5\u7528\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u6cd5\u6765\u89e3\u51b3\u3002 \\[L(\\beta, \\lambda) = (\\textbf{y} - \\textbf{X}\\beta)^T(\\textbf{y}-\\textbf{X}\\beta) + \\lambda e_j^T \\beta \\] \u5bf9 \\(\\beta\\) \u6c42\u5bfc\uff0c\u5e76\u4ee4\u5bfc\u6570\u4e3a 0\uff0c\u6709\uff1a \\[\\frac{\\partial L(\\beta, \\lambda)}{\\partial \\beta} = - 2\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\beta) + \\lambda e_j = 0\\] \u89e3\u51fa\uff1a \\[\\begin{align} \\beta_0 &= (\\textbf{X}^T\\textbf{X})^{-1} \\textbf{X}^T\\textbf{y} - \\frac{\\lambda}{2}(\\textbf{X}^T \\textbf{X})^{-1} e_j \\\\ &= \\hat{\\beta}- \\frac{\\lambda}{2}(\\textbf{X}^T \\textbf{X})^{-1} e_j \\end{align}\\] \u7b49\u5f0f\u4e24\u8fb9\u4e58\u4ee5 \\(e_j^T\\) \uff0c\u5e76\u5e26\u5165 \\(\\beta_j = 0\\) \uff0c\u6709\uff1a \\[\\begin{align} e_j^T\\beta_0 = 0 &= e_j^T \\hat{\\beta} + \\frac{\\lambda}{2} e_j^T(\\textbf{X}^T \\textbf{X})^{-1} e_j \\\\ &= \\hat{\\beta}_j + \\frac{\\lambda}{2}v_j \\end{align}\\] \u56e0\u6b64\u6709\uff1a \\[\\lambda = - \\frac{2\\hat{\\beta}_j}{v_j}\\] \u5e26\u5165\u53ef\u5f97\uff1a \\[\\begin{align} \\text{RSS}_0 &= (\\textbf{y} - \\textbf{X}\\beta_0)^T(\\textbf{y}-\\textbf{X}\\beta_0) \\\\ &= (\\textbf{y} - \\textbf{X}\\hat{\\beta} + \\frac{\\lambda}{2}\\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1} e_j)^T(\\textbf{y}-\\textbf{X}\\hat{\\beta} + \\frac{\\lambda}{2}\\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1} e_j) \\\\ &= \\text{RSS}_1 + \\frac{\\lambda}{2} [e_j^T(\\textbf{X}^T \\textbf{X})^{-1}\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\hat{\\beta}) + (\\textbf{y} - \\textbf{X}\\hat{\\beta})^T \\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1} e_j)] \\\\ &~~~~ + \\frac{\\lambda^2}{4}e_j^T (\\textbf{X}^T \\textbf{X})^{-1} e_j \\\\ &= \\text{RSS}_1 + \\frac{\\lambda^2}{4}e_j^T (\\textbf{X}^T \\textbf{X})^{-1} e_j \\\\ &= \\text{RSS}_1 + \\frac{\\hat{\\beta}_j^2}{v_j} \\end{align}\\] \u5176\u4e2d\uff0c\u4e2d\u95f4\u9879\u53ef\u4ee5\u6d88\u53bb\u7684\u539f\u56e0\u662f\uff1a \\[\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\hat{\\beta}) = \\textbf{X}^T[\\textbf{y} - \\textbf{X}(\\textbf{X}^T \\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}] = 0 \\] \u76f4\u89c2\u7406\u89e3\uff0c \\(\\textbf{X}\\) \u548c \\(\\textbf{y} - \\textbf{X}\\hat{\\beta}\\) \u662f\u6b63\u4ea4\u7684\uff0c\u56e0\u4e3a \\(\\textbf{X}\\hat{\\beta}\\) \u6b63\u662f \\(\\textbf{y}\\) \u5728 \\(\\textbf{X}\\) \u6240\u5728\u5e73\u9762\u4e0a\u7684\u6295\u5f71\u3002","title":"\u8bc1\u660e"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#322-the-gaussmarkov-theorem","text":"\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5f97\u51fa\u7684 \\(\\beta\\) \u5728\u6240\u6709\u7ebf\u6027 \u65e0\u504f \u4f30\u8ba1\u4e2d\u5747\u65b9\u8bef\u5dee\u6700\u5c0f\u3002\u5f53\u7136\uff0c\u5982\u679c\u6211\u4eec\u613f\u610f\u4e3a\u4e86\u8fdb\u4e00\u6b65\u51cf\u5c0f\u8bef\u5dee\u5f15\u5165\u4e00\u70b9 bias\uff0c\u5b8c\u5168\u53ef\u80fd\u627e\u5230\u4e00\u4e2a\u66f4\u5c0f\u5747\u65b9\u8bef\u5dee\u7684 \u6709\u504f \u4f30\u8ba1\u3002 the least squares estimates of the parameters \u03b2 have the smallest variance among all linear unbiased estimates \u73b0\u5728\u6211\u4eec\u6765\u8bc1\u660e\u8fd9\u4e2a\u7ed3\u8bba\u3002\u5bf9\u4e8e\u7ebf\u6027\u4f30\u8ba1\uff1a \\[\\textbf{y} = \\textbf{X}\\beta\\] \\(\\textbf{y}\\) \u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u90fd\u53ef\u4ee5\u770b\u4f5c \\(\\textbf{X}\\) \u4e2d\u7684\u4e00\u884c\u4e0e\u5411\u91cf \\(\\beta\\) \u7684\u7ebf\u6027\u7ec4\u5408\u3002","title":"3.2.2 The Gauss\u2013Markov Theorem"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#_6","text":"\u90a3\u4e48\uff0c\u9488\u5bf9\u65e0\u504f\u6027\uff0c\u6211\u4eec\u9700\u8981\u8bc1\u660e\u6700\u5c0f\u4e8c\u4e58\u6cd5\u4f30\u8ba1\u51fa\u7684 \\(\\hat{\\beta}\\) \u6ee1\u8db3\uff1a \\[ E(\\alpha^T \\hat{\\beta}) = \\alpha^T\\beta\\] \u5176\u4e2d \\(\\alpha\\) \u662f\u4efb\u610f\u5411\u91cf\u3002 \\[\\begin{align} E(\\alpha^T \\hat{\\beta}) &= E(\\alpha^T (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}) \\\\ &= E(\\alpha^T (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{X} \\beta) \\\\ &= \\alpha^T \\beta \\end{align} \\]","title":"\u65e0\u504f\u6027"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#_7","text":"Gauss\u2013Markov theorem \u6307\u51fa\uff0c\u5982\u679c\u8fd8\u5b58\u5728\u5176\u4ed6\u7ebf\u6027\u4f30\u8ba1 \\(c^T \\textbf{y}\\) \u6ee1\u8db3\uff1a \\[ E(c^T \\textbf{y}) = \\alpha^T\\beta\\] \u90a3\u4e48\u5fc5\u7136\u6709\uff1a \\[\\text{Var}(\\alpha^T \\hat{\\beta}) \\leq \\text{Var}(c^T \\textbf{y})\\] \u8bc1\u660e\uff1a TBD","title":"\u5747\u65b9\u8bef\u5dee\u6700\u5c0f"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#33-subset-selection","text":"\u6700\u5c0f\u4e8c\u4e58\u6cd5\u7684\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a \u9884\u6d4b\u7cbe\u5ea6\u3002\u867d\u7136\u5b83\u662f\u65e0\u504f\u7684\uff0c\u4f46\u662f\u65b9\u5dee\u5f88\u5927\u3002\u5982\u679c\u6211\u4eec\u5ffd\u7565\u4e00\u90e8\u5206\u6a21\u578b\u53c2\u6570\uff0c\u867d\u7136\u4f1a\u53d8\u6210\u6709\u504f\u4f30\u8ba1\uff0c\u4f46\u662f\u53ef\u80fd\u4f1a\u6781\u5927\u63d0\u9ad8\u7cbe\u5ea6\u3002 \u53ef\u89e3\u91ca\u6027\uff08\u5373\u6a21\u578b\u590d\u6742\u5ea6\uff09\u3002\u5f53\u6a21\u578b\u53c2\u6570\u5f88\u591a\u65f6\uff0c\u6211\u4eec\u60f3\u53bb\u786e\u5b9a\u4e00\u5c0f\u90e8\u5206\u5177\u6709\u6700\u5927\u5f71\u54cd\u7684\u6a21\u578b\u53c2\u6570\uff0c\u4e3a\u6b64\u6211\u4eec\u613f\u610f\u727a\u7272\u4e00\u90e8\u5206\u65e0\u5173\u7d27\u8981\u7684\u53c2\u6570\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u9009\u53d6\u53d8\u91cf\u5b50\u96c6\uff0c\u5373\u201cmodel selection\u201d\u3002","title":"3.3 Subset Selection"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#331-best-subset-selection","text":"\u6700\u4f73\u5b50\u96c6\u662f\u6307\u4ece\u6240\u6709\u5177\u6709 \\(k (k <= p)\\) \u4e2a\u53d8\u91cf\u7684\u5b50\u96c6\u4e2d\uff0cRSS \u6700\u5c0f\u7684\u90a3\u4e2a\u3002 \u5f53\u7136\uff0c\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u5c31\u662f\u4ece\u904d\u5386\u6240\u6709\u7684\u7ec4\u5408\u3002\u8fd9\u6837\u505a\u7684\u590d\u6742\u5ea6\u662f \\(2^p\\) \uff0c\u53ea\u9002\u7528\u4e8e\u5c0f\u89c4\u6a21\u7684\u95ee\u9898\u3002","title":"3.3.1 Best-Subset Selection"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#332-forward-and-backward-stepwise-selection","text":"\u201c\u524d\u5411\u9010\u6b65\u9009\u62e9\u201d\u662f\u4e00\u79cd\u8d2a\u5fc3\u7b97\u6cd5\u3002\u5b83\u6309\u987a\u5e8f\u52a0\u5165\u6700\u80fd\u63d0\u9ad8\u62df\u5408\u5ea6\u7684\u53c2\u6570\u3002\u5b83\u867d\u7136\u4e0d\u4e00\u5b9a\u627e\u5230\u6700\u4f18\u89e3\uff0c\u4f46\u662f\u5b83\u4f18\u52bf\u5728\u4e8e\uff1a \u8fd0\u7b97\u91cf\u5c0f\u3002\u5f53\u7ef4\u5ea6 \\(p >= 40\\) \u65f6\uff0c\u51e0\u4e4e\u65e0\u6cd5\u7b97\u51fa\u6700\u4f18\u89e3\u3002\u4f46\u662f\u4f9d\u65e7\u53ef\u4ee5\u7528 forward stepwise selection \uff08\u5373\u4f7f\u7ef4\u5ea6 p \u5927\u4e8e\u6837\u672c\u6570 N\uff09\u3002 \u65b9\u5dee\u5c0f\u3002\u6700\u4f18\u5b50\u96c6\u65b9\u5dee\u6bd4 forward stepwise selection \u5927\uff0c\u867d\u7136\u540e\u8005\u53ef\u80fd\u4f1a\u6709\u4e00\u5b9a\u7684 bias\u3002 \u90a3\u4e48\u5982\u4f55\u9009\u62e9\u201c\u6700\u80fd\u63d0\u9ad8\u62df\u5408\u5ea6\u201c\u7684\u53c2\u6570\u5462\uff1f\u6211\u4eec\u5728\u4e4b\u524d\u201c\u663e\u8457\u6027\u5206\u6790\u201d\u4e2d\u5df2\u7ecf\u8bc1\u660e\u4e86\uff0c\u53bb\u6389\u4e00\u4e2a\u53c2\u6570\u5bf9\u6b8b\u5dee\u7684\u5f71\u54cd\u4e3a\u5176 z-score \u7684\u5e73\u65b9\u3002\u90a3\u4e48\uff0c\u6211\u4eec\u76f4\u63a5 \u4ece z-score \u6700\u5927\u7684\u53c2\u6570\u5f00\u59cb\u4f9d\u6b21\u52a0\u5165 \u5373\u53ef\u3002\u7b2c \\(j\\) \u4e2a\u53c2\u6570\u7684 z-score \u53ef\u4ee5\u7531\u4e8e\u4e0b\u5f0f\u8ba1\u7b97\uff1a \\[ z_j = \\frac{\\hat{\\beta}_j}{\\hat{\\sigma} \\sqrt{v_j} }\\] where: \\[\\hat{\\sigma}^2 =\\frac{\\text{RSS}_1}{N-p-1} =\\frac{1}{N-p-1} \\sum_{i=1}^{N} (y_i-\\hat{y})^2\\] \\[v_j = (\\textbf{X}^T\\textbf{X})^{-1}_{jj}\\] \u201c\u540e\u5411\u9010\u6b65\u9009\u62e9\u201d \u4e0e \u201c\u524d\u5411\u9010\u6b65\u9009\u62e9\u201c\u76f8\u53cd\u3002\u5b83\u4ece\u5168\u96c6\u5f00\u59cb\uff0c\u4f9d\u6b21\u53bb\u6389\u6700\u65e0\u5173\u7d27\u8981\u7684\u53d8\u91cf\uff08z-score \u6700\u5c0f\u7684\uff09\u3002\u5b83\u53ea\u80fd\u7528\u4e8e\u6837\u672c\u6570 N \u5927\u4e8e\u7ef4\u5ea6 p \u7684\u60c5\u5f62\u3002","title":"3.3.2 Forward- and Backward-Stepwise Selection"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#34-shrinkage-methods","text":"Subset selection \u786e\u5b9e\u53ef\u4ee5\u5e2e\u6211\u4eec\u7b80\u5316\u6a21\u578b\uff0c\u5e76\u4e14\u8fd8\u53ef\u80fd\u964d\u4f4e\u8bef\u5dee\u3002\u4f46\u662f\uff0c\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u79bb\u6563\u7684\u8fc7\u7a0b\uff08\u53c2\u6570\u8981\u4e48\u88ab\u4e22\u5f03\u8981\u4e48\u88ab\u4fdd\u7559\uff0c\u6ca1\u6709\u4e2d\u95f4\u72b6\u6001\uff09\uff0c\u5b83\u901a\u5e38\u5177\u6709\u8f83\u5927\u7684\u65b9\u5dee\u3002Shrinkage methods \u66f4\u52a0\u8fde\u7eed\uff0c\u56e0\u6b64\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u3002","title":"3.4 Shrinkage Methods"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#341-ridge-regression","text":"Ridge Regression \u901a\u8fc7\u7ed9\u53c2\u6570\u6570\u91cf\u589e\u52a0\u4e00\u4e2a\u60e9\u7f5a\u9879\u6765\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u3002\u5b83\u7684\u4f18\u5316\u76ee\u6807\uff1a \\[\\hat{\\beta} = \\mathop{\\arg \\min}_{\\beta} \\sum_{i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\\] \u8fd9\u91cc\u7684 \\(\\lambda\\) \u63a7\u5236\u6a21\u578b\u201c\u7f29\u5c0f\u201d\u7684\u7a0b\u5ea6\uff0c \\(\\lambda\\) \u8d8a\u5927\uff0c\u5f97\u5230\u7684\u6a21\u578b\u590d\u6742\u5ea6\u8d8a\u4f4e\u3002 \u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c \u60e9\u7f5a\u9879\u4e2d\u4e0d\u5305\u542b\u5e38\u6570\u9879 \\(\\beta_0\\) \uff0c\u5426\u5219\u6a21\u578b\u4e0d\u7a33\u5b9a\u3002\u5f53\u9009\u53d6 \\(y_i = y_i + c\\) \u65f6\uff0c\u9884\u6d4b\u503c \\(\\hat{y}_i\\) \u7684\u53d8\u5316\u91cf\u4e0d\u662f \\(c\\) \u3002 \u4e0e\u7ecf\u5178\u7684 Linear Regression \u4e0d\u540c\uff0cRidge Regression \u8981\u6c42\u8f93\u5165 \\(\\textbf{X}, \\textbf{y}\\) \u662f\u7ecf\u8fc7\u4e86 \u4e2d\u5fc3\u5316 (centering) \u7684\u3002\u5e76\u4e14\uff0c\u8fd9\u91cc\u7684\u6a21\u578b\u53c2\u6570 \\(\\beta\\) \u662f \\(p\\) \u7ef4\u800c\u4e0d\u662f \\(p+1\\) \u7ef4\u7684\u3002 \u4e0b\u9762\u6211\u4eec\u6765\u8bc1\u660e\u8fd9\u4e00\u70b9\u3002 \\(\\beta_0\\) \u7531\u4e8e\u4e0d\u542b \\(\\lambda\\) \uff0c\u53ef\u4ee5\u5355\u72ec\u4f18\u5316\u3002\u6211\u4eec\u5148\u5bf9 \\(\\beta_0\\) \u6c42\u5bfc\uff0c\u5e76\u4ee4\u5bfc\u6570\u4e3a0: \\[\\sum_{i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j) = 0\\] \u5f97\u5230\uff1a \\[\\beta_0 = \\frac{1}{N}(\\sum_{i=1}^N y_i - \\sum_{i=1}^N \\sum_{j=1}^{p} x_{ij}\\beta_j) \\] \u4ee4 \\(\\overline{x_j} = \\frac{1}{N} \\sum_{i=1}^N x_{ij}\\) \uff0c\u6709\uff1a \\[\\beta_0 = \\frac{1}{N}\\sum_{i=1}^N y_i - \\sum_{j=1}^{p} \\overline{x_{j}} \\beta_j \\] \u6211\u4eec\u4ee5\u4e0b\u7684\u53d8\u5f62\u4e3b\u8981\u662f\u4e3a\u4e86\u5c06\u4f18\u5316\u76ee\u6807\u51fd\u6570\u5199\u6210\u77e9\u9635\u4e58\u6cd5\u5f62\u5f0f\uff0c\u4ee5\u8fdb\u884c\u8fd0\u7b97\u3002 \\[\\begin{align} \\hat{\\beta} &= \\mathop{\\arg \\min}_{\\beta} \\sum_{i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\\\ &= \\mathop{\\arg \\min}_{\\beta} \\sum_{i=1}^N(y_i - \\beta_0 - \\sum_{j=1}^p \\overline{x_j}\\beta_j - \\sum_{j=1}^p (x_{ij} - \\overline{x_j}) \\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\end{align}\\] \u73b0\u5728\u6211\u4eec\u4ee4\uff1a \\[\\begin{align} \\beta_0^c &= \\beta_0 + \\sum_{j=1}^p \\overline{x_j}\\beta_j =\\frac{1}{N} \\sum_{i=1}^N y_{i} \\\\ \\beta_j^c&= \\beta_j & (j>=1) \\end{align}\\] \u53ef\u4ee5\u5f97\u51fa\uff1a \\[\\begin{align} \\hat{\\beta} &= \\mathop{\\arg \\min}_{\\beta^c} \\sum_{i=1}^N(y_i - \\beta_0^c - \\sum_{j=1}^p (x_{ij} - \\overline{x_j}) \\beta_j^c)^2 + \\lambda \\sum_{j=1}^p {\\beta_j^c}^2 \\end{align}\\] \u6211\u4eec\u518d\u4ee4\uff1a \\[\\begin{align} y_i^c &= y_i - \\beta_0^c = y_i - \\frac{1}{N} \\sum_{i=1}^N y_i \\\\ x_{ij}^c&= x_{ij} - \\overline{x_j} & (j >=1) \\end{align}\\] \u6709\uff1a \\[\\begin{align} \\hat{\\beta} &= \\mathop{\\arg \\min}_{\\beta^c} \\sum_{i=1}^N(y_i^c - \\sum_{j=1}^p (x_{ij}^c \\beta_j^c)^2) + \\lambda \\sum_{j=1}^p {\\beta_j^c}^2 \\\\ &=\\mathop{\\arg \\min}_{\\beta} (\\textbf{y} - \\textbf{X}\\beta)^T(\\textbf{y} - \\textbf{X}\\beta) + \\lambda(\\beta^T\\beta) \\end{align}\\] \u5176\u4e2d\uff0c \\(\\textbf{X}, \\textbf{y}, \\beta\\) \u90fd\u7ecf\u8fc7\u4e86\u4e2d\u5fc3\u5316\uff0c\u5e76\u4e14\u662f \\(p\\) \u7ef4\u7684\u3002 \u8be5\u5f0f\u5bf9 \\(\\beta\\) \u6c42\u5bfc\u5e76\u4ee4\u5bfc\u6570\u4e3a 0\uff0c\u6709\uff1a \\[ -\\textbf{X}^T(\\textbf{y} - \\textbf{X}\\beta) + \\lambda \\beta = 0\\] \u89e3\u5f97\uff1a \\[ \\beta = (\\textbf{X}^T\\textbf{X} + \\lambda \\textbf{I})^{-1} \\textbf{X}^T \\textbf{y}\\] \u6211\u4eec\u770b\u5230\uff0c\u5373\u4f7f \\(\\textbf{X}^T\\textbf{X}\\) \u662f\u975e\u6ee1\u79e9\u7684\uff0c\u7531\u4e8e\u591a\u52a0\u4e86\u4e00\u4e2a \\(\\lambda \\textbf{I}\\) \uff0c\u5b83\u4ecd\u662f\u4e00\u4e2a\u53ef\u9006\u77e9\u9635\u3002\u8fd9\u4e5f\u662f ridge regression \u7684\u53e6\u4e00\u4e2a\u4f18\u52bf\u3002","title":"3.4.1 Ridge Regression"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#ridge-regression-and-svd","text":"\u5947\u5f02\u503c\u5206\u89e3 (singular value decomposition, SVD) \u5c06\u4e00\u4e2a\u77e9\u9635\u5206\u89e3\u4e3a\u4e09\u4e2a\u77e9\u9635\u7684\u4e58\u79ef\uff1a \\[ \\textbf{X}_{N \\times p} = \\textbf{U}_{N \\times N} \\mathbf{\\Sigma}_{N \\times p} \\textbf{V}^T_{p \\times p} \\] \u5176\u4e2d\uff1a \\(\\textbf{U}_{N \\times N}\\) \u662f\u4e00\u4e2a\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635\uff0c\u5728 \\(\\mathbb{R}^{N \\times N}\\) \u7a7a\u95f4\u3002\u5b83\u4ee3\u8868\u4e86\u65cb\u8f6c(rotation) \\(\\mathbf{\\Sigma}_{N \\times p}\\) \u662f\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635\uff0c\u4f46\u662f\u4e0d\u4e00\u5b9a\u662f\u65b9\u9635\u3002\u5b83\u4ee3\u8868\u62c9\u4f38(scaling) \\(\\textbf{V}^T_{p \\times p}\\) \u662f\u4e00\u4e2a\u5355\u4f4d\u6b63\u4ea4\u77e9\u9635\uff0c\u5728 \\(\\mathbb{R}^{p \\times p}\\) \u7a7a\u95f4\u3002\u5b83\u4ee3\u8868\u65cb\u8f6c(rotation) \u5bf9\u4e8e\u666e\u901a\u7684\u7ebf\u6027\u56de\u5f52\uff0c\u6709\uff1a \\[\\begin{align} \\hat{y} = \\textbf{H}y &= \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^Ty \\\\ &= \\textbf{U}\\mathbf{\\Sigma}\\textbf{V}^T(\\textbf{V}\\mathbf{\\Sigma}^T\\mathbf{\\Sigma}\\textbf{V}^T)^{-1} \\textbf{V}\\mathbf{\\Sigma}^T\\textbf{U}^T y \\\\ &= \\textbf{U}\\mathbf{\\Sigma} (\\mathbf{\\Sigma}^T\\mathbf{\\Sigma})^{-1} \\mathbf{\\Sigma}^T\\textbf{U}^T y \\\\ &= \\textbf{U}\\textbf{U}^T y \\end{align}\\] \u800c\u5bf9\u4e8e ridge regression\uff0c\u6709\uff1a \\[\\begin{align} \\hat{y} &= \\textbf{X}(\\textbf{X}^T\\textbf{X} + \\lambda \\textbf{I})^{-1} \\textbf{X}^T \\textbf{y} \\\\ &= \\textbf{U}\\mathbf{\\Sigma}(\\mathbf{\\Sigma}^T\\mathbf{\\Sigma} + \\lambda \\textbf{I})^{-1} \\mathbf{\\Sigma}^T\\textbf{U}^T y \\end{align}\\] \u5047\u8bbe SVD \u5206\u89e3\u7684\u5947\u5f02\u503c\u4e3a \\(\\sigma_1, \\sigma_2, ... , \\sigma_p\\) \uff0c\u6211\u4eec\u6709\uff1a \\[\\begin{align} \\hat{y} &= \\textbf{U}\\mathbf{\\Sigma}(\\mathbf{\\Sigma}^T\\mathbf{\\Sigma} + \\lambda \\textbf{I})^{-1} \\mathbf{\\Sigma}^T\\textbf{U}^T y \\\\ &= \\sum_{j=1}^p \\textbf{u}_j \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda} \\textbf{u}_j^T \\textbf{y} \\end{align}\\] \u5176\u4e2d \\(\\textbf{u}_j\\) \u8868\u793a\u77e9\u9635 \\(\\textbf{U}\\) \u7684\u7b2c \\(j\\) \u5217\u3002 \u56e0\u6b64\uff0c\u4ece\u76f4\u89c2\u610f\u4e49\u4e0a\u7406\u89e3\uff0cridge regression \u76f8\u6bd4\u666e\u901a\u7684 regression \u5c31\u662f\u5bf9 \\(\\textbf{U}\\) \u7684\u6bcf\u4e00\u5217\u9644\u52a0\u4e86\u4e00\u4e2a\u7cfb\u6570 \\(\\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda} \\leq 1\\) \u3002\u8fd9\u4e2a\u7cfb\u6570\u4e0e\u8be5\u5217\u5bf9\u5e94\u7684\u5947\u5f02\u503c\u76f8\u5173\u3002\u800c\u6211\u4eec\u5728 SVD \u5b9a\u4e49\u4e2d\u77e5\u9053 \\(\\sigma_j\\) \u4ee3\u8868\u4e86\u5728 \\(\\textbf{u}_j\\) \u65b9\u5411\u7684\u7f29\u653e\u7cfb\u6570\u3002\u663e\u7136\uff0c \\(\\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda}\\) \u5728 \\(\\sigma_j\\) \u8d8a\u5c0f\u65f6\uff0cshrinkage \u8d8a\u5927\u3002\u56e0\u6b64\uff0c\u76f4\u89c2\u7406\u89e3\uff0cridge regression \u4f1a\u503e\u5411\u4e8e\u5ffd\u7565\u8f93\u5165 \\(\\textbf{X}\\) \u65b9\u5dee\u8f83\u5c0f\u7684\u65b9\u5411\u3002 the small singular values correspond to directions in the column space of X having small variance, and ridge regression shrinks these directions the most. \u8fd9\u662f\u4e2a\u6bd4\u8f83\u5408\u7406\u7684\u5047\u8bbe\uff0c\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5bf9\u4e8e\u6837\u672c\u4e2d\u51e0\u4e4e\u4e00\u6837\u7684\u8f93\u5165\u53c2\u6570\u5e76\u4e0d\u662f\u5f88\u5173\u5fc3.","title":"Ridge Regression and SVD"},{"location":"mathematics/ESL/ESL3_LinearMethodsForRegression/#reference","text":"ESL solution ESL Chinese Simple Linear Regression Proofs involving ordinary least squares","title":"Reference"},{"location":"mathematics/matrix/MatrixCalculus/","text":"\u5411\u91cf\u548c\u77e9\u9635\u6c42\u5bfc \u5411\u91cf\u3001\u77e9\u9635\u6c42\u5bfc\u5176\u5b9e\u5c31\u4e24\u4e2a\u5185\u5bb9 \u5206\u5b50\u6bcf\u4e2a\u5143\u7d20\u5bf9\u5206\u6bcd\u6bcf\u4e2a\u5143\u7d20\u6c42\u5bfc \u5c06\u7ed3\u679c\u4ee5\u4e00\u5b9a\u65b9\u5f0f\u5e03\u5c40 \u5bf9\u4e8e 1\uff0c\u6ca1\u4ec0\u4e48\u7279\u522b\u7684\uff0c\u5c31\u662f\u6807\u91cf\u4e4b\u95f4\u7684\u6c42\u5bfc\u3002 \u5bf9\u4e8e 2\uff0c\u6211\u4eec\u9700\u8981\u5206\u60c5\u51b5\u8ba8\u8bba\u3002 \u6c42\u5bfc\u5e03\u5c40 \u6c42\u5bfc\u7ed3\u679c\u7684\u5e03\u5c40\u6839\u636e\u5b9a\u4e49\u4e0d\u540c\u6709\u6240\u4e0d\u540c\uff0c\u6ca1\u6709\u7edf\u4e00\u3002\u6240\u4ee5\u7ecf\u5e38\u5728\u4e0d\u540c\u7684\u4e66\u4e0a\u770b\u5230\u4e0d\u4e00\u6837\u7684\u516c\u5f0f\uff0c\u4f7f\u4eba\u4ea7\u751f\u56f0\u60d1\u3002 \u5e38\u89c1\u7684\u6c42\u5bfc\u7c7b\u578b\u5982\u4e0b\uff1a \u5206\u6bcd \\ \u5206\u5b50 \u6807\u91cf \u5411\u91cf \u77e9\u9635 \u6807\u91cf \\(\\frac{\\partial y}{ \\partial x}\\) \\(\\frac{ \\partial \\textbf{y} }{ \\partial x }\\) \\(\\frac{\\partial \\textbf{Y}}{\\partial x}\\) \u5411\u91cf \\(\\frac{\\partial y}{ \\partial \\textbf{x}}\\) \\(\\frac{\\partial \\textbf{y} }{ \\partial \\textbf{x}}\\) / \u77e9\u9635 \\(\\frac{ \\partial y }{ \\partial \\textbf{X} }\\) / / \u6211\u4eec\u5212\u6389\u7684\u7c7b\u578b\u662f\u56e0\u4e3a\u5176\u7ed3\u679c\u65e0\u6cd5\u5728\u4e8c\u7ef4\u77e9\u9635\u4e2d\u5f88\u597d\u5730\u8868\u793a\uff0c\u5728\u4f18\u5316\u95ee\u9898\u4e2d\u4e5f\u4e0d\u5e38\u89c1\u3002 \u672a\u5212\u6389\u7684\u7c7b\u578b\u4e2d\uff0c\u552f\u4e00\u5e03\u5c40\u6709\u6b67\u4e49\u7684\u5c31\u662f\u5411\u91cf\u5bf9\u5411\u91cf\u7684\u6c42\u5bfc\uff1a \\(\\frac{ \\partial \\textbf{y} }{ \\partial \\textbf{x} }\\) \u5411\u91cf\u5bf9\u5411\u91cf\u6c42\u5bfc \u6b67\u4e49\u5728\u4e8e\uff0c\u5047\u8bbe \\(\\textbf{y}\\) \u662f\u4e00\u4e2a \\(m\\) \u7ef4\u5411\u91cf\uff0c \\(\\textbf{x}\\) \u662f\u4e00\u4e2a \\(n\\) \u7ef4\u5411\u91cf\uff0c\u90a3\u6c42\u5bfc\u7ed3\u679c\u662f\u4e00\u4e2a \\(m \\times n\\) \u77e9\u9635\u8fd8\u662f \\(n \\times m\\) \u77e9\u9635\u5462\uff1f \u5206\u5b50\u5e03\u5c40\uff0c\u5373\u4ee5\u5206\u5b50 \\(\\textbf{y}\\) \u7684\u5143\u7d20\u6570\u4f5c\u4e3a\u884c\u6570\u3002\u7ed3\u679c\u662f\u4e00\u4e2a \\(m \\times n\\) \u77e9\u9635\uff0c\u4e5f\u79f0\u4e3a\u96c5\u53ef\u6bd4\uff08Jacobian\uff09\u77e9\u9635\u3002 \\[ \\frac{ \\partial \\textbf{ y } }{ \\partial \\textbf{ x } } = \\begin{bmatrix} \\frac{ \\partial {y_1} }{ \\partial {x_1} } & \\frac{\\partial {y_1} }{\\partial {x_2} } & \\cdots &\\frac{\\partial {y_1} }{\\partial {x_n} } \\\\ \\frac{\\partial {y_2} }{\\partial {x_1} } & \\frac{\\partial {y_2} }{\\partial {x_2} } & \\cdots &\\frac{\\partial {y_2} }{\\partial {x_n} } \\\\ \\vdots & \\vdots & & \\vdots \\\\ \\frac{\\partial {y_m} }{\\partial {x_1} } & \\frac{\\partial {y_m} }{\\partial {x_2} } & \\cdots &\\frac{\\partial {y_m} }{\\partial {x_n} } \\\\ \\end{bmatrix}_{m \\times n} \\] \u5206\u6bcd\u5e03\u5c40\uff0c\u5373\u4ee5\u5206\u6bcd \\(\\textbf{x}\\) \u7684\u5143\u7d20\u6570\u4f5c\u4e3a\u884c\u6570\u3002\u7ed3\u679c\u662f\u4e00\u4e2a \\(n \\times m\\) \u77e9\u9635\uff0c\u4e5f\u79f0\u4e3a\u68af\u5ea6(Gradient)\u77e9\u9635\u3002 \\[ \\frac{\\partial \\textbf{ y }}{\\partial \\textbf{ x } } = \\begin{bmatrix} \\frac{\\partial {y_1} }{\\partial {x_1} } & \\frac{\\partial {y_2} }{\\partial {x_1} } & \\cdots &\\frac{\\partial {y_m} }{\\partial {x_1} } \\\\ \\frac{\\partial {y_1} }{\\partial {x_2} } & \\frac{\\partial {y_2} }{\\partial {x_2} } & \\cdots &\\frac{\\partial {y_m } }{\\partial {x_2} } \\\\ \\vdots & \\vdots & & \\vdots \\\\ \\frac{\\partial {y_1} }{\\partial {x_n} } & \\frac{\\partial {y_2} }{\\partial {x_n} } & \\cdots &\\frac{\\partial {y_m} }{\\partial {x_n} } \\\\ \\end{bmatrix}_{n \\times m} \\] \u4e24\u79cd\u5e03\u5c40\u5747\u53ef\uff0c\u5728\u4e00\u672c\u4e66\u4e2d\u4e00\u822c\u662f\u4e00\u81f4\u7684\u3002 \u6807\u91cf\u5bf9\u5411\u91cf\u6c42\u5bfc \u6807\u91cf\u5e38\u89c1\u7684\u6709\u4ee5\u4e0b\u51e0\u79cd\u5f62\u5f0f\uff1a \\(a^T x\\) \\(x^T a\\) \\(x^T A x\\) \u4ece\u5b9a\u4e49\u4e0a\u770b\uff0c1 \u548c 2 \u7c7b\u4f3c\uff1a \u9996\u5148\u5b9a\u4e49\uff1a \\[S = a^T x = x^T a = \\sum_{i=1}^n a_ix_i\\] \u5f97\u51fa\uff1a \\[ \\frac{\\partial S}{\\partial x_i} = a_i\\] \u56e0\u6b64\uff1a \\[\\frac{\\partial a^Tx}{\\partial x} = \\frac{\\partial x^Ta}{\\partial x} = [ \\frac{\\partial S}{\\partial x_1}, \\frac{\\partial S}{\\partial x_2}, \\cdots, \\frac{\\partial S}{\\partial x_n}]^T = a\\] 3 \u7a0d\u5fae\u590d\u6742\uff1a \\[ S = \\sum_{i=1}^n \\sum_{j=1}^n x_iA_{i,j}x_j\\] \\[ \\frac{\\partial S}{\\partial x_k} = \\sum_{j=1}^n A_{k,j}x_j + \\sum_{i=1}^n x_iA_{i,k} = (A_{k,i} + A_{i,k})x_i\\] \u5373\u6c42\u5bfc\u540e\u5411\u91cf\u7684\u7b2c k \u4e2a\u5143\u7d20\u662f A \u7684\u7b2c k \u884c\u4e0e x \u7684\u5185\u79ef + \u7b2c k \u5217\u4e0e x \u7684\u5185\u79ef\u3002\u8fd9\u5176\u5b9e\u5c31\u662f\u77e9\u9635\u4e0e\u5411\u91cf\u4e58\u6cd5\u7684\u5b9a\u4e49\u3002 \\[\\frac{\\partial x^TAx}{\\partial x} = [ \\frac{\\partial S}{\\partial x_1}, \\frac{\\partial S}{\\partial x_2}, \\cdots, \\frac{\\partial S}{\\partial x_n}]^T = Ax + A^Tx \\] \u4f8b\uff1a\u6700\u5c0f\u4e8c\u4e58\u6cd5 \u6700\u5c0f\u4e8c\u4e58\u6cd5\u662f\u6700\u6d41\u884c\u7684\u7ebf\u6027\u6a21\u578b\u62df\u5408\u65b9\u6cd5\u3002\u5b83\u7684\u76ee\u7684\u662f\u627e\u51fa\u7cfb\u6570 \\(\\mathbf{\\beta}\\) \u4f7f \\(||Y-\\hat Y||_2\\) \uff08residual sum of squares, RSS\uff09\u6700\u5c0f\uff1a \\[\\text{RSS}(\\mathbf{\\beta} ) = \\sum_{j=1}^N (y_j - X_j^T\\mathbf{\\beta} )^2 \\] \u5176\u4e2d \\(j\\) \u4ee3\u8868\u8bad\u7ec3\u6570\u636e\u7684\u5e8f\u53f7\u3002\u4e00\u5171\u6709 \\(N\\) \u7ec4\u8bad\u7ec3\u6570\u636e\u3002 \u7528\u77e9\u9635\u5f62\u5f0f\u8868\u793a\u4e3a\uff1a \\[\\text{RSS}(\\mathbf{\\beta}) = (\\textbf{y} - \\textbf{X}\\mathbf{\\beta} )^T(\\textbf{y} - \\textbf{X}\\mathbf{\\beta} )\\] \u8fd9\u91cc\u9700\u8981\u7528 \\(\\text{RSS}(\\mathbf{\\beta})\\) \u5bf9 \\(\\mathbf{\\beta}\\) \u6c42\u5bfc\uff0c\u5f97\u51fa\u4e8c\u6b21\u51fd\u6570\u6700\u503c\u70b9\u3002 \\[\\text{RSS}(\\mathbf{\\beta}) = \\textbf{y}^T\\textbf{y} -\\textbf{y}^T \\textbf{X} \\mathbf{\\beta} - \\mathbf{\\beta}^T \\textbf{X}^T \\textbf{y} + \\mathbf{\\beta}^T \\textbf{X}^T \\textbf{X}\\mathbf{\\beta}\\] \u5957\u7528\u4e0a\u9762\u7684\u7ed3\u8bba\uff0c\u53ef\u4ee5\u5f97\u5230\uff1a \\[ \\frac{ \\partial \\text{RSS}(\\mathbf{\\beta})}{\\partial \\mathbf{\\beta}} = - 2\\textbf{X}^T\\textbf{y} + 2\\textbf{X}^T\\textbf{X}\\mathbf{\\beta}\\] \u4ee4\u5176\u4e3a 0 \u53ef\u4ee5\u89e3\u51fa\uff1a \\[\\hat{\\mathbf{\\beta}} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}\\]","title":"\u5411\u91cf\u548c\u77e9\u9635\u6c42\u5bfc"},{"location":"mathematics/matrix/MatrixCalculus/#_1","text":"\u5411\u91cf\u3001\u77e9\u9635\u6c42\u5bfc\u5176\u5b9e\u5c31\u4e24\u4e2a\u5185\u5bb9 \u5206\u5b50\u6bcf\u4e2a\u5143\u7d20\u5bf9\u5206\u6bcd\u6bcf\u4e2a\u5143\u7d20\u6c42\u5bfc \u5c06\u7ed3\u679c\u4ee5\u4e00\u5b9a\u65b9\u5f0f\u5e03\u5c40 \u5bf9\u4e8e 1\uff0c\u6ca1\u4ec0\u4e48\u7279\u522b\u7684\uff0c\u5c31\u662f\u6807\u91cf\u4e4b\u95f4\u7684\u6c42\u5bfc\u3002 \u5bf9\u4e8e 2\uff0c\u6211\u4eec\u9700\u8981\u5206\u60c5\u51b5\u8ba8\u8bba\u3002","title":"\u5411\u91cf\u548c\u77e9\u9635\u6c42\u5bfc"},{"location":"mathematics/matrix/MatrixCalculus/#_2","text":"\u6c42\u5bfc\u7ed3\u679c\u7684\u5e03\u5c40\u6839\u636e\u5b9a\u4e49\u4e0d\u540c\u6709\u6240\u4e0d\u540c\uff0c\u6ca1\u6709\u7edf\u4e00\u3002\u6240\u4ee5\u7ecf\u5e38\u5728\u4e0d\u540c\u7684\u4e66\u4e0a\u770b\u5230\u4e0d\u4e00\u6837\u7684\u516c\u5f0f\uff0c\u4f7f\u4eba\u4ea7\u751f\u56f0\u60d1\u3002 \u5e38\u89c1\u7684\u6c42\u5bfc\u7c7b\u578b\u5982\u4e0b\uff1a \u5206\u6bcd \\ \u5206\u5b50 \u6807\u91cf \u5411\u91cf \u77e9\u9635 \u6807\u91cf \\(\\frac{\\partial y}{ \\partial x}\\) \\(\\frac{ \\partial \\textbf{y} }{ \\partial x }\\) \\(\\frac{\\partial \\textbf{Y}}{\\partial x}\\) \u5411\u91cf \\(\\frac{\\partial y}{ \\partial \\textbf{x}}\\) \\(\\frac{\\partial \\textbf{y} }{ \\partial \\textbf{x}}\\) / \u77e9\u9635 \\(\\frac{ \\partial y }{ \\partial \\textbf{X} }\\) / / \u6211\u4eec\u5212\u6389\u7684\u7c7b\u578b\u662f\u56e0\u4e3a\u5176\u7ed3\u679c\u65e0\u6cd5\u5728\u4e8c\u7ef4\u77e9\u9635\u4e2d\u5f88\u597d\u5730\u8868\u793a\uff0c\u5728\u4f18\u5316\u95ee\u9898\u4e2d\u4e5f\u4e0d\u5e38\u89c1\u3002 \u672a\u5212\u6389\u7684\u7c7b\u578b\u4e2d\uff0c\u552f\u4e00\u5e03\u5c40\u6709\u6b67\u4e49\u7684\u5c31\u662f\u5411\u91cf\u5bf9\u5411\u91cf\u7684\u6c42\u5bfc\uff1a \\(\\frac{ \\partial \\textbf{y} }{ \\partial \\textbf{x} }\\)","title":"\u6c42\u5bfc\u5e03\u5c40"},{"location":"mathematics/matrix/MatrixCalculus/#_3","text":"\u6b67\u4e49\u5728\u4e8e\uff0c\u5047\u8bbe \\(\\textbf{y}\\) \u662f\u4e00\u4e2a \\(m\\) \u7ef4\u5411\u91cf\uff0c \\(\\textbf{x}\\) \u662f\u4e00\u4e2a \\(n\\) \u7ef4\u5411\u91cf\uff0c\u90a3\u6c42\u5bfc\u7ed3\u679c\u662f\u4e00\u4e2a \\(m \\times n\\) \u77e9\u9635\u8fd8\u662f \\(n \\times m\\) \u77e9\u9635\u5462\uff1f \u5206\u5b50\u5e03\u5c40\uff0c\u5373\u4ee5\u5206\u5b50 \\(\\textbf{y}\\) \u7684\u5143\u7d20\u6570\u4f5c\u4e3a\u884c\u6570\u3002\u7ed3\u679c\u662f\u4e00\u4e2a \\(m \\times n\\) \u77e9\u9635\uff0c\u4e5f\u79f0\u4e3a\u96c5\u53ef\u6bd4\uff08Jacobian\uff09\u77e9\u9635\u3002 \\[ \\frac{ \\partial \\textbf{ y } }{ \\partial \\textbf{ x } } = \\begin{bmatrix} \\frac{ \\partial {y_1} }{ \\partial {x_1} } & \\frac{\\partial {y_1} }{\\partial {x_2} } & \\cdots &\\frac{\\partial {y_1} }{\\partial {x_n} } \\\\ \\frac{\\partial {y_2} }{\\partial {x_1} } & \\frac{\\partial {y_2} }{\\partial {x_2} } & \\cdots &\\frac{\\partial {y_2} }{\\partial {x_n} } \\\\ \\vdots & \\vdots & & \\vdots \\\\ \\frac{\\partial {y_m} }{\\partial {x_1} } & \\frac{\\partial {y_m} }{\\partial {x_2} } & \\cdots &\\frac{\\partial {y_m} }{\\partial {x_n} } \\\\ \\end{bmatrix}_{m \\times n} \\] \u5206\u6bcd\u5e03\u5c40\uff0c\u5373\u4ee5\u5206\u6bcd \\(\\textbf{x}\\) \u7684\u5143\u7d20\u6570\u4f5c\u4e3a\u884c\u6570\u3002\u7ed3\u679c\u662f\u4e00\u4e2a \\(n \\times m\\) \u77e9\u9635\uff0c\u4e5f\u79f0\u4e3a\u68af\u5ea6(Gradient)\u77e9\u9635\u3002 \\[ \\frac{\\partial \\textbf{ y }}{\\partial \\textbf{ x } } = \\begin{bmatrix} \\frac{\\partial {y_1} }{\\partial {x_1} } & \\frac{\\partial {y_2} }{\\partial {x_1} } & \\cdots &\\frac{\\partial {y_m} }{\\partial {x_1} } \\\\ \\frac{\\partial {y_1} }{\\partial {x_2} } & \\frac{\\partial {y_2} }{\\partial {x_2} } & \\cdots &\\frac{\\partial {y_m } }{\\partial {x_2} } \\\\ \\vdots & \\vdots & & \\vdots \\\\ \\frac{\\partial {y_1} }{\\partial {x_n} } & \\frac{\\partial {y_2} }{\\partial {x_n} } & \\cdots &\\frac{\\partial {y_m} }{\\partial {x_n} } \\\\ \\end{bmatrix}_{n \\times m} \\] \u4e24\u79cd\u5e03\u5c40\u5747\u53ef\uff0c\u5728\u4e00\u672c\u4e66\u4e2d\u4e00\u822c\u662f\u4e00\u81f4\u7684\u3002","title":"\u5411\u91cf\u5bf9\u5411\u91cf\u6c42\u5bfc"},{"location":"mathematics/matrix/MatrixCalculus/#_4","text":"\u6807\u91cf\u5e38\u89c1\u7684\u6709\u4ee5\u4e0b\u51e0\u79cd\u5f62\u5f0f\uff1a \\(a^T x\\) \\(x^T a\\) \\(x^T A x\\) \u4ece\u5b9a\u4e49\u4e0a\u770b\uff0c1 \u548c 2 \u7c7b\u4f3c\uff1a \u9996\u5148\u5b9a\u4e49\uff1a \\[S = a^T x = x^T a = \\sum_{i=1}^n a_ix_i\\] \u5f97\u51fa\uff1a \\[ \\frac{\\partial S}{\\partial x_i} = a_i\\] \u56e0\u6b64\uff1a \\[\\frac{\\partial a^Tx}{\\partial x} = \\frac{\\partial x^Ta}{\\partial x} = [ \\frac{\\partial S}{\\partial x_1}, \\frac{\\partial S}{\\partial x_2}, \\cdots, \\frac{\\partial S}{\\partial x_n}]^T = a\\] 3 \u7a0d\u5fae\u590d\u6742\uff1a \\[ S = \\sum_{i=1}^n \\sum_{j=1}^n x_iA_{i,j}x_j\\] \\[ \\frac{\\partial S}{\\partial x_k} = \\sum_{j=1}^n A_{k,j}x_j + \\sum_{i=1}^n x_iA_{i,k} = (A_{k,i} + A_{i,k})x_i\\] \u5373\u6c42\u5bfc\u540e\u5411\u91cf\u7684\u7b2c k \u4e2a\u5143\u7d20\u662f A \u7684\u7b2c k \u884c\u4e0e x \u7684\u5185\u79ef + \u7b2c k \u5217\u4e0e x \u7684\u5185\u79ef\u3002\u8fd9\u5176\u5b9e\u5c31\u662f\u77e9\u9635\u4e0e\u5411\u91cf\u4e58\u6cd5\u7684\u5b9a\u4e49\u3002 \\[\\frac{\\partial x^TAx}{\\partial x} = [ \\frac{\\partial S}{\\partial x_1}, \\frac{\\partial S}{\\partial x_2}, \\cdots, \\frac{\\partial S}{\\partial x_n}]^T = Ax + A^Tx \\]","title":"\u6807\u91cf\u5bf9\u5411\u91cf\u6c42\u5bfc"},{"location":"mathematics/matrix/MatrixCalculus/#_5","text":"\u6700\u5c0f\u4e8c\u4e58\u6cd5\u662f\u6700\u6d41\u884c\u7684\u7ebf\u6027\u6a21\u578b\u62df\u5408\u65b9\u6cd5\u3002\u5b83\u7684\u76ee\u7684\u662f\u627e\u51fa\u7cfb\u6570 \\(\\mathbf{\\beta}\\) \u4f7f \\(||Y-\\hat Y||_2\\) \uff08residual sum of squares, RSS\uff09\u6700\u5c0f\uff1a \\[\\text{RSS}(\\mathbf{\\beta} ) = \\sum_{j=1}^N (y_j - X_j^T\\mathbf{\\beta} )^2 \\] \u5176\u4e2d \\(j\\) \u4ee3\u8868\u8bad\u7ec3\u6570\u636e\u7684\u5e8f\u53f7\u3002\u4e00\u5171\u6709 \\(N\\) \u7ec4\u8bad\u7ec3\u6570\u636e\u3002 \u7528\u77e9\u9635\u5f62\u5f0f\u8868\u793a\u4e3a\uff1a \\[\\text{RSS}(\\mathbf{\\beta}) = (\\textbf{y} - \\textbf{X}\\mathbf{\\beta} )^T(\\textbf{y} - \\textbf{X}\\mathbf{\\beta} )\\] \u8fd9\u91cc\u9700\u8981\u7528 \\(\\text{RSS}(\\mathbf{\\beta})\\) \u5bf9 \\(\\mathbf{\\beta}\\) \u6c42\u5bfc\uff0c\u5f97\u51fa\u4e8c\u6b21\u51fd\u6570\u6700\u503c\u70b9\u3002 \\[\\text{RSS}(\\mathbf{\\beta}) = \\textbf{y}^T\\textbf{y} -\\textbf{y}^T \\textbf{X} \\mathbf{\\beta} - \\mathbf{\\beta}^T \\textbf{X}^T \\textbf{y} + \\mathbf{\\beta}^T \\textbf{X}^T \\textbf{X}\\mathbf{\\beta}\\] \u5957\u7528\u4e0a\u9762\u7684\u7ed3\u8bba\uff0c\u53ef\u4ee5\u5f97\u5230\uff1a \\[ \\frac{ \\partial \\text{RSS}(\\mathbf{\\beta})}{\\partial \\mathbf{\\beta}} = - 2\\textbf{X}^T\\textbf{y} + 2\\textbf{X}^T\\textbf{X}\\mathbf{\\beta}\\] \u4ee4\u5176\u4e3a 0 \u53ef\u4ee5\u89e3\u51fa\uff1a \\[\\hat{\\mathbf{\\beta}} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}\\]","title":"\u4f8b\uff1a\u6700\u5c0f\u4e8c\u4e58\u6cd5"},{"location":"mathematics/probability/RigidBalls/","text":"\u5c0f\u7403\u78b0\u649e\u95ee\u9898\u4e2d\u7684\u7edf\u8ba1\u5b66 \u670b\u53cb\u7ed9\u6211\u53d1\u4e86\u4e00\u9053\u7b14\u8bd5\u9898\u8ba9\u6211\u5e2e\u5fd9\u505a\u3002\u9898\u76ee\u5927\u6982\u662f\u8fd9\u6837\uff1a (1) \u6709 1 \u4e2a\u5c0f\u7403\u653e\u5728\u4e00\u4e2a 1m \u957f\u7684\u5149\u6ed1\u51f9\u69fd\u91cc\uff0c\u5c0f\u7403\u521d\u59cb\u4f4d\u7f6e\u968f\u673a\uff08\u5747\u5300\u5206\u5e03\uff09\uff0c\u521d\u59cb\u901f\u5ea6 1m/s\uff0c\u65b9\u5411\u968f\u673a\u3002\u6c42\u5c0f\u7403\u6ed1\u51fa\u51f9\u69fd\u7684\u671f\u671b\u503c\u3002 (2) \u5982\u679c\u6709 2 \u4e2a\u5c0f\u7403\uff0c\u4e14\u5c0f\u7403\u78b0\u649e\u4e3a\u521a\u6027\u78b0\u649e\uff0c\u6c42\u4e24\u4e2a\u5c0f\u7403\u5168\u90e8\u4ece\u51f9\u69fd\u6389\u843d\u7684\u65f6\u95f4\u671f\u671b\u503c\u3002 (3) \u5982\u679c\u6709 n \u4e2a\u5c0f\u7403\u5462\uff1f \u8fd9\u9053\u9898\u5176\u5b9e\u7528\u76f4\u89c9\u505a\u5f88\u7b80\u5355\u3002\u4e3b\u8981\u6709\u4e24\u4e2a\u5173\u7a8d\uff1a 1. \u521a\u6027\u78b0\u649e\u540e\uff0c\u5c0f\u7403\u4ea4\u6362\u901f\u5ea6 \u52a8\u91cf\u5b88\u6052+\u80fd\u91cf\u5b88\u6052 2. n \u4e2a\u5c0f\u7403\u5728\u8f68\u9053\u4e0a\u5747\u5300\u5206\u5e03\u5927\u6982\u662f\u4ec0\u4e48\u6837\uff1f \u5176\u671f\u671b\u5c31\u662f\u5747\u5300\u628a\u8f68\u9053\u5206\u6210 n + 1 \u4efd\u3002 \u56e0\u6b64\u7b54\u6848\u662f\uff1a1/2\uff0c2/3\uff0c\u548c n/(n+1) \u5f53\u7136\uff0c\u8fd9\u5b8c\u5168\u4e0d\u591f\u4e25\u8c28\u3002\u4e0b\u9762\u8fdb\u5165\u6570\u5b66\u90e8\u5206\u3002 N \u4e2a\u72ec\u7acb\u540c\u5206\u5e03\u6700\u5927\u503c\u7684\u671f\u671b \u8fd9\u9053\u9898\u53ef\u4ee5\u63d0\u70bc\u51fa\u4e00\u4e2a\u6570\u5b66\u95ee\u9898\uff0c\u5373\u6c42 N \u4e2a\u72ec\u7acb\u540c\u5206\u5e03\u6700\u5927\u503c\u7684\u671f\u671b\u3002 \u5047\u8bbe \\(Y\\) \u662f N \u4e2a\u968f\u673a\u53d8\u91cf\u7684\u6700\u5927\u503c\uff0c\u5219 \\(Y\\) \u7684\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\uff08Cumulative Distribution Function\uff09\u4e3a\uff1a \\[G(y) = P(Y \\leq y) = P( \\max(X_1, X_2, ..., X_n) \\leq y)\\] \u7531\u4e8e X \u76f8\u4e92\u72ec\u7acb\uff0c\u4e0a\u5f0f\u7b49\u4ef7\u4e8e\uff1a \\[\\begin{align} G(y) &= P(X_1 \\leq y) P( X_2 \\leq y) P(X_n \\leq y) \\\\ &= F^n(y) \\end{align}\\] \u5176\u4e2d \\(F\\) \u8868\u793a \\(X\\) \u7684\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\u3002 \u90a3\u4e48 y \u7684\u671f\u671b\u503c\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a \\[\\begin{align} E(Y) &= \\int_a^b y G'(y) dy \\\\ &= yG(y)|_a^b - \\int_a^bG(y)dy \\\\ &= b - \\int_a^bF^n(y)dy \\end{align}\\] \u5747\u5300\u5206\u5e03\u60c5\u5f62 \u5bf9\u4e8e \\((a, b)\\) \u533a\u95f4\u4e0a\u7684\u5747\u5300\u5206\u5e03\u6709\uff1a \\[F(x) = \\frac{x-a}{b-a}\\] \u6545 \\[\\begin{align} E(Y) &= b - \\int_a^bF^n(y)dy \\\\ &= b - \\frac{1}{(b-a)^n} \\int_a^b(y-a)^n dy \\\\ &= b - \\frac{b-a}{n+1} \\\\ &= \\frac{a + nb}{n+1} \\end{align}\\] \u5e26\u5165\u9898\u76ee\u4e2d\u7684\u533a\u95f4 \\((0,1)\\) \u5f97\u5230\uff1a \\[E(Y) = \\frac{n}{n+1}\\]","title":"\u5c0f\u7403\u78b0\u649e\u95ee\u9898\u4e2d\u7684\u7edf\u8ba1\u5b66"},{"location":"mathematics/probability/RigidBalls/#_1","text":"\u670b\u53cb\u7ed9\u6211\u53d1\u4e86\u4e00\u9053\u7b14\u8bd5\u9898\u8ba9\u6211\u5e2e\u5fd9\u505a\u3002\u9898\u76ee\u5927\u6982\u662f\u8fd9\u6837\uff1a (1) \u6709 1 \u4e2a\u5c0f\u7403\u653e\u5728\u4e00\u4e2a 1m \u957f\u7684\u5149\u6ed1\u51f9\u69fd\u91cc\uff0c\u5c0f\u7403\u521d\u59cb\u4f4d\u7f6e\u968f\u673a\uff08\u5747\u5300\u5206\u5e03\uff09\uff0c\u521d\u59cb\u901f\u5ea6 1m/s\uff0c\u65b9\u5411\u968f\u673a\u3002\u6c42\u5c0f\u7403\u6ed1\u51fa\u51f9\u69fd\u7684\u671f\u671b\u503c\u3002 (2) \u5982\u679c\u6709 2 \u4e2a\u5c0f\u7403\uff0c\u4e14\u5c0f\u7403\u78b0\u649e\u4e3a\u521a\u6027\u78b0\u649e\uff0c\u6c42\u4e24\u4e2a\u5c0f\u7403\u5168\u90e8\u4ece\u51f9\u69fd\u6389\u843d\u7684\u65f6\u95f4\u671f\u671b\u503c\u3002 (3) \u5982\u679c\u6709 n \u4e2a\u5c0f\u7403\u5462\uff1f \u8fd9\u9053\u9898\u5176\u5b9e\u7528\u76f4\u89c9\u505a\u5f88\u7b80\u5355\u3002\u4e3b\u8981\u6709\u4e24\u4e2a\u5173\u7a8d\uff1a 1. \u521a\u6027\u78b0\u649e\u540e\uff0c\u5c0f\u7403\u4ea4\u6362\u901f\u5ea6 \u52a8\u91cf\u5b88\u6052+\u80fd\u91cf\u5b88\u6052 2. n \u4e2a\u5c0f\u7403\u5728\u8f68\u9053\u4e0a\u5747\u5300\u5206\u5e03\u5927\u6982\u662f\u4ec0\u4e48\u6837\uff1f \u5176\u671f\u671b\u5c31\u662f\u5747\u5300\u628a\u8f68\u9053\u5206\u6210 n + 1 \u4efd\u3002 \u56e0\u6b64\u7b54\u6848\u662f\uff1a1/2\uff0c2/3\uff0c\u548c n/(n+1) \u5f53\u7136\uff0c\u8fd9\u5b8c\u5168\u4e0d\u591f\u4e25\u8c28\u3002\u4e0b\u9762\u8fdb\u5165\u6570\u5b66\u90e8\u5206\u3002","title":"\u5c0f\u7403\u78b0\u649e\u95ee\u9898\u4e2d\u7684\u7edf\u8ba1\u5b66"},{"location":"mathematics/probability/RigidBalls/#n","text":"\u8fd9\u9053\u9898\u53ef\u4ee5\u63d0\u70bc\u51fa\u4e00\u4e2a\u6570\u5b66\u95ee\u9898\uff0c\u5373\u6c42 N \u4e2a\u72ec\u7acb\u540c\u5206\u5e03\u6700\u5927\u503c\u7684\u671f\u671b\u3002 \u5047\u8bbe \\(Y\\) \u662f N \u4e2a\u968f\u673a\u53d8\u91cf\u7684\u6700\u5927\u503c\uff0c\u5219 \\(Y\\) \u7684\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\uff08Cumulative Distribution Function\uff09\u4e3a\uff1a \\[G(y) = P(Y \\leq y) = P( \\max(X_1, X_2, ..., X_n) \\leq y)\\] \u7531\u4e8e X \u76f8\u4e92\u72ec\u7acb\uff0c\u4e0a\u5f0f\u7b49\u4ef7\u4e8e\uff1a \\[\\begin{align} G(y) &= P(X_1 \\leq y) P( X_2 \\leq y) P(X_n \\leq y) \\\\ &= F^n(y) \\end{align}\\] \u5176\u4e2d \\(F\\) \u8868\u793a \\(X\\) \u7684\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\u3002 \u90a3\u4e48 y \u7684\u671f\u671b\u503c\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a \\[\\begin{align} E(Y) &= \\int_a^b y G'(y) dy \\\\ &= yG(y)|_a^b - \\int_a^bG(y)dy \\\\ &= b - \\int_a^bF^n(y)dy \\end{align}\\]","title":"N \u4e2a\u72ec\u7acb\u540c\u5206\u5e03\u6700\u5927\u503c\u7684\u671f\u671b"},{"location":"mathematics/probability/RigidBalls/#_2","text":"\u5bf9\u4e8e \\((a, b)\\) \u533a\u95f4\u4e0a\u7684\u5747\u5300\u5206\u5e03\u6709\uff1a \\[F(x) = \\frac{x-a}{b-a}\\] \u6545 \\[\\begin{align} E(Y) &= b - \\int_a^bF^n(y)dy \\\\ &= b - \\frac{1}{(b-a)^n} \\int_a^b(y-a)^n dy \\\\ &= b - \\frac{b-a}{n+1} \\\\ &= \\frac{a + nb}{n+1} \\end{align}\\] \u5e26\u5165\u9898\u76ee\u4e2d\u7684\u533a\u95f4 \\((0,1)\\) \u5f97\u5230\uff1a \\[E(Y) = \\frac{n}{n+1}\\]","title":"\u5747\u5300\u5206\u5e03\u60c5\u5f62"},{"location":"mathematics/probability/martingale/","text":"Martingale Definition Process X is a martingale if for all n: \\[ E[X_{n+1}|F_n] = X_n \\] Where Fn is the history of Xn (called filtration) Which means, on the n+1 step, the expectation of X shall be the same as any step before. A martingale may be thought of as a \u201cfair game\u201d, because given the information in current and previous plays (Fn), you don't expect to change your total winning (X). Example Here are two practical example to help you understand it. Partial sum process A simple coin game, in \\(i\\) turn we bet \\(X_i\\) , and \\(S_n\\) is our winning money after n turn. Let Xi be independent, define the partial sum process: \\[\\begin{aligned} S_0 &= 0, \\\\ S_n &= \\sum_{ i=1 }^n X_i, n=1, 2,... \\end{aligned}\\] Sn is a martingale iff: \\[E(X_i) = 0\\] Proof With Xi being independent, and E(Xi) = 0, we have: \\[\\begin{aligned} E[S_{n+1}|X_1,...X_n] &= E[S_n + X_{n+1} | X_1,...X_n] \\\\ &= S_n + E[X_{n+1} | X_1,...X_n] \\\\ &= S_n + E[X_{n+1}] \\\\ &= S_n \\end{aligned}\\] Gambler's Ruin Problem A classic problem on martingale. Consider a gambler who starts with an initial fortune of $1 and then on each successive gamble either wins $1 or loses $1 independent of the past with probabilities p and q = 1\u2212p respectively. Let Rn denote the total fortune after the n th gamble. The gambler\u2019s objective is to reach a total fortune of $N, without first getting ruined (running out of money). If the gambler succeeds, then the gambler is said to win the game. Let \\(P_i\\) denote the probability that the gambler wins when the initial money \\(R_0 = i\\) , we have: \\[P_i = pP_{i+1} + qP_{i-1}\\] This is because P_i can only lead to two states: Winning $1 with probability p to state \\(P_{i+1}\\) Losing $1 with probability q to state \\(P_{i-1}\\) Subtract P_{i-1} from both sides of the equation, we get: \\[P_i = (p + q) P_i = pP_{i+1} + qP_{i-1}\\] i.e., \\[{P_{i+1} - P_i}= \\frac{ q }{ p }(P_i - P_{i-1})\\] Thus \\[\\begin{aligned} P_{i+1} - P_1 &= \\sum_{k=1}^i (P_{k+1} - P_k) \\\\ &= \\sum_{k=1}^i (\\frac{q}{p})^k P_1 \\end{aligned}\\] We have \\[P_{i+1}= \\begin{cases} P_1 \\frac{1-(q/p)^{i+1}}{1-(q/p)} &, p \\neq q\\\\ P_1 (i+1) &, p=q \\end{cases}\\] To solve P_1, pick i = N-1 and use the fact that P_N = 1 \\[P_1= \\begin{cases} \\frac{1-(q/p)}{1-(q/p)^N} &, p \\neq q\\\\ 1/N &, p=q \\end{cases}\\] Substitute P_1 and we have: \\[P_i= \\begin{cases} \\frac{ 1-(q/p)^i}{ 1-(q/p)^N } &, p \\neq q \\\\ i/N &, p=q \\end{cases}\\] Reference martingales.dvi (rice.edu) 4700-07-Notes-GR.pdf (columbia.edu)","title":"Martingale"},{"location":"mathematics/probability/martingale/#martingale","text":"","title":"Martingale"},{"location":"mathematics/probability/martingale/#definition","text":"Process X is a martingale if for all n: \\[ E[X_{n+1}|F_n] = X_n \\] Where Fn is the history of Xn (called filtration) Which means, on the n+1 step, the expectation of X shall be the same as any step before. A martingale may be thought of as a \u201cfair game\u201d, because given the information in current and previous plays (Fn), you don't expect to change your total winning (X).","title":"Definition"},{"location":"mathematics/probability/martingale/#example","text":"Here are two practical example to help you understand it.","title":"Example"},{"location":"mathematics/probability/martingale/#partial-sum-process","text":"A simple coin game, in \\(i\\) turn we bet \\(X_i\\) , and \\(S_n\\) is our winning money after n turn. Let Xi be independent, define the partial sum process: \\[\\begin{aligned} S_0 &= 0, \\\\ S_n &= \\sum_{ i=1 }^n X_i, n=1, 2,... \\end{aligned}\\] Sn is a martingale iff: \\[E(X_i) = 0\\]","title":"Partial sum process"},{"location":"mathematics/probability/martingale/#proof","text":"With Xi being independent, and E(Xi) = 0, we have: \\[\\begin{aligned} E[S_{n+1}|X_1,...X_n] &= E[S_n + X_{n+1} | X_1,...X_n] \\\\ &= S_n + E[X_{n+1} | X_1,...X_n] \\\\ &= S_n + E[X_{n+1}] \\\\ &= S_n \\end{aligned}\\]","title":"Proof"},{"location":"mathematics/probability/martingale/#gamblers-ruin-problem","text":"A classic problem on martingale. Consider a gambler who starts with an initial fortune of $1 and then on each successive gamble either wins $1 or loses $1 independent of the past with probabilities p and q = 1\u2212p respectively. Let Rn denote the total fortune after the n th gamble. The gambler\u2019s objective is to reach a total fortune of $N, without first getting ruined (running out of money). If the gambler succeeds, then the gambler is said to win the game. Let \\(P_i\\) denote the probability that the gambler wins when the initial money \\(R_0 = i\\) , we have: \\[P_i = pP_{i+1} + qP_{i-1}\\] This is because P_i can only lead to two states: Winning $1 with probability p to state \\(P_{i+1}\\) Losing $1 with probability q to state \\(P_{i-1}\\) Subtract P_{i-1} from both sides of the equation, we get: \\[P_i = (p + q) P_i = pP_{i+1} + qP_{i-1}\\] i.e., \\[{P_{i+1} - P_i}= \\frac{ q }{ p }(P_i - P_{i-1})\\] Thus \\[\\begin{aligned} P_{i+1} - P_1 &= \\sum_{k=1}^i (P_{k+1} - P_k) \\\\ &= \\sum_{k=1}^i (\\frac{q}{p})^k P_1 \\end{aligned}\\] We have \\[P_{i+1}= \\begin{cases} P_1 \\frac{1-(q/p)^{i+1}}{1-(q/p)} &, p \\neq q\\\\ P_1 (i+1) &, p=q \\end{cases}\\] To solve P_1, pick i = N-1 and use the fact that P_N = 1 \\[P_1= \\begin{cases} \\frac{1-(q/p)}{1-(q/p)^N} &, p \\neq q\\\\ 1/N &, p=q \\end{cases}\\] Substitute P_1 and we have: \\[P_i= \\begin{cases} \\frac{ 1-(q/p)^i}{ 1-(q/p)^N } &, p \\neq q \\\\ i/N &, p=q \\end{cases}\\]","title":"Gambler's Ruin Problem"},{"location":"mathematics/probability/martingale/#reference","text":"martingales.dvi (rice.edu) 4700-07-Notes-GR.pdf (columbia.edu)","title":"Reference"}]}