<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>ESL3 LinearMethodsForRegression - My Docs</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css">

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../..">My Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href="../.." class="nav-link">Ye's Notes</a>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Mathematics <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="./" class="dropdown-item active">ESL3 LinearMethodsForRegression</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../.." class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" class="nav-link disabled">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="2"><a href="#32-linear-regression-models-and-least-squares" class="nav-link">3.2 Linear Regression Models and Least Squares</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#33-subset-selection" class="nav-link">3.3 Subset Selection</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="2"><a href="#34-shrinkage-methods" class="nav-link">3.4 Shrinkage Methods</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#reference" class="nav-link">Reference</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p>一个线性回归模型假设回归函数 E(Y|X) 对于输入 X 是线性的。
它的优势在于：
- 简单
- 能够表示每个输入对输出的影响
- 输入可以进行变换
- 他们有时候比复杂的方法更精准，尤其是在样本数量少、低信噪比或者稀疏矩阵的情形。</p>
<h2 id="32-linear-regression-models-and-least-squares">3.2 Linear Regression Models and Least Squares</h2>
<p>$p$ 维线性回归模型形式如下：</p>
<p>$$f(X) = \beta_0 + \sum_{j=1}^p X_j \beta_j$$</p>
<p>我们需要估计一组参数 $\beta$，使残差平方和（Residual Sum of Squares）最小：</p>
<p>$$\begin{align}
\text{RSS}(\boldsymbol{\beta}) &amp;= (\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )^T(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} ) \
&amp;= \boldsymbol{y}^T\boldsymbol{y} - \boldsymbol{y}^T\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{y} + \boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}
\end{align}$$</p>
<p>其中，$ \boldsymbol{X} $ 是一个 $N \times (p+1)$ 矩阵，$\boldsymbol{y}$ 是 $N \times 1$ 观测值。</p>
<p>对 $\beta$ 求导可以得到：</p>
<p>$$ \frac{\partial \text{RSS}(\beta)}{\partial \beta} = -2 \boldsymbol{X}^T\boldsymbol{y} + 2\boldsymbol{X}^T\boldsymbol{X} \boldsymbol{\beta}$$</p>
<p>由于二阶导数正定，令一阶导数为 0 向量，得出极值点（即估计值）：</p>
<p>$$ \hat{\beta}= (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}$$</p>
<p>$$\hat{\boldsymbol{y}} = \boldsymbol{X} \hat{\beta} = \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}$$</p>
<p>我们称 $\boldsymbol{H} = \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T$ 为估计矩阵（"hat" matrix），它满足对称性和幂等性：</p>
<p>$$\boldsymbol{H}^T = \boldsymbol{H}$$</p>
<p>$$\boldsymbol{H}^T\boldsymbol{H} = \boldsymbol{H}$$</p>
<p>当 $\boldsymbol{X}$ 中某些列线性相关（即非满秩矩阵）时，$(\boldsymbol{X}^T\boldsymbol{X})$ 是奇异矩阵，它只能求广义逆矩阵，不止一个解。因此，我们需要将冗余的输入剔除掉，大部分求解软件都实现了这个功能。</p>
<h4 id="_1">估计参数的统计特性</h4>
<p>为了确定估计的参数 $\boldsymbol{\hat{\beta}}$ 的统计特性，我们假设：</p>
<ul>
<li>每个观测值 $y_i$ 相互独立</li>
<li>$y_i$有固定的噪声 $\varepsilon \sim N(0, \sigma^2)$</li>
</ul>
<p>那么估计值 $\hat{\beta}$ 的方差为：</p>
<p>$$ \text{Var}(\hat{\beta}) = (\boldsymbol{X}^T\boldsymbol{X})^{-1} \sigma^2$$</p>
<p>where :</p>
<p>$$\hat{\sigma}^2 =  \frac{\text{RSS}}{N-p-1}= \frac{1}{N-p-1} \sum_{i=1}^{N} (y_i-\hat{y})^2$$</p>
<h5 id="_2">证明</h5>
<p>N 个 y 的观测值可以表示为：</p>
<p>$$ \boldsymbol{y} =  \boldsymbol{X}\beta +  \boldsymbol{\varepsilon}$$</p>
<p>其中 $ \boldsymbol{\varepsilon} $ 是 $N \times 1$ 的噪声。因此有：
$$\begin{align}
\hat{\beta} &amp;= (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y} \
&amp;= \beta + (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{\varepsilon}
\end{align}$$</p>
<p>无偏性（期望值为 $\beta$）：
$$E(\hat{\beta}) = \beta + (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T E(\boldsymbol{\varepsilon}) = \beta$$</p>
<p>协方差矩阵（注意是$\beta \beta^T$ 而非 $\beta^T \beta$，是一个矩阵）：</p>
<p>$$\begin{align}
\text{Var}(\hat{\beta}) &amp;= E[(\beta - \hat{\beta})(\beta - \hat{\beta})^T] \
&amp;=E[(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}^T\boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}] \
&amp;= (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T E(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}^T) \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1} \
&amp;= \sigma^2 (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T \boldsymbol{I} \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1} \
&amp;= \sigma^2 (\boldsymbol{X}^T\boldsymbol{X})^{-1}
\end{align}$$</p>
<p>可以得到：</p>
<p>$$ \hat{\beta} \sim N(\beta, \sigma^2 (\boldsymbol{X}^T\boldsymbol{X})^{-1})$$</p>
<p>下面来确定 $\sigma^2$ 。</p>
<p>我们可以通过观测值 $y$ 和预测值 $\hat{y}$ 的差来得到噪声 $\varepsilon$。</p>
<p>$$\begin{align}
\boldsymbol{y - \hat{y}} &amp;= \boldsymbol{X}\beta +  \boldsymbol{\varepsilon} -\boldsymbol{X}\hat{\beta} \
&amp;=  \boldsymbol{X}\beta +  \boldsymbol{\varepsilon} - \boldsymbol{X}(\beta + (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{\varepsilon}) \
&amp;= (\boldsymbol{I -H} )\boldsymbol{\varepsilon}
\end{align}$$</p>
<p>$$\begin{align}
\sum_{i=1}^N(y_i - \hat{y_i})^2 &amp;= (\boldsymbol{y - \hat{y}})^T (\boldsymbol{y - \hat{y}}) \
&amp;= \boldsymbol{\varepsilon}^T(\boldsymbol{I - H}) \boldsymbol{\varepsilon} \
&amp;= \sum_{k =1}^N \varepsilon_k^2- \sum_{i, j = 1}^N \varepsilon_i \varepsilon_j  H_{ij}
\end{align}$$</p>
<p>其期望值为：</p>
<p>$$\begin{align}
E[\sum_{i=1}^N(y_i - \hat{y_i})^2] &amp;= E[\sum_{k =1}^N \varepsilon_k^2- \sum_{i, j = 1}^N \varepsilon_i \varepsilon_j  H_{ij} ] \
&amp;= N\sigma^2 - E(\sum_{i, j = 1}^N \varepsilon_i \varepsilon_j  H_{ij})
\end{align}$$</p>
<p>由于 $\varepsilon_i, \varepsilon_j$ 是独立的，当 $i \neq j$ 时：
$$\text{Cov}(\varepsilon_i, \varepsilon_j) = E(\varepsilon_i \varepsilon_j) - E(\varepsilon_i)E(\varepsilon_j) = 0$$</p>
<p>因此：
$$\begin{align}
E[\sum_{i=1}^N(y_i - \hat{y_i})^2] &amp;= N\sigma^2 - E(\sum_{i, j = 1}^N \varepsilon_i \varepsilon_j  H_{ij}) \
&amp;= N\sigma^2 - E(\sum_{i=1}^{N}\varepsilon_i^2H_{ii}) \
&amp;= \sigma^2[N - \text{trace}(\boldsymbol{H})]
\end{align}$$</p>
<p>这里再利用公式：
$$\text{trace}(ABC) = \text{trace}(CAB) $$</p>
<p>得到：</p>
<p>$$\begin{align}
E[\sum_{i=1}^N(y_i - \hat{y_i})^2] &amp;= \sigma^2[N - \text{trace}(\boldsymbol{H})] \
&amp;= \sigma^2[N - \text{trace}(\boldsymbol{X(X^TX)^{-1}X^T})] \
&amp;= \sigma^2[N - \text{trace}(\boldsymbol{X^TX(X^TX)^{-1}}<em>{(p+1) \times (p+1)})] \
&amp;= \sigma^2[N - \text{trace}(\boldsymbol{I}</em>{(p+1) \times (p+1)})] \
&amp;= \sigma^2(N - p -1)
\end{align}$$</p>
<p>因此，对 $\sigma^2$ 的无偏估计就是：</p>
<p>$$\hat{\sigma}^2 = \frac{1}{N-p-1} \sum_{i=1}^{N} (y_i-\hat{y})^2$$</p>
<h4 id="_3">模型误差的统计特性</h4>
<p>由于我们对第 i 个样本的噪声 $\varepsilon_i $ 无偏估计就是 $\hat{\varepsilon_i} = y_i - \hat{y_i}$，我们计算其方差：</p>
<p>$$\begin{align}
\text{Var}(\hat{\boldsymbol{\varepsilon}}) &amp;= \text{Var}(\boldsymbol{y} - \hat{\boldsymbol{y}}) \
&amp;= \text{Var}[(\boldsymbol{I} - \boldsymbol{H}){\boldsymbol{\varepsilon}}]
\end{align}$$</p>
<p>由于 $D(AX) = AD(X)A^T$：</p>
<p>$$\begin{align}
\text{Var}(\hat{\boldsymbol{\varepsilon}}) &amp;= \text{Var}[(\boldsymbol{I} - \boldsymbol{H}){\boldsymbol{\varepsilon}}] \
&amp;= (\boldsymbol{I} - \boldsymbol{H}) \text{Var}(\boldsymbol{\varepsilon}) (\boldsymbol{I} - \boldsymbol{H})
\end{align}$$</p>
<p>由于 $\varepsilon \sim N(0, \sigma^2)$，因此：</p>
<p>$$\text{Var}(\boldsymbol{\varepsilon}) = \sigma^2 \boldsymbol{I}_{N \times N}$$</p>
<p>而 $\boldsymbol{H} = \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T$ 满足对称性和幂等性：</p>
<p>$$\boldsymbol{H}^T = \boldsymbol{H}$$</p>
<p>$$\boldsymbol{H}^T\boldsymbol{H} = \boldsymbol{H}$$</p>
<p>因此有结论：</p>
<p>$$\text{Var}(\hat{\boldsymbol{\varepsilon}}) = \sigma^2 (\boldsymbol{I} - \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T)$$</p>
<h4 id="_4">显著性分析</h4>
<p>当我们判断哪些参数可以忽略以降低模型复杂度时，我们可以使用 F-statistic 进行显著性分析。假设我们将 $\beta$ 维度从 $p_1 + 1$ 降低到 $p_0 + 1$：</p>
<p>$$ F = \frac{(\text{RSS}_0 - \text{RSS}_1) / (p_1 - p_0)}{\text{RSS}_1 / (N- p_1 -1)} $$</p>
<p>F-statistic 描述了每个被忽略的参数对 RSS 的平均贡献，用 $\hat{\sigma}^2$ 进行了 normalize。</p>
<p>当 $p_1 - p_0 =1$ 即仅去掉一个参数时（假设 $\beta_j = 0$），该公式可以简化为对应的 z-score 的平方，其中 z-score 为：</p>
<p>$$ z_j = \frac{\hat{\beta}_j}{\hat{\sigma} \sqrt{v_j} }$$</p>
<p>where:</p>
<p>$$\hat{\sigma}^2 =\frac{\text{RSS}<em>1}{N-p-1} =\frac{1}{N-p-1} \sum</em>{i=1}^{N} (y_i-\hat{y})^2$$</p>
<p>$$v_j = (\boldsymbol{X}^T\boldsymbol{X})^{-1}_{jj}$$</p>
<h5 id="_5">证明</h5>
<p>这个证明同时也是习题 3.1</p>
<blockquote>
<p>Ex. 3.1 Show that the F statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding z-score (3.12).</p>
</blockquote>
<p>实际上我们需要证明，在去掉模型的第 j 个参数后：</p>
<p>$$  \text{RSS}_0 - \text{RSS}_1 =  \frac{\hat{\beta}_j^2}{v_j} $$</p>
<p>上式中唯一未知的就是 $\text{RSS}_0$，它实质上是求一个带约束的优化问题：</p>
<p>$$\begin{align}
\min_{\beta \in \mathbb{R}^{(p+1) \times 1}} (\textbf{y} - \textbf{X}\beta)^T(\textbf{y}-\textbf{X}\beta) \
\text{s.t.}  ~\beta_j = 0
\end{align}$$</p>
<p>我们可以用拉格朗日乘子法来解决。</p>
<p>$$L(\beta, \lambda) = (\textbf{y} - \textbf{X}\beta)^T(\textbf{y}-\textbf{X}\beta) + \lambda e_j^T \beta $$</p>
<p>对 $\beta$ 求导，并令导数为 0，有：</p>
<p>$$\frac{\partial L(\beta, \lambda)}{\partial \beta} = - 2\textbf{X}^T(\textbf{y} - \textbf{X}\beta) + \lambda e_j = 0$$</p>
<p>解出：</p>
<p>$$\begin{align}
\beta_0 &amp;= (\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T\textbf{y}  - \frac{\lambda}{2}(\textbf{X}^T \textbf{X})^{-1} e_j \
&amp;= \hat{\beta}- \frac{\lambda}{2}(\textbf{X}^T \textbf{X})^{-1} e_j
\end{align}$$</p>
<p>等式两边乘以 $e_j^T$，并带入$\beta_j = 0$，有：
$$\begin{align}
e_j^T\beta_0 = 0 &amp;= e_j^T \hat{\beta} +  \frac{\lambda}{2} e_j^T(\textbf{X}^T \textbf{X})^{-1} e_j \
&amp;= \hat{\beta}_j + \frac{\lambda}{2}v_j
\end{align}$$</p>
<p>因此有：
$$\lambda = - \frac{2\hat{\beta}_j}{v_j}$$</p>
<p>带入可得：
$$\begin{align}
\text{RSS}_0 &amp;= (\textbf{y} - \textbf{X}\beta_0)^T(\textbf{y}-\textbf{X}\beta_0) \
&amp;= (\textbf{y} - \textbf{X}\hat{\beta} + \frac{\lambda}{2}\textbf{X}(\textbf{X}^T \textbf{X})^{-1} e_j)^T(\textbf{y}-\textbf{X}\hat{\beta} + \frac{\lambda}{2}\textbf{X}(\textbf{X}^T \textbf{X})^{-1} e_j) \
&amp;= \text{RSS}_1 + \frac{\lambda}{2} [e_j^T(\textbf{X}^T \textbf{X})^{-1}\textbf{X}^T(\textbf{y} - \textbf{X}\hat{\beta}) + (\textbf{y} - \textbf{X}\hat{\beta})^T \textbf{X}(\textbf{X}^T \textbf{X})^{-1} e_j)] \
&amp;~~~~ + \frac{\lambda^2}{4}e_j^T (\textbf{X}^T \textbf{X})^{-1} e_j \
&amp;= \text{RSS}_1 + \frac{\lambda^2}{4}e_j^T (\textbf{X}^T \textbf{X})^{-1} e_j \
&amp;= \text{RSS}_1 + \frac{\hat{\beta}_j^2}{v_j}
\end{align}$$</p>
<p>其中，中间项可以消去的原因是：</p>
<p>$$\textbf{X}^T(\textbf{y} - \textbf{X}\hat{\beta}) = \textbf{X}^T[\textbf{y} - \textbf{X}(\textbf{X}^T \textbf{X})^{-1}\textbf{X}^T\textbf{y}] = 0 $$</p>
<p>直观理解，$\textbf{X}$ 和 $\textbf{y} - \textbf{X}\hat{\beta}$ 是正交的，因为 $\textbf{X}\hat{\beta}$ 正是 $\textbf{y}$ 在 $\textbf{X}$ 所在平面上的投影。</p>
<h3 id="322-the-gaussmarkov-theorem">3.2.2 The Gauss–Markov Theorem</h3>
<p>最小二乘法得出的 $\beta$ 在所有线性<strong>无偏</strong>估计中均方误差最小。当然，如果我们愿意为了进一步减小误差引入一点 bias，完全可能找到一个更小均方误差的<strong>有偏</strong>估计。</p>
<blockquote>
<p>the least squares estimates of the parameters β have the smallest variance among all linear <strong>unbiased</strong> estimates</p>
</blockquote>
<p>现在我们来证明这个结论。对于线性估计：
$$\boldsymbol{y} = \boldsymbol{X}\beta$$
$\boldsymbol{y}$ 中的每一个元素都可以看作 $\boldsymbol{X}$ 中的一行与向量 $\beta$ 的线性组合。</p>
<h4 id="_6">无偏性</h4>
<p>那么，针对无偏性，我们需要证明最小二乘法估计出的 $\hat{\beta}$ 满足：</p>
<p>$$ E(\alpha^T \hat{\beta}) = \alpha^T\beta$$</p>
<p>其中 $\alpha$ 是任意向量。</p>
<p>$$\begin{align}
E(\alpha^T \hat{\beta}) &amp;= E(\alpha^T  (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}) \
&amp;= E(\alpha^T  (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{X} \beta) \
&amp;= \alpha^T \beta
\end{align} $$</p>
<h4 id="_7">均方误差最小</h4>
<p>Gauss–Markov theorem 指出，如果还存在其他线性估计 $c^T \boldsymbol{y}$ 满足：
$$ E(c^T \boldsymbol{y}) = \alpha^T\beta$$
那么必然有：</p>
<p>$$\text{Var}(\alpha^T \hat{\beta}) \leq \text{Var}(c^T \boldsymbol{y})$$</p>
<p>证明：</p>
<p>TBD</p>
<h2 id="33-subset-selection">3.3 Subset Selection</h2>
<p>最小二乘法的两个主要问题：</p>
<ul>
<li>预测精度。虽然它是无偏的，但是方差很大。如果我们忽略一部分模型参数，虽然会变成有偏估计，但是可能会极大提高精度。</li>
<li>可解释性（即模型复杂度）。当模型参数很多时，我们想去确定一小部分具有最大影响的模型参数，为此我们愿意牺牲一部分无关紧要的参数。</li>
</ul>
<p>因此，我们需要选取变量子集，即“model selection”。</p>
<h3 id="331-best-subset-selection">3.3.1 Best-Subset Selection</h3>
<p>最佳子集是指从所有具有 $k (k &lt;= p)$ 个变量的子集中，RSS 最小的那个。</p>
<p>当然，最简单的方式就是从遍历所有的组合。这样做的复杂度是 $2^p$，只适用于小规模的问题。</p>
<h3 id="332-forward-and-backward-stepwise-selection">3.3.2 Forward- and Backward-Stepwise Selection</h3>
<p>“前向逐步选择”是一种贪心算法。它按顺序加入最能提高拟合度的参数。它虽然不一定找到最优解，但是它优势在于：</p>
<ul>
<li>运算量小。当维度 $p &gt;= 40$ 时，几乎无法算出最优解。但是依旧可以用 forward stepwise selection （即使维度 p 大于样本数 N）。</li>
<li>方差小。最优子集方差比 forward stepwise selection 大，虽然后者可能会有一定的 bias。</li>
</ul>
<p><img alt="Subset selection" src="https://upload-images.jianshu.io/upload_images/4482847-e13f1157e2a1bf8b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" /></p>
<p>那么如何选择“最能提高拟合度“的参数呢？我们在之前“显著性分析”中已经证明了，去掉一个参数对残差的影响为其 z-score 的平方。那么，我们直接<strong>从 z-score 最大的参数开始依次加入</strong>即可。第 $j$ 个参数的 z-score 可以由于下式计算：</p>
<p>$$ z_j = \frac{\hat{\beta}_j}{\hat{\sigma} \sqrt{v_j} }$$</p>
<p>where:</p>
<p>$$\hat{\sigma}^2 =\frac{\text{RSS}<em>1}{N-p-1} =\frac{1}{N-p-1} \sum</em>{i=1}^{N} (y_i-\hat{y})^2$$</p>
<p>$$v_j = (\boldsymbol{X}^T\boldsymbol{X})^{-1}_{jj}$$</p>
<p>“后向逐步选择” 与 “前向逐步选择“相反。它从全集开始，依次去掉最无关紧要的变量（z-score 最小的）。它只能用于样本数 N 大于维度 p 的情形。</p>
<h2 id="34-shrinkage-methods">3.4 Shrinkage Methods</h2>
<p>Subset selection 确实可以帮我们简化模型，并且还可能降低误差。但是，因为它是一个离散的过程（参数要么被丢弃要么被保留，没有中间状态），它通常具有较大的方差。Shrinkage methods 更加连续，因此具有更好的性能。</p>
<h3 id="341-ridge-regression">3.4.1 Ridge Regression</h3>
<p>Ridge Regression 通过给参数数量增加一个惩罚项来降低模型复杂度。它的优化目标：</p>
<p>$$\hat{\beta} = \mathop{\arg \min}<em>{\beta}  \sum</em>{i=1}^N(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2$$</p>
<p>这里的 $\lambda$ 控制模型“缩小”的程度，$\lambda$ 越大，得到的模型复杂度越低。</p>
<p>值得注意的是，<strong>惩罚项中不包含常数项</strong> $\beta_0$，否则模型不稳定。当选取 $y_i = y_i + c$ 时，预测值 $\hat{y}_i$ 的变化量不是 $c$。</p>
<p>与经典的 Linear Regression 不同，Ridge Regression 要求输入 $\textbf{X}, \textbf{y}$ 是经过了<strong>中心化</strong> (centering) 的。并且，这里的模型参数 $\beta$ 是 $p$ 维而不是 $p+1$ 维的。</p>
<p>下面我们来证明这一点。</p>
<p>$\beta_0$ 由于不含 $\lambda$，可以单独优化。我们先对 $\beta_0$ 求导，并令导数为0:</p>
<p>$$\sum_{i=1}^N(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j) = 0$$</p>
<p>得到：
$$\beta_0 = \frac{1}{N}(\sum_{i=1}^N y_i - \sum_{i=1}^N \sum_{j=1}^{p} x_{ij}\beta_j) $$</p>
<p>令 $\overline{x_j} = \frac{1}{N} \sum_{i=1}^N x_{ij}$，有：</p>
<p>$$\beta_0 = \frac{1}{N}\sum_{i=1}^N y_i - \sum_{j=1}^{p} \overline{x_{j}} \beta_j $$</p>
<p>我们以下的变形主要是为了将优化目标函数写成矩阵乘法形式，以进行运算。</p>
<p>$$\begin{align}
\hat{\beta} &amp;= \mathop{\arg \min}<em>{\beta}  \sum</em>{i=1}^N(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2 \
&amp;= \mathop{\arg \min}<em>{\beta}  \sum</em>{i=1}^N(y_i - \beta_0 - \sum_{j=1}^p \overline{x_j}\beta_j - \sum_{j=1}^p (x_{ij} - \overline{x_j}) \beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2
\end{align}$$</p>
<p>现在我们令：</p>
<p>$$\begin{align}
\beta_0^c &amp;= \beta_0 + \sum_{j=1}^p \overline{x_j}\beta_j =\frac{1}{N} \sum_{i=1}^N y_{i} \
\beta_j^c&amp;= \beta_j &amp; (j&gt;=1)
\end{align}$$</p>
<p>可以得出：</p>
<p>$$\begin{align}
\hat{\beta} &amp;= \mathop{\arg \min}<em>{\beta^c}  \sum</em>{i=1}^N(y_i - \beta_0^c - \sum_{j=1}^p (x_{ij} - \overline{x_j}) \beta_j^c)^2 + \lambda \sum_{j=1}^p {\beta_j^c}^2
\end{align}$$</p>
<p>我们再令：</p>
<p>$$\begin{align}
y_i^c &amp;= y_i - \beta_0^c = y_i - \frac{1}{N} \sum_{i=1}^N y_i \
x_{ij}^c&amp;= x_{ij} - \overline{x_j} &amp; (j &gt;=1)
\end{align}$$</p>
<p>有：</p>
<p>$$\begin{align}
\hat{\beta} &amp;= \mathop{\arg \min}<em>{\beta^c}  \sum</em>{i=1}^N(y_i^c - \sum_{j=1}^p (x_{ij}^c \beta_j^c)^2) + \lambda \sum_{j=1}^p {\beta_j^c}^2 \
&amp;=\mathop{\arg \min}_{\beta} (\textbf{y} - \textbf{X}\beta)^T(\textbf{y} - \textbf{X}\beta) + \lambda(\beta^T\beta)
\end{align}$$</p>
<p>其中，$\textbf{X}, \textbf{y}, \beta$ 都经过了中心化，并且是 $p$ 维的。</p>
<p>该式对 $\beta$ 求导并令导数为 0，有：</p>
<p>$$ -\textbf{X}^T(\textbf{y} - \textbf{X}\beta)  + \lambda \beta = 0$$</p>
<p>解得：</p>
<p>$$ \beta = (\textbf{X}^T\textbf{X} + \lambda \textbf{I})^{-1} \textbf{X}^T \textbf{y}$$</p>
<p>我们看到，即使 $\textbf{X}^T\textbf{X}$ 是非满秩的，由于多加了一个 $\lambda \textbf{I}$，它仍是一个可逆矩阵。这也是 ridge regression 的另一个优势。</p>
<h1 id="reference">Reference</h1>
<ol>
<li><a href="https://yuhangzhou88.github.io/ESL_Solution/">ESL solution</a></li>
<li><a href="https://github.com/szcf-weiya/ESL-CN">ESL Chinese</a></li>
<li><a href="https://webspace.maths.qmul.ac.uk/b.bogacka/SM_I_2013_LecturesWeek_6.pdf">Simple Linear Regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares#Unbiasedness_of_.CE.B2.CC.82">Proofs involving ordinary least squares</a></li>
</ol></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
